{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtHGfnSicZL5gRZZzNvIYL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"tyN38MPZW54F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668013387456,"user_tz":-60,"elapsed":25406,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"a3e50244-b9b3-49f1-8452-4d593f820406"},"source":["import sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","sys.path.append('/content/drive/MyDrive/finance_course/2022/lesson7')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["* Main challenge in quantitative finance: risk measurement and assessment.\n","\n","* There is a wide range of risk measures that spread through the Market\n","  * we will concentrate specifically on two of them: Value at Risk VaR and Expected Shortfall ES.\n","\n","## VaR and Expected Shortfall\n","\n","* Value-at-risk is defined as the loss level that will not be exceeded with a certain confidence level during a certain period of time.\n","\n","$$\\textrm{VaR}_{\\alpha}(X) = -F^{-1}_X(\\alpha)$$\n","\n","* It is the loss corresponding to the $(100-\\alpha)\\textrm{th}$ percentile of the portfolio returns over the next $N$ days\n","* By definition it is a function of two parameters: the time horizon (i.e. $N$ days usually set to 1) and the confidence level (usually 95\\%). \n","* VaR asks the question \"how bad can things get ?\"\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1KGeDQ1uEs5O3ve4AFUHirfXvg-YUbfw9)\n","\n","</center>\n","\n","* For example, if a bank's 10-day 99\\% VaR is 3 million, there is considered to be only a 1\\% chance that losses will exceed 3 million in 10 days. \n","\n","* If VaR is used to limit the risks taken by a trader, it can lead to undesirable results. \n","  * A trader is asked that the one-day 99\\% VaR of its portfolio must be kept less than 10 million. The trader could construct a portfolio where there is a 99\\% chance that the daily loss is less than 10 million and a 1\\% chance that it is 500 million. The trader is satisfying the risk limits imposed by the bank, but is clearly taking unacceptable risks. \n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1Aal5SZ-yD2EZp_lW9hlNbBCoharaGmsl)\n","\n","</center>\n","\n","* A measure that produces better incentives for traders is *expected shortfall*. \n","* It is the expected loss during an N-day period, conditional that the loss is greater than the $\\alpha^{th}$ percentile of the loss distribution\n","\n","$$\\textrm{ES}_{\\alpha}(X) = \\frac{1}{\\alpha}\\int_0^\\alpha \\textrm{VaR}_p(X) dp$$\n","\n","*It is the average amount that is lost over a 10-day period, assuming that the loss is greater than the corresponding VaR.\n","* Expected Shortfall asks \"if things do get bad, what is our expected loss ?\"\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=109LfaKNqj4M_zqkeChlyy3pUXEREf-Cg)\n","\n","</center>\n","\n"],"metadata":{"id":"ckB4UCP3_UNx"}},{"cell_type":"markdown","source":["* We use a ficticious portfolio: 60\\% of AAPL and 40\\% NFLX shares. \n","* The dataset is avaialble in [historical_data.csv](https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/historical_data.csv).\n","\n","## Parametric VaR, ES\n","\n","* From the return historical series extract $\\mu$ and $\\sigma$ from a Gaussian fit of the return distribution obtained from the historical series.\n","* Apply directly the definitions of VaR and ES.\n"],"metadata":{"id":"Pg_5tg47Ffov"}},{"cell_type":"code","source":["# read sample, create portfolio col, returns and drop\n","import pandas as pd\n","from scipy.stats import norm\n","import numpy as np\n"],"metadata":{"id":"9dX0DQ01Go3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Numerical integration can be done using $\\texttt{scipy.integrate.quad}$, for example\n","\n","$$\\int_{0}^{7} x^2 dx = \\frac{x^3}{3}\\bigg|_0^7 = 114.3$$"],"metadata":{"id":"Q1D_y2uhGoTT"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.integrate import quad\n","\n","def f(x):\n","  return x**2\n","\n","print (quad(f, 0, 7))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3C_7uBzIHYIL","executionInfo":{"status":"ok","timestamp":1666978031812,"user_tz":-120,"elapsed":960,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"0efceb75-484f-4dd3-f4e0-e82ca9fe0882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(114.33333333333334, 1.2693549914880957e-12)\n"]}]},{"cell_type":"code","source":["# estimate mean, std and create functions for VaR e ES\n","# compute VaR and ES for sample\n","import numpy as np\n","from scipy.integrate import quad\n","from scipy.stats import norm, t\n","\n"],"metadata":{"id":"YfSTRnQjEl85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<center>\n","\n","![](https://drive.google.com/uc?id=1cb6udUY_9jqKiAAUbAwCLMv-HS6Y2vuU)\n","\n","</center>\n","\n","* Many theories in finance assume the normality of stock returns, unfortunately normality is not a realistic assumption as can be seen by performing a Gaussian fit to the return distribution.\n","\n","* A better model results in a more precise estimate of VaR and ES can be obtained with a t-student. \n","\n"],"metadata":{"id":"3ahCs6DYI49O"}},{"cell_type":"code","source":["# fit t-student and compute VaR and ES\n","from scipy.stats import t\n"],"metadata":{"id":"q4F4XlPSJ-Vd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Historical Simulation\n","\n","* Avoid any assumption on the return distribution, by random sampling on the historical returns directly. \n","  * The parameter estimation is not required in this case, but heavily relies on the assumption that past behaviors are indicative of what might happen in the future. \n","  * Historical series was as large as possible otherwise our results will be affected by lack of statistics. \n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1QFljTnNxfqD61FEmX3yXkBWRIouecr7j)\n","\n","</center>\n"],"metadata":{"id":"gln9BIZoKbNn"}},{"cell_type":"code","source":["# generate events from df, VaR and ES discrete\n","from numpy.random import choice\n","from numpy import percentile\n","\n"],"metadata":{"id":"jTKdz9-qKz8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SGjRXql1PtNW"},"source":["### Stress-test and Backtesting\n","\n","* It is generally useful to check how VaR would behave under the most extreme market moves seen in the last years.\n","  * This kind of test is called **stress-test**;\n","  * it is done by extracting from the historical series particular days with exceptionally large variation of the market variables, to take into account extreme events that can happen more frequently in reality than in a simulation (where usually Gaussian tails are assumed). \n","  * For example a 5-standard deviation move is expected to happen once every 7000 years but in practice can be observed twice over 10 years.\n","\n","* **Backtesting** consists of assessing how well the VaR estimate would have performed in the past. \n","  * Basically it has to be tested how often the daily loss exceeded the daily X% VaR just computed. \n","    * If it happens on about (100- X)% of the times we can be confident that our estimate is correct. \n","    * Clearly back-testing makes sense only if VaR has been estimated on an independent historical sample with respect to that used in the test."]},{"cell_type":"code","source":["# backtest\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJZeihLgMJKh","executionInfo":{"status":"ok","timestamp":1666978081588,"user_tz":-120,"elapsed":388,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"43710b66-6120-49fd-dd15-703217229f6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.04976525821596244\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4P6bnsoQWPM"},"source":["## Credit VaR\n","\n","* Credit VaR is defined as a percentile of the **credit loss** distribution. \n","  * In this case we are concerned with the default risk associated to counter-parties instead of the market risk.\n","* The **exposure** $EE(\\tau)$ defined as the sum of the discounted cash flows at the default date $\\tau$. \n","* The corresponding **loss** is then given by $L =(1−R)·EE(\\tau)$\n","  * where $L$ is non-zero only in scenarios of early counter-party default.\n","* Credit VaR can be expressed as the $\\alpha$-quantile of the distribution of $L$. \n","  * Time horizon is usually set to one year and the percentile to the 99.9th, so it returns the loss that is exceeded only in 1 case out of 1000.\n","\n","### Credit VaR and MC Simulation\n","* Credit VaR can be calculated through a simulation of the evolution of a portfolio up to the risk horizon, including possible defaults of the counter-parties.\n","* In each experiment the portfolio is priced obtaining a number of scenarios to draw the loss distribution.\n","\n","#### Example\n","* Consider a portfolio of 20 zero coupon bonds each one with a default probability of 8% and the same face value (100 EUR). The recovery rate in case of default is 40% and the risk free rate is 1%."]},{"cell_type":"code","source":["# bond class with NPV with default\n","from finmarkets import generate_dates\n","\n"],"metadata":{"id":"AQTT9gTMPpDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQStBMspoOz6"},"source":["# compute credit var for 5 bonds, generate defaults with ExpDefault\n","import pandas as pd\n","import numpy as np\n","from datetime import date\n","from dateutil.relativedelta import relativedelta\n","from finmarkets import DiscountCurve, CreditCurve, ExpDefault\n","from scipy.stats import multivariate_normal, norm\n","\n","start_date = date.today()\n","df = pd.read_excel(\"https://github.com/matteosan1/finance_course/raw/develop/libro/input_files/discount_factors_2022-10-05.xlsx\")\n","pillars = [start_date + relativedelta(months=i) for i in df['months']]\n","dc = DiscountCurve(pillars, df['dfs'])\n","\n","maturity = 2\n","coupons = [0.01, 0.02, 0.03, 0.04, 0.05]\n","N = 10000\n","rho = 0.9999999\n","l = 0.5\n","R = 0.4\n","horizon = 1\n","\n","start_date = date.today()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","from matplotlib.patches import ConnectionPatch\n","\n","plt.rcParams['figure.figsize'] = (6,9)\n","ax1 = plt.subplot2grid((3,3),(0,0), colspan = 3, rowspan=2)\n","ax2 = plt.subplot2grid((3,3),(2,2), colspan = 1)\n","\n","y, x, _ = ax1.hist(L, bins=320)\n","ax1.set_xlim(50, 320)\n","ax1.vlines(np.percentile(L, 99.9), 0, y[1]*0.4, color='purple')\n","ax1.text(np.percentile(L, 99.9), y[1]*0.5, \"Credit VaR\", fontsize=12, color='purple', rotation=90)\n","ax1.grid(True)\n","ax1.set_axisbelow(True)\n","ax1.add_patch(Rectangle((290, 0), 20, 500, edgecolor='red', fill=False))\n","\n","con = ConnectionPatch(xyA=(290, 0), # lower left corner\n","                      xyB=(300, 500), # lower left corner\n","                      coordsA=\"data\", coordsB=\"data\",\n","                      axesA=ax1, axesB=ax2, color=\"red\")\n","ax2.add_artist(con)\n","\n","con = ConnectionPatch(xyA=(290, 500), # upper left corner\n","                      xyB=(290, 500), # upper left corner\n","                      coordsA=\"data\", coordsB=\"data\",\n","                      axesA=ax1, axesB=ax2, color=\"red\")\n","ax2.add_artist(con)\n","\n","con = ConnectionPatch(xyA=(310, 0), # lower right corner\n","                      xyB=(309, 500), # lower right corner\n","                      coordsA=\"data\", coordsB=\"data\",\n","                      axesA=ax1, axesB=ax2, color=\"red\")\n","ax2.add_artist(con)\n","\n","con = ConnectionPatch(xyA=(310, 500), # upper right corner\n","                      xyB=(310, 500), # upper right corner\n","                      coordsA=\"data\", coordsB=\"data\",\n","                      axesA=ax1, axesB=ax2, color=\"red\")\n","ax2.add_artist(con)\n","\n","y, x, _ = ax2.hist(L, bins=320)\n","ax2.grid(True)\n","ax2.vlines(np.percentile(L, 99.9), 0, 120, color='purple')\n","ax2.text(np.percentile(L, 99.9), 150, \"Credit VaR\", fontsize=12, color='purple', rotation=90)\n","ax2.set_xlim(290, 310)\n","ax2.set_ylim(0, 500)\n","ax2.set_xlabel(\"loss\")\n","plt.show()"],"metadata":{"id":"yIF-Wv1bvfbJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtATujWma8HY"},"source":["## CVA and DVA\n","\n","* Suppose you have a portfolio of derivatives. \n","* If a counter-party defaults and the present value of the portfolio at default is positive to the surviving party (you), then the actual gain is only given by the recovery fraction of the value. \n","* If however the present value is negative to you, you have to pay it in full to the liquidators of the defaulted entity.\n","* This behaviour creates an asymmetry which can be corrected by changing the definition of the deal value as the value without counter-party risk minus a positive adjustment, called Credit Valuation Adjustment (CVA).\n","\n","$$\\textrm{CVA} = (1-R)\\int_0^{T} D(t)\\cdot EE(t) dQ(t)$$\n","\n","* where $T$ is the latest maturity in the portfolio, $D$ is the discount factor, $EE$ is the expected exposure or $\\mathbb{E}[\\textrm{max}(0, \\mathrm{NPV_{portfolio}})]$, and $dQ$ is the probability of default between $t$ and $t + dt$.\n","\n","* Or it's discrete version:\n","\n","$$\\textrm{CVA} = (1-R)\\sum_i D(t_i)\\cdot EE(t_i)\\cdot  Q(t_{i-1}, t_i))$$\n","\n","* **Credit VaR measures the risk of losses faced due to the default of some counter-party, while CVA measures the price adjustment of a contract due to this risk**.\n","\n","### DVA\n","* The adjustment seen from the point of view of our counter-party is positive, and is called Debit Valuation Adjustment, DVA. \n","* It is positive because the early default of the client itself would imply a discount on its payment obligations, and this means a gain. \n","\n","* When both parties have a non-null probability of default, they consistently include both CVA and DVA into the valuation. So they will mark **a positive CVA to be subtracted** and **a positive DVA to be added** to the default-risk-free price of the deal. \n","  * The CVA of one party will be the DVA of the other one and vice versa.\n","\n","$$\\textrm{price = default risk free price + DVA - CVA}$$\n","\n","## CVA Computation\n","\n","* CVA can be computed with Monte Carlo simulation. \n","  1. Compute the portfolio value at each time point for each MC scenario.    \n","  2. Calculate the CVA using one of the equation above. \n","  3. Average the CVA of all the scenarios to get its estimate.\n","\n","#### Example\n","* Imagine a 3-years zero coupon bond with a face value FV = 100. \n","* The bond issuer has the following default probabilities 10%, 20% and 30% for 1, 2 and 3 years respectively and the recovery rate is 40%. \n","* The risk free rate is instead 3% flat.\n"]},{"cell_type":"code","metadata":{"id":"exljwHqtYvU2"},"source":["# CVA of a bond\n","import pandas as pd\n","import numpy as np\n","from dateutil.relativedelta import relativedelta\n","from finmarkets import DiscountCurve, CreditCurve\n","\n","FV = 100\n","r = 0.03\n","R = 0.4\n","maturity = 3\n","\n","start_date = date.today()\n","df = pd.read_excel(\"https://github.com/matteosan1/finance_course/raw/develop/libro/input_files/discount_factors_2022-10-05.xlsx\")\n","pillars = [start_date + relativedelta(months=i) for i in df['months']]\n","dc = DiscountCurve(pillars, df['dfs'])\n","\n","pillar_dates = [start_date + relativedelta(years=i) for i in range(maturity+1)]\n","S = [1, 0.9, 0.8, 0.7]\n","cc = CreditCurve(pillar_dates, S)\n"],"execution_count":null,"outputs":[]}]}