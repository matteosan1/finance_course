{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","sys.path.append('/content/drive/MyDrive/finance_course/2022/lesson9')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYlB0A6j0sSc","executionInfo":{"status":"ok","timestamp":1668704439072,"user_tz":-60,"elapsed":37466,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"d1eb57f5-790f-4b4c-9d24-2175137eda6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Non Linear Models\n","\n","* When the target variable $y$ has a more complicated relationship with the independent variables $X$ and linear models are not working anymore we need to move to **non-linear models**.\n","\n","* If the model is not know it is possible to use machine learning techniques in order to infeer its characteristics directly from the dataset."],"metadata":{"id":"67tChQac95-J"}},{"cell_type":"markdown","metadata":{"id":"QBUcBK_6sRos"},"source":["## Machine Learning\n","\n","### Neural Network Definition\n","* Artificial Neural Networks (ANN or simply NN) are information processing models that are developed by inspiring from the working principles of human brain. \n","  * **Their most essential property is the ability of learning from sample sets.** \n","\n","* The basic unit of ANN architecture are neurons. \n","\n","<center>\n","\n","![Model of an artificial neuron.](https://drive.google.com/uc?id=1sT_uKTvHpG4KJqBICnhYlimAz7UIagsk)\n","\n","</center>\n","  \n","$$ \\textrm{Inputs} = \\sum_{i=1}^{N} x_i w_i +w_0 = \\Sigma \\rightarrow = f(\\Sigma) \\rightarrow \\textrm{Output}$$  \n","\n","* The *activation function* is used to add non-linearity to the respons of the neuron.\n","  * There are many different types of activation function\n","    * *step function* which returns just 0 or 1 according to the input value \n","    * *sigmoid* which can be thought of as the continuous version of the step function)\n","    * rectified Linear Unit (ReLU) \n","    * hyperbolic tangent (tanh).\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1yPgenOKBcnH3B_F1jx1Q6T1Bc249n9ya)\n","\n","</center>\n","\n","### Supervised Training of a Neuron\n","\n","* In the process of training a neuron we would like to teach it to give the \"correct\" output providing a certain input (hence the name *supervised*).\n","\n","1. Inputs from the *training* set are presented to the neuron one after the other together with the target output;\n","2. the neuron weights are modified in order to make the neuron output as close as possible to the target;\n","3. when an entire pass through all of the input training vectors is completed (an *epoch*) the neuron has learnt. \n","  * Actually we can present many times the same set to the neuron to make it learn better (but not too many times, see **overfitting**).\n","\n","* Using just a neuron is a too simple architecture. The next step is to put together more neurons in *layers*.\n","\n","### Multilayered Neural Networks\n","\n","<center>\n","\n","![A multilayered neural network.](https://drive.google.com/uc?id=1_D3eO0Bb5XwF9SIFbEvsMNNbz_hI3EuX)\n","\n","</center>\n","\n","* In a multilayered NN each neuron from the *input layer* is fed up to each neuron in the next hidden layer, and from there to each neuron on the output layer. \n","  * There can be any number of neurons per layer.\n","\n","### Training a Multilayered Neural Network\n","\n","* The training of a multilayered NN follows similar these steps:\n","  1. present a training sample to the neural network and compute the network output obtained by calculating activations of each neuron of each layer;\n","  2. calculate the **loss** as the difference between the NN predicted and the target output;\n","  3. \"re-adjust\" the weights of the network such that the difference with the target output decreases;\n","  5. continue the process for each input several times (epochs).\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1M38qS_oDvO45sOA894UqMTyw50K2o5zS)\n","\n","</center>\n","\n","* The NN loss is computed by the *loss function*, possible choices are\n","  * Mean Absolute Error (MAE): the average of the absolute value of the differences between the predictions and true values. It represents how far off we are on average from the correct value;\n","  * Root Mean Squared Error (MSE): the square root of the average of the squared differences between the predictions and true values. It penalizes larger errors more heavily and is commonly used in regression tasks. \n","\n","* **Back propagation** is the algorithm used to reduce the loss function:\n","  * the current loss is \"propagated\" backwards to previous layers, where it is used to modify the weights.\n","\n","$$\\min_{w} L(w_{11}, w_{12},\\ldots) \\implies \\frac{\\partial L}{\\partial w_{ij}} = 0$$\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1NQFCPJomQQD4l1KcK-7iBTLxDkE8DpiG)\n","\n","</center>\n","\n","* Weights are modified using a function called *Optimization Function* (we will use *Adam* as optimizator in the following but there are more).\n","\n"]},{"cell_type":"markdown","source":["## Regression and Classification\n","\n","### Classification \n","* Is the process of finding a function to split the dataset into classes based on different parameters. \n","  * The goal is to find the mapping function between the input and the **discrete** output($y$).\n","\n","* Email spam detection: the model is trained on the basis of millions of emails on different parameters, and whenever it receives a new email, it identifies whether the email is spam or not.\n","* Classification algorithms can also be in speech recognition, car plates identification, etc.\n","\n","### Regression\n","* Is the process of finding the correlations between dependent and independent variables. \n","  * The goal is to find the mapping function to map the input variable to the **continuous** output variable.\n","\n","* Housing price prediction: the input data can be different home features and the output prediction will be pricing estimate. \n","  * In general whenever we are dealing with function approximation this kind of algorithms can be applied. \t\n"],"metadata":{"id":"CTCOmeauJuKM"}},{"cell_type":"markdown","source":["### Technical Note\n","\n","* Neural network training and testing is performed using $\\tt{keras}$ (which is based on a Google opensource library called $\\tt{tensorflow}$) and $\\tt{scikit-learn}$ which provide many useful utilitites for the training.\n","\n","## Function approximation \n","\n","* Let's design an ANN which is capable of learning the functional form underlying a set of data ([function_approx.csv](https://github.com/matteosan1/finance_course/raw/develop/libro/input_files/function_approx.csv)).\n","\n","* **SPOILER** the relation between $X$ and $y$ is $f(x) = x^3 +2$."],"metadata":{"id":"dMWyVDadKPLi"}},{"cell_type":"code","metadata":{"id":"fJpgtXvlOh5x"},"source":["# load the dataset\n","import numpy as np\n","import pandas as pd\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check min and max for input and output\n"],"metadata":{"id":"ICz_MECxvxAO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFyOnuYbsRot"},"source":["* **Usually when dealing with multi-input NN it is good practice to transform each variable to have all the inputs with uniform scales (usually [0,1])**. \n","* This is done to provide the NN with *normalized* data, infact it can be fooled by very large or very small numbers giving unstable results."]},{"cell_type":"code","metadata":{"id":"qEAYJm2pOywq"},"source":["# normalize data, split train and test\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjalpeQxsRot"},"source":["### Neural Network Design\n","\n","* There is no rule to guide developers into the design of a neural network in terms of number of layers and neuron per layer. \n","* The most common strategy is *trial and error* where you pick up the solution giving the best accuracy. \n","  * In general a larger number of nodes is better to catch highly structured data with a lot of feature although it may require larger training sample to work correctly.\n","  * **As a rule of thumb a NN with just one hidden layer with a number of neurons averaging the inputs and outputs is sufficient in most cases.** \n","\n","\n","* Let's use two layers with 15 and 5 neurons and a *tanh* activation function. \n","* The $\\tt{inputs}$ parameter has to be set to 1 since we have just one single input, the $x$ value. \n","\n","<img src=\"https://drive.google.com/uc?id=1VBb5EA8dX9ZeAGD_EkUvbv0pkBXv_TID\">"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yKgXvYwasRou"},"source":["# design the neural network model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","model = Sequential()\n","model.add(Dense(15, input_dim=1, activation='sigmoid'))\n","model.add(Dense(5, activation='sigmoid'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='mse', optimizer='adam')\n","\n","model.fit(train_X, train_y, epochs=100, batch_size=10, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load model trained with 500 epochs\n","from tensorflow.keras.models import load_model\n","\n","model500 = load_model(\"/content/drive/MyDrive/finance_course/2022/lesson9/func_500epochs\")\n"],"metadata":{"id":"_NlCkAh9zvUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vANrQHHfsRou"},"source":["* After the training is completed we can evaluate how good it is. \n","  * **Usually performance are measured using the loss function value at the end of the training.**\n","  * a *perfect* prediction would lead to a loss = 0 so the lower this number the better the agreement. "]},{"cell_type":"markdown","metadata":{"id":"YkFFWMAksRou"},"source":["* The picture below are shown the actual function we want to approximate and different predictions of our NN obtained with four epoch numbers (5, 100, 800, 5000).\n","\n","<img src=\"https://drive.google.com/uc?id=1oWfQq6q7PVzrZ979FS_weAJUj3cL7AOu\">\n","\n","* The agreement improves with higher number of epochs which means that the NN has more opportunities to adapt the weights and reduce the loss to the target values. \n","\n","### Overfitting (Overtraining)\n","\n","* Increasing too much the number of epochs may lead to overfitting: \n","  * the NN learns too well the training sample but its performance degrade substantially in an independent sample. \n","* It is required to split the available sample in two parts: training and testing (e.g. 80% and 20%) \n","  * **training** to perform the setting of the weights;\n","  * **testing** to cross-check the performance in an independent sample. \n","\n","* To check if this is the case we can *evaluate* our NN with both the training ad the testing samples. \n","  * If the losses are comparable the NN is ok otherwise if the training losses are much smaller than the testing we had overfitting.\n","  * In this second case if we need more accuracy we need to either increase the training sample or to change the NN design.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uvtyrPzxsRou"},"source":["# evaluate on train and test\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87lJvt7u5eLr"},"source":["## A Feature Not a Bug\n","\n","* If you ran the previous example you would most likely obtain different results.\n","  * **This is not a bug but a feature of NN**, let's see which are the possible sources for such discrepancies.\n","\n","* **Stochastic learning algorithm**: NN algorithm is stochastic i.e. its behaviour incorporates elements of randomness (beware that stochastic\n","does not mean learning a random model). \n","* Their randomness comes from: \n","  * the *random initial weights*, which allow the model to try learning from a different starting point in the search space each time; \n","  * the *random shuffle of examples during training*, which ensures that each gradient estimate and weight update is slightly different. \n","\n","* The impact is that each time it is run on the same data, it learns a slightly different model and when evaluated, may have a slightly different performance. \n","\n","* You can control randomness by setting the seed used by the pseudorandom number generator: although this is not a good approach in practice:\n","  * **there is no best seed for any algorithm**; \n","  * you need to summarize the performance by fitting multiple times a model on your dataset and averaging its predictions.\n"]}]}