{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","sys.path.append('/content/drive/MyDrive/finance_course/2022/lesson9')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYlB0A6j0sSc","executionInfo":{"status":"ok","timestamp":1669038401103,"user_tz":-60,"elapsed":23304,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"3eefa1fd-84ff-409f-b5a7-1d87b2ecb8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Non Linear Models\n","\n","* When the target variable $y$ has a more complicated relationship with the independent variables $X$ and linear models are not working anymore we need to move to **non-linear models**.\n","\n","* If the model is not know it is possible to use machine learning techniques in order to infeer its characteristics directly from the dataset."],"metadata":{"id":"67tChQac95-J"}},{"cell_type":"markdown","metadata":{"id":"QBUcBK_6sRos"},"source":["## Machine Learning\n","\n","### Neural Network Definition\n","* Artificial Neural Networks (ANN or simply NN) are information processing models that are developed by inspiring from the working principles of human brain. \n","  * **Their most essential property is the ability of learning from sample sets.** \n","\n","* The basic unit of ANN architecture are neurons. \n","\n","<center>\n","\n","![Model of an artificial neuron.](https://drive.google.com/uc?id=1sT_uKTvHpG4KJqBICnhYlimAz7UIagsk)\n","\n","</center>\n","  \n","$$ \\textrm{Inputs} = \\sum_{i=1}^{N} x_i w_i +w_0 = \\Sigma \\rightarrow = f(\\Sigma) \\rightarrow \\textrm{Output}$$  \n","\n","* The *activation function* is used to add non-linearity to the respons of the neuron.\n","  * There are many different types of activation function\n","    * *step function* which returns just 0 or 1 according to the input value \n","    * *sigmoid* which can be thought of as the continuous version of the step function)\n","    * rectified Linear Unit (ReLU) \n","    * hyperbolic tangent (tanh).\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1yPgenOKBcnH3B_F1jx1Q6T1Bc249n9ya)\n","\n","</center>\n","\n","### Supervised Training of a Neuron\n","\n","* In the process of training a neuron we would like to teach it to give the \"correct\" output providing a certain input (hence the name *supervised*).\n","\n","1. Inputs from the *training* set are presented to the neuron one after the other together with the target output;\n","2. the neuron weights are modified in order to make the neuron output as close as possible to the target;\n","3. when an entire pass through all of the input training vectors is completed (an *epoch*) the neuron has learnt. \n","  * Actually we can present many times the same set to the neuron to make it learn better (but not too many times, see **overfitting**).\n","\n","* Using just a neuron is a too simple architecture. The next step is to put together more neurons in *layers*.\n","\n","### Multilayered Neural Networks\n","\n","<center>\n","\n","![A multilayered neural network.](https://drive.google.com/uc?id=1_D3eO0Bb5XwF9SIFbEvsMNNbz_hI3EuX)\n","\n","</center>\n","\n","* In a multilayered NN each neuron from the *input layer* is fed up to each neuron in the next hidden layer, and from there to each neuron on the output layer. \n","  * There can be any number of neurons per layer.\n","\n","### Training a Multilayered Neural Network\n","\n","* The training of a multilayered NN follows similar these steps:\n","  1. present a training sample to the neural network and compute the network output obtained by calculating activations of each neuron of each layer;\n","  2. calculate the **loss** as the difference between the NN predicted and the target output;\n","  3. \"re-adjust\" the weights of the network such that the difference with the target output decreases;\n","  5. continue the process for each input several times (epochs).\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1M38qS_oDvO45sOA894UqMTyw50K2o5zS)\n","\n","</center>\n","\n","* The NN loss is computed by the *loss function*, possible choices are\n","  * Mean Absolute Error (MAE): the average of the absolute value of the differences between the predictions and true values. It represents how far off we are on average from the correct value;\n","  * Root Mean Squared Error (MSE): the square root of the average of the squared differences between the predictions and true values. It penalizes larger errors more heavily and is commonly used in regression tasks. \n","\n","* **Back propagation** is the algorithm used to reduce the loss function:\n","  * the current loss is \"propagated\" backwards to previous layers, where it is used to modify the weights.\n","\n","$$\\min_{w} L(w_{11}, w_{12},\\ldots) \\implies \\frac{\\partial L}{\\partial w_{ij}} = 0$$\n","\n","<center>\n","\n","![](https://drive.google.com/uc?id=1NQFCPJomQQD4l1KcK-7iBTLxDkE8DpiG)\n","\n","</center>\n","\n","* Weights are modified using a function called *Optimization Function* (we will use *Adam* as optimizator in the following but there are more).\n","\n"]},{"cell_type":"markdown","source":["## Regression and Classification\n","\n","### Classification \n","* Is the process of finding a function to split the dataset into classes based on different parameters. \n","  * The goal is to find the mapping function between the input and the **discrete** output($y$).\n","\n","* Email spam detection: the model is trained on the basis of millions of emails on different parameters, and whenever it receives a new email, it identifies whether the email is spam or not.\n","* Classification algorithms can also be in speech recognition, car plates identification, etc.\n","\n","### Regression\n","* Is the process of finding the correlations between dependent and independent variables. \n","  * The goal is to find the mapping function to map the input variable to the **continuous** output variable.\n","\n","* Housing price prediction: the input data can be different home features and the output prediction will be pricing estimate. \n","  * In general whenever we are dealing with function approximation this kind of algorithms can be applied. \t\n"],"metadata":{"id":"CTCOmeauJuKM"}},{"cell_type":"markdown","source":["### Technical Note\n","\n","* Neural network training and testing is performed using $\\tt{keras}$ (which is based on a Google opensource library called $\\tt{tensorflow}$) and $\\tt{scikit-learn}$ which provide many useful utilitites for the training.\n","\n","## Function approximation \n","\n","* Let's design an ANN which is capable of learning the functional form underlying a set of data ([function_approx.csv](https://github.com/matteosan1/finance_course/raw/develop/libro/input_files/function_approx.csv)).\n","\n","* **SPOILER** the relation between $X$ and $y$ is $f(x) = x^3 +2$."],"metadata":{"id":"dMWyVDadKPLi"}},{"cell_type":"code","metadata":{"id":"fJpgtXvlOh5x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669038403126,"user_tz":-60,"elapsed":2028,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"8749907a-fbe1-4024-a5d3-429c5daa2733"},"source":["# load the dataset\n","import numpy as np\n","import pandas as pd\n","\n","df = pd.read_csv(\"https://github.com/matteosan1/finance_course/raw/develop/libro/input_files/function_approx.csv\")\n","print (df.head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["       X         y\n","0 -2.000 -6.000000\n","1 -1.999 -5.988006\n","2 -1.998 -5.976024\n","3 -1.997 -5.964054\n","4 -1.996 -5.952096\n"]}]},{"cell_type":"code","source":["# check min and max for input and output\n","print (df['y'].min(), df['y'].max())"],"metadata":{"id":"ICz_MECxvxAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668684234156,"user_tz":-60,"elapsed":348,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"a9be7616-73c5-46bc-c2b5-35342d0de376"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-6.0 9.999999999994714\n"]}]},{"cell_type":"markdown","metadata":{"id":"mFyOnuYbsRot"},"source":["* **Usually when dealing with multi-input NN it is good practice to transform each variable to have all the inputs with uniform scales (usually [0,1])**. \n","* This is done to provide the NN with *normalized* data, infact it can be fooled by very large or very small numbers giving unstable results."]},{"cell_type":"code","metadata":{"id":"qEAYJm2pOywq","colab":{"base_uri":"https://localhost:8080/","height":437},"executionInfo":{"status":"error","timestamp":1669038512659,"user_tz":-60,"elapsed":281,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"d760f2b5-d5b5-4c62-b468-c3256ced8f60"},"source":["# normalize data, split train and test\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","scale_X = MinMaxScaler()\n","scale_y = MinMaxScaler()\n","\n","x = np.array([[1,2,3,4,5,6]])\n","\n","X_scale = scale_X.fit_transform(x)\n","y_scale = scale_y.fit_transform(df['y'])\n","\n","print (y_scale.min(), y_scale.max())\n","\n","train_X, test_X, train_y, test_y = train_test_split(X_scale, y_scale, test_size=0.2)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-864137de3507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                 )\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-6.         -5.988006   -5.97602399 ...  9.97602399  9.988006\n 10.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."]}]},{"cell_type":"markdown","metadata":{"id":"GjalpeQxsRot"},"source":["### Neural Network Design\n","\n","* There is no rule to guide developers into the design of a neural network in terms of number of layers and neuron per layer. \n","* The most common strategy is *trial and error* where you pick up the solution giving the best accuracy. \n","  * In general a larger number of nodes is better to catch highly structured data with a lot of feature although it may require larger training sample to work correctly.\n","  * **As a rule of thumb a NN with just one hidden layer with a number of neurons averaging the inputs and outputs is sufficient in most cases.** \n","\n","\n","* Let's use two layers with 15 and 5 neurons and a *tanh* activation function. \n","* The $\\tt{inputs}$ parameter has to be set to 1 since we have just one single input, the $x$ value. \n","\n","<img src=\"https://drive.google.com/uc?id=1VBb5EA8dX9ZeAGD_EkUvbv0pkBXv_TID\">"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yKgXvYwasRou","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668684689181,"user_tz":-60,"elapsed":82942,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"d998cd4c-727e-43cd-aaac-532578a5c1e8"},"source":["# design the neural network model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","model = Sequential()\n","model.add(Dense(15, input_dim=1, activation='sigmoid'))\n","model.add(Dense(5, activation='sigmoid'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='mse', optimizer='adam')\n","\n","model.fit(train_X, train_y, epochs=100, batch_size=10, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0372\n","Epoch 2/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0309\n","Epoch 3/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0237\n","Epoch 4/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0145\n","Epoch 5/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0091\n","Epoch 6/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0078\n","Epoch 7/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0076\n","Epoch 8/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0075\n","Epoch 9/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0075\n","Epoch 10/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0075\n","Epoch 11/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0075\n","Epoch 12/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 13/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 14/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 15/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 16/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 17/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 18/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0074\n","Epoch 19/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0073\n","Epoch 20/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0073\n","Epoch 21/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0073\n","Epoch 22/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0073\n","Epoch 23/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0073\n","Epoch 24/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 25/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 26/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 27/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 28/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 29/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 30/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0072\n","Epoch 31/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 32/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 33/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 34/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 35/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 36/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 37/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 38/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 39/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 40/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 41/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0071\n","Epoch 42/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 43/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 44/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 45/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 46/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 47/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0070\n","Epoch 48/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 49/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 50/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 51/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 52/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 53/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 54/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 55/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0068\n","Epoch 56/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0069\n","Epoch 57/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0068\n","Epoch 58/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0067\n","Epoch 59/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0067\n","Epoch 60/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0067\n","Epoch 61/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0066\n","Epoch 62/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0066\n","Epoch 63/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0065\n","Epoch 64/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0065\n","Epoch 65/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0064\n","Epoch 66/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0063\n","Epoch 67/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0062\n","Epoch 68/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0061\n","Epoch 69/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0060\n","Epoch 70/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0059\n","Epoch 71/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0057\n","Epoch 72/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0056\n","Epoch 73/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0055\n","Epoch 74/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0053\n","Epoch 75/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0052\n","Epoch 76/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0050\n","Epoch 77/100\n","320/320 [==============================] - 1s 3ms/step - loss: 0.0048\n","Epoch 78/100\n","320/320 [==============================] - 1s 3ms/step - loss: 0.0047\n","Epoch 79/100\n","320/320 [==============================] - 1s 3ms/step - loss: 0.0045\n","Epoch 80/100\n","320/320 [==============================] - 1s 3ms/step - loss: 0.0043\n","Epoch 81/100\n","320/320 [==============================] - 1s 3ms/step - loss: 0.0042\n","Epoch 82/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0040\n","Epoch 83/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0039\n","Epoch 84/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0037\n","Epoch 85/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0035\n","Epoch 86/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0034\n","Epoch 87/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0032\n","Epoch 88/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0030\n","Epoch 89/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0029\n","Epoch 90/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0027\n","Epoch 91/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0026\n","Epoch 92/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0024\n","Epoch 93/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0023\n","Epoch 94/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0022\n","Epoch 95/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0020\n","Epoch 96/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0019\n","Epoch 97/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0018\n","Epoch 98/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0017\n","Epoch 99/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0016\n","Epoch 100/100\n","320/320 [==============================] - 1s 2ms/step - loss: 0.0015\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd6d4ea5a90>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# load model trained with 500 epochs\n","from tensorflow.keras.models import load_model\n","\n","model500 = load_model(\"/content/drive/MyDrive/finance_course/2022/lesson9/func_500epochs\")\n","\n","x0 = 1\n","x_transf = scale_X.transform([[x0]])\n","print (scale_y.inverse_transform(model.predict(x_transf)))\n","\n","print (scale_y.inverse_transform(model500.predict(x_transf)))\n"],"metadata":{"id":"_NlCkAh9zvUR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668684809742,"user_tz":-60,"elapsed":757,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"4e32fa23-a7d7-4997-8fd4-f767f83affc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 17ms/step\n","[[3.7787256]]\n","1/1 [==============================] - 0s 52ms/step\n","[[2.9655228]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  \"X does not have valid feature names, but\"\n"]}]},{"cell_type":"markdown","metadata":{"id":"vANrQHHfsRou"},"source":["* After the training is completed we can evaluate how good it is. \n","  * **Usually performance are measured using the loss function value at the end of the training.**\n","  * a *perfect* prediction would lead to a loss = 0 so the lower this number the better the agreement. "]},{"cell_type":"markdown","metadata":{"id":"YkFFWMAksRou"},"source":["* The picture below are shown the actual function we want to approximate and different predictions of our NN obtained with four epoch numbers (5, 100, 800, 5000).\n","\n","<img src=\"https://drive.google.com/uc?id=1oWfQq6q7PVzrZ979FS_weAJUj3cL7AOu\">\n","\n","* The agreement improves with higher number of epochs which means that the NN has more opportunities to adapt the weights and reduce the loss to the target values. \n","\n","### Overfitting (Overtraining)\n","\n","* Increasing too much the number of epochs may lead to overfitting: \n","  * the NN learns too well the training sample but its performance degrade substantially in an independent sample. \n","* It is required to split the available sample in two parts: training and testing (e.g. 80% and 20%) \n","  * **training** to perform the setting of the weights;\n","  * **testing** to cross-check the performance in an independent sample. \n","\n","* To check if this is the case we can *evaluate* our NN with both the training ad the testing samples. \n","  * If the losses are comparable the NN is ok otherwise if the training losses are much smaller than the testing we had overfitting.\n","  * In this second case if we need more accuracy we need to either increase the training sample or to change the NN design.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uvtyrPzxsRou","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668685060544,"user_tz":-60,"elapsed":404,"user":{"displayName":"Matteo Sani","userId":"10639058540037747059"}},"outputId":"9a974b63-6409-47e4-e0d2-5bccd61b0ca5"},"source":["# evaluate on train and test\n","eval_train = model500.evaluate(train_X, train_y)\n","eval_test = model500.evaluate(test_X, test_y)\n","\n","print(eval_train)\n","print(eval_test)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100/100 [==============================] - 0s 1ms/step - loss: 9.4398e-06\n","26/26 [==============================] - 0s 1ms/step - loss: 1.0648e-05\n","9.439824680157471e-06\n","1.0648057468642946e-05\n"]}]},{"cell_type":"markdown","metadata":{"id":"87lJvt7u5eLr"},"source":["## A Feature Not a Bug\n","\n","* If you ran the previous example you would most likely obtain different results.\n","  * **This is not a bug but a feature of NN**, let's see which are the possible sources for such discrepancies.\n","\n","* **Stochastic learning algorithm**: NN algorithm is stochastic i.e. its behaviour incorporates elements of randomness (beware that stochastic\n","does not mean learning a random model). \n","* Their randomness comes from: \n","  * the *random initial weights*, which allow the model to try learning from a different starting point in the search space each time; \n","  * the *random shuffle of examples during training*, which ensures that each gradient estimate and weight update is slightly different. \n","\n","* The impact is that each time it is run on the same data, it learns a slightly different model and when evaluated, may have a slightly different performance. \n","\n","* You can control randomness by setting the seed used by the pseudorandom number generator: although this is not a good approach in practice:\n","  * **there is no best seed for any algorithm**; \n","  * you need to summarize the performance by fitting multiple times a model on your dataset and averaging its predictions.\n"]}]}