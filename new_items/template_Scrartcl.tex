\documentclass[]{scrartcl}

\title{Choosing Number of Hidden Layers and Nodes in FeedForward Neural Network}
\author{}

\begin{document}

\maketitle
\section{Hyper-parameters Selection}

Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer (the so called \emph{hyper-parameters}), in a feed-forward neural network ? 


%But once this network is initialized, you can iteratively tune the configuration during training using a number of ancillary algorithms; one family of these works by pruning nodes based on (small) values of the weight vector after a certain number of training epochs--in other words, eliminating unnecessary/redundant nodes (more on this below).

Every NN has three types of layers: input, hidden, and output. Creating the NN architecture, therefore, means coming up with values for the number of layers of each type and the number of nodes in each of these layers.

\subsection*{The Input Layer}

Simple, every NN has exactly one of them, no exceptions. With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. %Some NN configurations add one additional node for a bias term.

\subsection*{The Output Layer}

Like the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.

If the NN is a \textbf{regressor}, then the output layer has a single node, otherwise (the NN is a \textbf{classifier}), it also has a single node unless \emph{softmax} is used in which case the output layer has one node per class label in your model.

\subsection*{The Hidden Layers}

There are really two decisions that must be made regarding the hidden layers: how many hidden layers to actually have in the neural network and how many neurons will be in each of these layers. We will first examine how to determine the number of hidden layers to use with the neural network.

\subsubsection*{Hidden Layer Number}
If your data is linearly separable, then you don't need any hidden layers at all. Of course, you don't need a NN to resolve your data either, but it will still do the job.

Beyond that, there's a mountain of commentary on the question of hidden layer configuration in NNs. One issue within this subject on which there is a \textbf{consensus} is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few i.e. \emph{one hidden layer is sufficient for the large majority of problems}. Table~\ref{tab:nn} summarizes the capabilities of neural network architectures with various hidden layers.

%Problems that require two hidden layers are rarely encountered. However, neural networks with two hidden layers can represent functions with any kind of shape. %There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer.

\begin{center}
\begin{table}[htbp]
\begin{tabular}{|p{0.3\linewidth}|p{0.65\linewidth}|}
\hline
\vspace{1mm}
Number of Hidden Layers & \vspace{1mm} Result \vspace{2mm}\\
\hline
\vspace{0.2\baselineskip}
0 & Only capable of representing linear separable functions or decisions. \\
\hline
\vspace{0.2\baselineskip}
1 & Can approximate any function that contains a continuous mapping from one finite space to another.\\
\hline
\vspace{0.5\baselineskip}
2 & Can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy. \\
\hline
\vspace{0.1\baselineskip}
More than 2 & Additional layers can learn complex representations (sort of automatic feature engineering) \\
\hline
\end{tabular}
\caption{Determining the number of hidden layers.}
\label{tab:nn}
\end{table}
\end{center}

\subsubsection*{Hidden Neuron Number}
%Anyway deciding the number of hidden neuron layers is only a small part of the problem. You must also determine how many neurons will be in each of these hidden layers.
%So what about the size of the hidden layer(s) (how many neurons?). 

Deciding the number of neurons in the hidden layers is a very important part of deciding your overall neural network architecture. Though these layers do not directly interact with the external environment, they have a tremendous influence on the final output, hence their number must be carefully considered.

In order to secure the network ability to generalize the number of nodes has to be kept as low as possible. If you have a large excess of nodes, your network becomes a memory bank that can recall the training set to perfection, but does not perform well on samples that was not part of the training set. This is usually referred to as \emph{overfitting}.
%It can also happen when the neural network has so much information processing capacity that the limited amount of information contained in the training set is not enough to train all of the neurons in the hidden layers. 
A second kind of problem can occur with inordinately large number of neurons in the hidden layers, indeed the network training time can increase to the point that it is impossible to adequately train the NN. 

On the other hand using too few neurons in the hidden layers will result in something called underfitting. It occurs when the number of neurons is inadequate to detect the signals in a complicated data set.

Obviously, some compromise must be reached between too many and too few neurons in the hidden layers. There are some empirically derived rules of thumb:
\begin{itemize}
\item the number of hidden neurons should be between the size of the input layer and the size of the output layer;
\item the number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer~\cite{bib:heaton};
\item the number of hidden neurons should be less than twice the size of the input layer~\cite{bib:heaton};
\item a rough approximation can be obtained by the geometric pyramid rule~\cite{bib:master} (e.g. for a three layer network with $n$ input and $m$ output neurons, the hidden layer would have $\sqrt{n\cdot m}$ neurons);
\item overfitting can usually be prevented by keeping the number of neurons below~\cite{bib:hagan}:
\begin{equation}
N_h = \frac{N_s}{\alpha\cdot(N_i+N_o)}
\end{equation}
where $N_i$ = number of input neurons, $N_o$ = number of output neurons, $N_s$ = number of samples in training data set, $\alpha$ = an arbitrary scaling factor usually between $[2-10]$. 
You want to limit the number of free parameters in your model (\emph{degree}) to a small portion of the degrees of freedom in your data which is the number of samples times the  dimensions in each sample or $N_s\cdot(N_i+N_o)$. So $\alpha$ is a way to indicate how general you want your model to be, or how much you want to prevent overfitting (with 10 giving the safest architecture).
\end{itemize}

In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules:
\begin{enumerate}
\item the number of hidden layers equals one; 
\item the number of neurons in that layer is the mean of the neurons in the input and output layers.
\end{enumerate}

\section{Optimization of the Network Configuration}

These rules provide a good starting point to consider. Ultimately, the selection of an architecture for your neural network will come down to \emph{trial and error}. 
But what exactly is meant by trial and error? You do not want to start throwing random numbers of layers and neurons at your network. 

\subsubsection*{Pruning}
The term \emph{pruning} describes a set of techniques to trim network size (by nodes, not layers) to improve computational and sometimes resolution performance. 
The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance. To get a rough idea of which nodes are not important it is enough to look at the weight matrix after training and consider to remove the nodes with the weights very close to zero. 

Obviously, if you use a pruning algorithm during training, then begin with a network configuration that has excess of nodes.

%Put another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single "up-front" (such as a genetic-algorithm-based algorithm), I don't know, though I do know that for now, this two-step optimization is more common.

\subsubsection*{Genetic Algorithms}

Another approach involves a set of techniques called "genetic algorithms" that try a small subset of the potential options (random number of layers and nodes per layer). It then treats this population of options as "parents" that create children by combining/ mutating one or more of the parents much like organisms evolve. The best children and some random ok children are kept in each generation and over generations, the fittest survive.

For about 100 or fewer parameters (such as the choice of the number of layers, types of layers, and the number of neurons per layer), this method is super effective. It can be used to create a number of potential network architectures for each generation and training them partially till the learning curve can be estimated. After a few generations, you may want to consider the point in which the train and validation start to have significantly different error rate (overfitting) as your objective function for choosing children. It may be a good idea to use a very small subset of your data (10-20\%) until you choose a final model to reach a conclusion faster. Also, use a single seed for your network initialization to properly compare the results.

10-50 generations should yield great results for a decent sized network.

\begin{thebibliography}{9}
\bibitem{bib:master} Masters, Timothy, \emph{Practical neural network recipes in C++}, Morgan Kaufmann, 1993.
\bibitem{bib:heaton} J. Heaton, \emph{Introduction to Neural Networks for Java} ($2^{nd}$ edition).
\bibitem{bib:hagan} Hagan, Demuth, Beale, De Jesus, \emph{Neural Network Design}.
\end{thebibliography}
\end{document}
