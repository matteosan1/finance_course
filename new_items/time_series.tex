\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{time\_series}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{end} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2015\PYZhy{}01\PYZhy{}01}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{start} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2007\PYZhy{}01\PYZhy{}01}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{symbols} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SPY}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TLT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSFT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{prices} \PY{o}{=} \PY{n}{ffn}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{n}{symbols}\PY{p}{,} \PY{n}{start}\PY{o}{=}\PY{n}{start}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{n}{end}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{prices}\PY{o}{.}\PY{n}{pct\PYZus{}change}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                 spy       tlt      msft
Date
2007-01-03       NaN       NaN       NaN
2007-01-04  0.002122  0.006063 -0.001675
2007-01-05 -0.007976 -0.004353 -0.005703
2007-01-08  0.004626  0.001793  0.009784
2007-01-09 -0.000850  0.000000  0.001003
{\ldots}              {\ldots}       {\ldots}       {\ldots}
2014-12-24  0.000096  0.005443 -0.006399
2014-12-26  0.003225  0.003711 -0.005401
2014-12-29  0.001343  0.007475 -0.008981
2014-12-30 -0.005366  0.002713 -0.009062
2014-12-31 -0.009923  0.001910 -0.012123

[2014 rows x 3 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{time-series}{%
\section{Time Series}\label{time-series}}

A time series is a sequence where a stochastic process is recorded over
regular time intervals.

Depending on the frequency, a time series can be of yearly (annual
budget), quarterly (expenses), monthly (air traffic), weekly (sales
qty), daily (weather), hourly (stocks price), minutes (inbound calls in
a call canter) or even seconds wise (web traffic).

It is very important to learn how to analyze a time series because it is
the preparatory step before you develop a forecast of the series.
Besides, time series forecasting has enormous commercial significance
because stuff that is important to a business like demand and sales,
number of visitors to a website, stock price etc are essentially time
series data. Time series analysis involves understanding various aspects
about the inherent nature of the series so that you are better informed
to create meaningful and accurate forecasts.

\hypertarget{visualizing-time-series}{%
\subsection{Visualizing Time Series}\label{visualizing-time-series}}

The data for a time series typically stores in .csv files or other
spreadsheet formats and contains two columns: the date and the measured
value. Sometimes it may contains one or more related variables that are
measured for the same time periods.

Clearly the best instrument to deal with time series is the
\(\tt{pandas}\) package. In the following we are going to use a example
dataset counting the total monthly scripts for pharmaceutical products
falling under ATC code A10, \href{here}{a10.csv}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{dateutil}\PY{n+nn}{.}\PY{n+nn}{parser} \PY{k}{import} \PY{n}{parse} 
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a10.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                 \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
               value
date
1991-07-01  3.526591
1991-08-01  3.180891
1991-09-01  3.252221
1991-10-01  3.611003
1991-11-01  3.565869
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib} \PY{k}{as} \PY{n+nn}{mpl}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{k}{def} \PY{n+nf}{plot\PYZus{}df}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tab:red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{n}{title}\PY{p}{,} \PY{n}{xlabel}\PY{o}{=}\PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{n}{ylabel}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{n}{plot\PYZus{}df}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} 
        \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Monthly anti\PYZhy{}diabetic drug sales in Australia from 1992 to 2008.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/pandas/plotting/\_converter.py:129: FutureWarning: Using an implicitly
registered datetime converter for a matplotlib plotting method. The converter
was registered by pandas on import. Future versions of pandas will require you
to explicitly register matplotlib converters.

To register the converters:
        >>> from pandas.plotting import register\_matplotlib\_converters
        >>> register\_matplotlib\_converters()
  warnings.warn(msg, FutureWarning)
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_4_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Since its a monthly time series and follows a certain repetitive pattern
every year, you can plot each year as a separate line in the same plot.
This lets you compare the year wise patterns side-by-side.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{n}{df}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{d}\PY{o}{.}\PY{n}{year} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{df}\PY{o}{.}\PY{n}{date}\PY{p}{]}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{d}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{df}\PY{o}{.}\PY{n}{date}\PY{p}{]}
\PY{n}{years} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{mycolors} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{mpl}\PY{o}{.}\PY{n}{colors}\PY{o}{.}\PY{n}{XKCD\PYZus{}COLORS}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                            \PY{n+nb}{len}\PY{p}{(}\PY{n}{years}\PY{p}{)}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{years}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}        
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{year}\PY{o}{==}\PY{n}{y}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{n}{mycolors}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{y}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{year}\PY{o}{==}\PY{n}{y}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{o}{.}\PY{l+m+mi}{9}\PY{p}{,} 
                 \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{year}\PY{o}{==}\PY{n}{y}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{,} 
                 \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{mycolors}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{xlim}\PY{o}{=}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{,} \PY{n}{ylim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}Drug Sales\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{xlabel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}Month\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{7}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Seasonal Plot of Drug Sales Time Series}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_6_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    There is a steep fall in drug sales every February, rising again in
March, falling again in April and so on. Clearly, the pattern repeats
within a given year, every year.

    \hypertarget{patterns-in-a-time-series}{%
\subsection{Patterns in a time series}\label{patterns-in-a-time-series}}

So far, we have seen the similarities to identify the pattern. Now, how
to find out any deviations from the usual pattern? Any time series may
be split into the following components

\begin{itemize}
\tightlist
\item
  base Level;
\item
  trend: is observed when there is an increasing or decreasing slope
  observed in the time series;
\item
  seasonality: is observed when there is a distinct repeated pattern
  observed between regular intervals due to seasonal factors. It could
  be because of the month of the year, the day of the month, weekdays or
  even time of the day;
\item
  error.
\end{itemize}

It is not mandatory that all time series must have a trend and/or
seasonality. A time series may not have a distinct trend but have a
seasonality. The opposite can also be true. So, a time series may be
imagined as a combination of the trend, seasonality and the error terms.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/guinearice.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
            \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Trend Only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
            \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Seasonality Only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/AirPassengers.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
            \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Trend and Seasonality}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.axes.\_subplots.AxesSubplot at 0x7f490831ce80>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_9_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Another aspect to consider is the cyclic behaviour. It happens when the
rise and fall pattern in the series does not happen in fixed
calendar-based intervals. Care should be taken to not confuse `cyclic'
effect with `seasonal' effect. If the patterns are not of fixed calendar
based frequencies, then it is cyclic. Because, unlike the seasonality,
cyclic effects are typically influenced by the business and other
socio-economic factors.

    \hypertarget{time-series-modelling}{%
\subsection{Time Series Modelling}\label{time-series-modelling}}

In this Section we are going to describe two simple models to describe
time series.

\hypertarget{autoregression-ar}{%
\subsubsection{Autoregression (AR)}\label{autoregression-ar}}

A regression model, such as linear regression, models an output value
based on a linear combination of input values (see CAPM discussion). For
example:

\[\hat{y} = \hat{\alpha} + \hat{\beta}X\] where \(\hat{y}\) is the
prediction, \(\hat{\alpha}\) and \(\hat{\beta}\) are coefficients found
by optimizing the model on training data, and \(X\) is an input value.

This technique can be used on time series too where input variables are
taken as observations at previous time steps, called
\emph{lag variables}.

For example, we can express the value for the next time step \(t\) given
the observations at the last two time steps (\(t-1\) and \(t-2\)). As a
regression model, this would look as follows:

\[Y_{t} = \phi_0 + \phi_1Y_{t-1} + \phi_2Y_{t-2}\]

Because the regression model uses data from the same input variable at
previous time steps, it is referred to as an \emph{autoregression}
(regression of itself). The autoregression (AR) model describes the next
step in the sequence as a linear function of the observations at prior
time steps. A pure Auto Regressive (AR only) model is one where \(Y_t\)
depends only on its own lags. That is, \(Y_t\) is a function of the
`lags of \(Y_t\)'

\[Y_t = \phi_0 + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \epsilon_t\]

where, \(Y_{t-1}\) is the lag-1 of the series, \(\beta_1\) is the
coefficient of lag-1 that the model estimates and \(\phi_0\) is the
intercept term, also estimated by the model. \(\epsilon_t\) is the error
associated to the estimate of the time series at time \(t\) and it is
usually normally distributed (\(\mathcal{N}(0, \sigma^2)\)), also called
\emph{white noise process}.

The notation for the model involves specifying the order of the model
\(p\) as a parameter to the AR function, e.g.~AR(\(p\)). For example,
AR(1) is a first-order autoregression model.

This kind of model can be easily coded in \(\tt{python}\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{AR}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{phi}\PY{p}{)}\PY{p}{:}
    \PY{n}{val} \PY{o}{=} \PY{n}{phi}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{b} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{phi}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{:}
        \PY{n}{val} \PY{o}{+}\PY{o}{=} \PY{n}{b} \PY{o}{*} \PY{n}{x}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
    \PY{k}{return} \PY{n}{val} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{p}{)}

\PY{n}{phi} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.7}\PY{p}{]}
\PY{n}{x} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
\PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
    \PY{n}{x}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{AR}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{phi}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{moving-average-ma}{%
\subsubsection{Moving Average (MA)}\label{moving-average-ma}}

The moving average (MA) instead is a common approach for modeling
univariate time series. It models the next step in the sequence as a
linear function of the residual errors from a mean process at prior time
steps. Beware that a moving average model is different from calculating
the moving average of the time series.

The notation for the model involves specifying the order of the model q
as a parameter to the MA function, e.g.~MA(q). For example, MA(1) is a
first-order moving average model.

\[Y_t = \mu + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q} + \epsilon_t\]
where \(\mu\) is the mean of the series, the
\(\theta_1, \ldots, \theta_q\) are the parameters of the model and the
\(\epsilon_t, \epsilon_{t−1},..., \epsilon_{t−q}\) are the error terms.

Thus, a moving-average model is conceptually a linear regression of the
current value of the series against current and previous (observed)
error terms. Those terms at each point are assumed to be mutually
independent and to come from the same distribution, typically a normal
distribution, with zero mean and constant variance.

\%A pure Moving Average (MA only) model is one where \(Y_t\) depends
only on the lagged forecast errors.
\%\[Y_t = \mu + \phi_1 \epsilon_{t-1} + \phi_2 \epsilon_{t-2} + \ldots + \phi_q \epsilon_{t-q} + \epsilon_t\]
\%where The error terms are the errors of the autoregressive models of
the respective lags. The errors \(\epsilon_t\) and \(\epsilon{t-1}\) are
the errors from the following equations:

\[Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_0 Y_{0} + \epsilon_t\]
\[Y_{t-1} = \phi_2 Y_{t-2} + \phi_3 Y_{t-3} + \ldots + \phi_0 Y_{0} + \epsilon_{t-1}\]

    \hypertarget{stationary-and-non-stationary-time-series}{%
\subsection{Stationary and Non-Stationary Time
Series}\label{stationary-and-non-stationary-time-series}}

A stationary series is one where the values of the series is not a
function of time. That is, the statistical properties of the series like
mean, variance and autocorrelation are constant over time. A stationary
time series is lacking of seasonal effects too.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} plot of stationary and not stationary series}
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{phi}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{phi}\PY{o}{*}\PY{n}{x} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{p}{)}

\PY{n}{phi} \PY{o}{=} \PY{l+m+mf}{0.5}
\PY{n}{x} \PY{o}{=} \PY{p}{[}\PY{n}{f}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{phi}\PY{p}{)}\PY{p}{]}
\PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{)}\PY{p}{:}
    \PY{n}{x}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{phi}\PY{p}{)}\PY{p}{)}

\PY{n}{phi} \PY{o}{=} \PY{l+m+mf}{1.051}
\PY{n}{x1} \PY{o}{=} \PY{p}{[}\PY{n}{f}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{phi}\PY{p}{)}\PY{p}{]}
\PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)}\PY{p}{:}
    \PY{n}{x1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{phi}\PY{p}{)}\PY{p}{)}
    
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Stationary Series}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Non Stationary Series}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{white-noise}{%
\subsubsection{White Noise}\label{white-noise}}

White noise is particular example of stationary series. Its mean and
variance does not change over time, the peculiarity is that the mean is
constant at 0.

If you consider the sound signals in an FM radio as a time series, the
blank sound you hear between the channels is white noise.
Mathematically, a sequence of completely random numbers with mean zero
is a white noise.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{randvals} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{randvals}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Random White Noise}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_17_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    MA series are always stationary since both mean and variance are costant
in time. For the mean we have that

\[\mathbb{E}(\textrm{MA}(q)) = \mu + \sum_{q}\theta_{t-q}\mathbb{E}(\epsilon_{t-q}) = \mu\]
while for the variance we have to remember that the \(\epsilon_t\) are
all independent identical distributed variable with constant variance so
\[\textrm{var(MA(}q)) = \sigma^2 \sum_{q}\theta_{t-q}\]

Contrary AR series are not always stationary. Consider for example an
AR(1) process which is given by:

\[ Y_t = c + \phi_1 Y_{t−1} + \epsilon_t\] where \(\epsilon_{t}\) is a
white noise process with zero mean and constant variance
\(\sigma_{\epsilon}^2\). If we recurse back in time a couple of time we
have

\[Y_t = c + \phi_1 Y_{t−1} + \epsilon_t = c + c\phi_1 + \phi_1^2 Y_{t-2} + \epsilon_t + \phi_1 \epsilon_{t-1} = 
c + c\phi_1 + c\phi_1^2 + \phi_1^3 Y_{t-3} + \epsilon_t + \phi_1 \epsilon_{t-1} + \phi_1^2 \epsilon_{t-2}\]

Imagine to repeat the process \(j\) times we have

\[Y_t = c[\sum_{s=0}^j \phi_1^s Y_{t−1}] + \phi_1^{j+1} Y_{t-j-1} + \sum_{s=0}^j \phi_1^s \epsilon_{t-s}\]

Now imagine to have an infinite series, the first sum becomes (it is a
\emph{geometric progressions})

\[c[\sum_{s=0}^j \phi_1^s Y_{t−1}] \underset{j\rightarrow\infty}{=} \cfrac{c}{1-\phi_1}\]

Also the second term tends to 0 if \(|\phi_1| < 1\) otherwise it is
infinite. So the expectation becomes:

\[
\mathbb{E}(Y_t) =
\begin{cases}
\frac{c}{1-\phi_1} + \mathbb{E}(\sum_{s=0}^{\infty} \phi_1^s \epsilon_{t-s})\quad (\textrm{with } |\phi_1| < 1), \\
\infty + \mathbb{E}(\sum_{s=0}^{\infty} \phi_1^s \epsilon_{t-s})\quad (\textrm{with } |\phi_1| >= 1)
\end{cases}
\] the last term is zero in the first case since \(\epsilon_t\) is a
zero mean distributed random variable. With similar calculation we can
find that the variance is given by

\[\textrm{var}(Y_t) = \mathbb{E}(Y_t^2) − \frac{c^2}{(1-\phi_1)^2} = \frac{\sigma_{\epsilon}^2}{1-\phi_1^2}\],

where \(\sigma_{\epsilon}\) is the standard deviation of the white noise
process. Hence we can conclude that the process is stationary if
\(|\phi_1| < 1\).

    \hypertarget{making-a-non-stationary-series-stationary}{%
\subsubsection{Making a Non Stationary Series
Stationary}\label{making-a-non-stationary-series-stationary}}

It is possible to make nearly any time series stationary by applying a
suitable transformation. This is useful since most statistical
forecasting methods are designed to work on a stationary time series.

You can make series stationary by:

\begin{itemize}
\tightlist
\item
  differencing the series (once or more times);
\item
  take the log of the series;
\item
  take the nth root of the series;
\item
  combination of the above.
\end{itemize}

The most common and convenient method to stationarize the series (and
the only one we are going to look at) is by differencing the series at
least once until it becomes approximately stationary.

If \(Y_t\) is the value at time \(t\), then the first difference of
\(Y = Y_t – Y_{t-1}\). In simpler terms, differencing the series is
nothing but subtracting the next value by the current value. If the
first difference doesn't make a series stationary, you can go for the
second differencing. And so on.

For example, consider the following series: {[}1, 5, 2, 12, 20{]} First
differencing gives: {[}5-1, 2-5, 12-2, 20-12{]} = {[}4, -3, 10, 8{]}
Second differencing gives: {[}-3-4, -10-3, 8-10{]} = {[}-7, -13, -2{]}

SPOSTARE There are various reasons why we have to make a non-stationary
series stationary before forecasting:

\begin{itemize}
\tightlist
\item
  forecasting a stationary series is relatively easy and the forecasts
  are more reliable;
\item
  autoregressive forecasting models (AR models) are essentially linear
  regression models that utilize the lag(s) of the series itself as
  predictors. We know that linear regression works best if the
  predictors (\(X\) variables) are not correlated against each other.
  So, stationarizing the series solves this problem since it removes any
  persistent autocorrelation, thereby making the predictors(lags of the
  series) in the forecasting models nearly independent.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{x1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x1}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{y1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{y1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First difference}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{y}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second difference}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{y1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{testing-stationarity}{%
\subsubsection{Testing Stationarity}\label{testing-stationarity}}

The stationarity of a series sometimes can be established by simply
looking at its plot. Another method could be to split the series into 2
or more contiguous parts and computing the summary statistics like the
mean and variance. If they are quite different then the series is not
likely to be stationary.

Both these two techniques are not rigorous anyway, so a quantitative
method to determine if a given series is stationary or not is needed.
This can be done using statistical tests called \emph{unit root tests}.
There are multiple implementations of such tests like:

\begin{itemize}
\tightlist
\item
  Augmented Dickey Fuller test (ADF Test)
\item
  Kwiatkowski-Phillips-Schmidt-Shin -- KPSS test (trend stationary)
\item
  Philips Perron test (PP Test)
\end{itemize}

In the next Section we will focus on the ADF test.

    \hypertarget{augmented-dickey-fuller-test}{%
\subsection{Augmented Dickey Fuller
Test}\label{augmented-dickey-fuller-test}}

Augmented Dickey Fuller (ADF) test is fundamentally a statistical
significance test. That means, there is a hypothesis testing involved
with a null and a alternate hypothesis and as a result a test statistic
is computed and the corresponding p-values get reported.

It is from the test statistic and the p-value, an inference as to
whether a given series is stationary or not can be made.

\hypertarget{hypothesis-testing}{%
\subsubsection{Hypothesis Testing}\label{hypothesis-testing}}

Let's start with a simple example: imagine that you and your friend play
a game. If a coin lands on heads, you win, if it lands on tails he wins.

Let's say the first two coin tosses landed on tails, meaning your friend
won twice. Should you be worried that he's using a rigged coin ? Well,
the probability of the coin landing on tails two times in a row is 25\%
(0.5*0.5) which is not unlikely. What if the coin landed on tails six
times in a row ? The probability of that occurring is approximately
1.56\% (\(0.5^6\)), which is highly unlikely. At this point, it would be
fair to assume that the coin is rigged. Typically, one would set a
threshold, usually 5\%, to determine if an event occurred by chance or
not (this threshold is usually referred to as \(\alpha\))

To understand hypothesis testing, let's describe some terminology:

\begin{itemize}
\tightlist
\item
  null hypothesis: the hypothesis that there is no significant
  difference between specified samples, any observed difference being
  due to sampling or experimental error. In our example is that the coin
  is a fair coin and that the observations are purely from chance;
\item
  alternative hypothesis: the hypothesis that sample observations are
  influenced by some non-random cause. The alternative hypothesis would
  then be that the coin is not fair, and thus, the observations did not
  happen by chance;
\item
  p-value: the probability of obtaining the observed results of a test,
  assuming that the null hypothesis is correct (i.e.~a smaller p-value
  means that there is stronger evidence in favor of the alternative
  hypothesis). The p-value in the scenario of flipping tails 2 times in
  a row is 25\% and 6 times in a row is 1.56\%;
\item
  \(\alpha\): the significance level; the probability of rejecting the
  null hypothesis when it is true (also known as type 1 error). The
  \(\alpha\) or level of significance would be 5\%.
\end{itemize}

The main rule in determining whether you reject the null hypotesis is
quite simple: \textbf{if the p-value is greater than \(\alpha\), do not
reject the null}. In the case of flipping tails 2 times in a row, we
would not reject the null since \(25\% > 5\%\). However, in the case of
flipping tails 6 times in a row, we would reject the null since
\(1.56\% < 5\%\).

These kind of tests is used to determine how likely or unlikely a
hypothesis is for a given sample of data. The last part of the
statement, \emph{for a given sample of data} is key because more often
than not, you won't be able to get an infinite amount of data or data
that represents the entire population.

Here are the general steps to perform a hypothesis test:

\begin{itemize}
\tightlist
\item
  state your null and alternative hypotheses;
\item
  set your significance level, \(\alpha\). This is typically set at 5\%
  but can be set at other levels depending on the situation and how
  severe it is to committing a type 1 and/or 2 error (wrongly reject the
  null or wrongly accept the null);
\item
  collect sample data and calculate sample statistics;
\item
  calculate the p-value given the sample statistics. Most likely this
  will be done through the t-score or z-score;
\item
  reject or do not reject the null hypothesis.
\end{itemize}

\hypertarget{t-score-z-score}{%
\subsubsection{t-Score, z-Score}\label{t-score-z-score}}

Z-score and t-score are both used in hypothesis testing. The first one
is calculated using the formula \[z = \frac{X-\mu}{\sigma}\] where
\(\sigma\) is the population standard deviation and \(\mu\) is the
population mean. From the definition it is clear that this score can be
used when the standard deviation of the population is known. Furthermore
it is recommended to have a sample size of at least 30.

Conversely t-scores are used when you don't know the population standard
deviation; and you can only make an estimate by using your sample:
\[t = \frac{X – \mu}{s/\sqrt{n}}\] where \(s\) is the standard deviation
of the sample and \(n\) the sample size.

METTERE IN FONDO Everitt, B. S.; Skrondal, A. (2010), The Cambridge
Dictionary of Statistics, Cambridge University Press. Gonick, L. (1993).
The Cartoon Guide to Statistics. HarperPerennial. Meier et. al.~(2014).
Applied Statistics for Public and Nonprofit Administration. Cengage
Learning. SoSci. (1999). Article posted on Vermont Tech website.
Retrieved 11/20/2016 from
https://simon.cs.vt.edu/SoSci/converted/T-Dist/.

\hypertarget{unit-root-test}{%
\subsubsection{Unit Root Test}\label{unit-root-test}}

Unit root is a characteristic of a time series that makes it
non-stationary. Technically speaking, a unit root is said to exist in a
time series when the value \(\phi_1 = 1\) in an autoregressive model

\[Y_t = c + \phi_1 Y_{t-1} + \epsilon_t\] where, \(Y_t\) is the value of
the time series at time \(t\).

As we have seen the presence of a unit root means the time series is
non-stationary and more generally, the number of unit roots contained in
the series corresponds to the number of differencing operations required
to make the series stationary.

\hypertarget{dickey-fuller-test}{%
\subsubsection{Dickey-Fuller Test}\label{dickey-fuller-test}}

Before going into \emph{augmented} version let's first understand what
is the Dickey-Fuller test. Imagine a simple AR(1) model

\[Y_t = \phi Y_{t − 1} + \epsilon_t\] where \(Y_t\) is the variable of
interest, \(t\) is the time index and \(\epsilon_t\) is the error term.
A unit root is present if \(\phi = 1\) and, as we have already seen, the
model would be non-stationary in this case.

\[\textrm{null hypothesis (H0)}:= \phi =1\]

The test consist of performing an autoregression whose model can be
written as

\[\Delta Y_t = (\phi − 1) Y_{t − 1} + \epsilon_t = \delta Y_{t − 1} + \epsilon_t\]

where \(\Delta\) represents the first difference. Once this model has
been estimated, testing for a unit root is equivalent to test for
\(\delta = 0\). If the null hypothesis is not rejected, the series is
taken to be non-stationary.

\hypertarget{augmented-dickey-fuller-adf-test}{%
\subsubsection{Augmented Dickey Fuller (ADF)
Test}\label{augmented-dickey-fuller-adf-test}}

As the name suggest, the ADF test is an \emph{augmented} version of the
Dickey Fuller test. The ADF test expands the Dickey-Fuller test equation
to include high order regressive process in the model

\[\Delta Y_t = \delta Y_{t-1} + \delta_1 \Delta Y_{t-1} + \cdots + \delta_{p-1} \Delta Y_{t-p+1} + \epsilon_t\]

If you notice, we have only added more differencing terms, while the
rest of the equation remains the same however the null hypothesis
however is still the same as the Dickey Fuller test.

A key point to remember here is: since the null hypothesis assumes the
presence of unit root, the p-value obtained should be less than the
significance level (say 0.05) in order to reject the null hypothesis,
thereby, inferring that the series is stationary.

\hypertarget{adf-test-in-python}{%
\subsubsection{ADF Test in Python}\label{adf-test-in-python}}

The \texttt{statsmodel} package provides a reliable implementation of
the ADF test via the \(\tt{adfuller()}\) function in
\texttt{statsmodels.tsa.stattools}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{adfuller}

\PY{n}{result} \PY{o}{=} \PY{n}{adfuller}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{values}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ADF Statistic: }\PY{l+s+si}{\PYZob{}result[0]\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value: }\PY{l+s+si}{\PYZob{}result[1]\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{for} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o+ow}{in} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Critial Values:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{   }\PY{l+s+si}{\PYZob{}key\PYZcb{}}\PY{l+s+s1}{, }\PY{l+s+si}{\PYZob{}value\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
ADF Statistic: 3.145185689306739
p-value: 1.0
Critial Values:
   1\%, -3.465620397124192
Critial Values:
   5\%, -2.8770397560752436
Critial Values:
   10\%, -2.5750324547306476
    \end{Verbatim}

    \hypertarget{autocorrelation-and-partial-autocorrelation-functions}{%
\subsection{Autocorrelation and Partial Autocorrelation
Functions}\label{autocorrelation-and-partial-autocorrelation-functions}}

\hypertarget{autocorrelation}{%
\subsubsection{Autocorrelation}\label{autocorrelation}}

Autocorrelation is simply the correlation of a series with its own
\emph{lags} (the previous values of the series). If a series is
significantly autocorrelated, that means, the lags may be helpful in
predicting the current value.

We can use statistical measures to calculate the correlation between the
output variable and values at previous time steps (i.e.~at various
different lags). Again, because the correlation is calculated between
the variable and itself at previous time steps, it is called an
\emph{autocorrelation}.

    \hypertarget{lag-plots}{%
\subsubsection{Lag Plots}\label{lag-plots}}

A lag plot is a scatter plot of a time series against a lag of itself.
It is normally used to check for autocorrelation. If there is any
pattern existing in the series like the one you see below, the series is
autocorrelated. If there is no such pattern, the series is likely to be
random white noise.

\texttt{pandas} provides a built-in plot to do exactly this, called the
\textbackslash{}texttt\{lag\_plot()\} function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{from} \PY{n+nn}{pandas}\PY{n+nn}{.}\PY{n+nn}{plotting} \PY{k}{import} \PY{n}{lag\PYZus{}plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ytick.left}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{k+kc}{False}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axes.titlepad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{24}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{ax} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{axes}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{lag\PYZus{}plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{lag}\PY{o}{=}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{firebrick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lag }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lag Plots of Drug Sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+m+mf}{1.05}\PY{p}{)}    
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    A similar type of quick check thatn can be done is to directly calculate
the correlation between the observation and lag variable. This
correlation can be calculated easily using the \texttt{corr()} function
on the \texttt{DataFrame} of the lagged dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{concat}

\PY{n}{dataframe} \PY{o}{=} \PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{dataframe}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t\PYZhy{}4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{result} \PY{o}{=} \PY{n}{dataframe}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
          t-4         t
t-4  1.000000  0.878839
t    0.878839  1.000000
    \end{Verbatim}

    This is a good confirmation for the plot above since it shows a strong
positive correlation (0.88) between the observation and the lag-4 value.

    \hypertarget{autocorrelation-plots}{%
\subsubsection{Autocorrelation Plots}\label{autocorrelation-plots}}

The autocorrelation for an observation and a lag is comprised of both
the direct correlation and indirect correlations. These indirect
correlations are a linear function of the correlation of the
observation, with observations at intermediate time steps.

A plot of the autocorrelation of a time series by lag is called the
AutoCorrelation Function, or the acronym ACF. This plot can also be
called a correlogram.

In order to plot the autocorrelation coefficient for each lag variable,
\texttt{pandas} provides a built-in plot called the
\textt{autocorrelation_plot()} function.

The plot provides the lag number along the \(x\)-axis and the
correlation coefficient value between -1 and 1 on the \(y\)-axis. The
plot also includes solid and dashed lines that indicate the 95\% and
99\% confidence interval for the correlation values. Correlation values
above these lines are more significant than those below the line,
providing a threshold or cutoff for selecting more relevant lag values.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pandas}\PY{n+nn}{.}\PY{n+nn}{plotting} \PY{k}{import} \PY{n}{autocorrelation\PYZus{}plot}

\PY{n}{autocorrelation\PYZus{}plot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The \texttt{statsmodels} library also provides a version of the plot in
the \textbackslash{}texttt\{plot\_acf()\} function as a line plot.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}acf}

\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{acf}

\PY{n+nb}{print} \PY{p}{(}\PY{n}{acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{nlags}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[1.         0.92056815 0.88782519 0.85385862 0.84052841 0.82523769
 0.79629658 0.77950157 0.75953251 0.74337588 0.74521347 0.74134847
 0.78031252 0.71424686 0.68014097 0.65401657 0.63791893 0.62349882
 0.60171747 0.58230335 0.5638103 ]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_34_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{partial-autocorrelation-function}{%
\subsubsection{Partial Autocorrelation
Function}\label{partial-autocorrelation-function}}

Partial Autocorrelation function (PACF) also conveys similar information
to ACF but it only deals with the pure correlation of a series and its
lag, excluding the correlation contributions from the intermediate lags.

The partial autocorrelation of lag-\(k\) of a series is the coefficient
of that lag in the autoregression equation of \(Y\). For example, if
\(Y_t\) is the current series and \(Y_{t-k}\) is the lag-\(k\) of \(Y\),
then the partial autocorrelation of lag-3 \((Y_{t-3})\) is the
coefficient \(\phi_3\) of \(Y_{t-3}\) in the following equation:

\[Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \phi_3 Y_{t-3} + \ldots\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{pacf}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}pacf}

\PY{n+nb}{print} \PY{p}{(}\PY{n}{pacf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{nlags}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}pacf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[ 1.          0.92510297  0.28297106  0.0759758   0.16921494  0.09370324
 -0.06396075  0.0560044   0.01650882  0.00431904  0.17496764  0.08742467
  0.41635623 -0.63684013 -0.15223434  0.10337984 -0.10246178  0.04619914
  0.26331492 -0.06447131 -0.05881505]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_36_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{intuition-for-acf-and-pacf-plots}{%
\subsection{Intuition for ACF and PACF
Plots}\label{intuition-for-acf-and-pacf-plots}}

Plots of the autocorrelation function and the partial autocorrelation
function for a time series tell a very different story. We can use the
intuition for ACF and PACF above to explore some thought experiments.

\hypertarget{autoregression-intuition}{%
\subsubsection{Autoregression
Intuition}\label{autoregression-intuition}}

Consider a time series that was generated by an autoregression (AR)
process with a lag-\(k\).

We know that the ACF describes the autocorrelation between an
observation and another observation at a prior time step that includes
direct and indirect dependence information. This means we would expect
the ACF for the AR(\(k\)) time series to be strong to a lag-\(k\) and
the inertia of that relationship would carry on to subsequent lag
values, trailing off at some point as the effect was weakened.

We know that the PACF only describes the direct relationship between an
observation and its lag. This would suggest that there would be no
correlation for lag values beyond \(k\).

This is exactly the expectation of the ACF and PACF plots for an
AR(\(k\)) process.

\hypertarget{moving-average-intuition}{%
\subsubsection{Moving Average
Intuition}\label{moving-average-intuition}}

Consider a time series that was generated by a moving average (MA)
process with a lag-\(k\).

Remember that the moving average process is an autoregression model of
the time series of residual errors from prior predictions. Another way
to think about the moving average model is that it corrects future
forecasts based on errors made on recent forecasts.

We would expect the ACF for the MA(\(k\)) process to show a strong
correlation with recent values up to the lag-\(k\), then a sharp decline
to low or no correlation. By definition, this is how the process was
generated.

For the PACF, we would expect the plot to show a strong relationship to
the lag and a trailing off of correlation from the lag onwards.

Again, this is exactly the expectation of the ACF and PACF plots for an
MA(\(k\)) process.

    \hypertarget{cointegration}{%
\subsection{Cointegration}\label{cointegration}}

Cointegration tests identify scenarios where two or more non-stationary
time series are integrated together in a way that they cannot deviate
from equilibrium in the long term. They are used to identify the
long-term relationships between two or more sets of variables. The
concept was first introduced by Robert Engle and Clive Granger, in 1987.

Before the introduction of cointegration tests, economists relied on
linear regressions to find the relationship between several time series
processes. However, Granger and Newbold argued that linear regression
was an incorrect approach for analyzing time series due to the
possibility of producing spurious correlation.

A spurious correlation occurs when two or more associated variables are
deemed causally related due to either a coincidence or an unknown third
factor. A possible result is a misleading statistical relationship
between several time series variables.

\hypertarget{engle-granger-two-step-method}{%
\subsubsection{Engle-Granger Two-Step
Method}\label{engle-granger-two-step-method}}

There are various methods of testing for cointegration. Among them the
Engle-Granger Two-Step method starts by creating residuals based on the
static regression of the two series and then testing these residuals for
the presence of unit roots (it uses the Augmented Dickey-Fuller Test
(ADF) to test for the stationarity of the residual series). If the time
series are cointegrated, the Engle-Granger method will show the
stationarity of the residuals.

Let's see an example involving the time series of the number of Google
searches about Milan and Inter football clubs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{milan\PYZus{}inter.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Inter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{milan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Milan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{adfuller}

\PY{c+c1}{\PYZsh{} regress one series against the other}
\PY{n}{ls} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{milan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{res} \PY{o}{=} \PY{n}{ls}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the residuals}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{res}\PY{o}{.}\PY{n}{resid}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} perform an ADF test on the residuals}
\PY{n}{r} \PY{o}{=} \PY{n}{adfuller}\PY{p}{(}\PY{n}{res}\PY{o}{.}\PY{n}{resid}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test statistic: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}value: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alpha 5}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{ (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
test statistic: -6.105445934885
p-value: 9.609589483491024e-08
alpha 5\% (-2.8728086526320302)
    \end{Verbatim}

    The resulting p-value indicates that the residual time series is
stationary so the two initial series are cointegrated.

    \hypertarget{granger-causality}{%
\subsubsection{Granger Causality}\label{granger-causality}}

Granger causality test is used to determine if one time series will be
useful to forecast another. It is based on the idea that if \(X\)
Granger-causes \(Y\), then the forecast of \(Y\) based on previous
values of \(Y\) AND the previous values of \(X\) should outperform the
forecast of \(Y\) based on previous values of \(Y\) alone.

So, understand that Granger causality should not be used to test if a
lag of \(Y\) causes \(Y\). Instead, it is generally used on exogenous
(not \(Y\) lag) variables only.

A nice implementation of this test is available in the
\texttt{statsmodel} package. It accepts a 2D array with 2 columns as the
main argument. The values are in the first column and the predictor
(\(X\)) is in the second column. The second argument \texttt{maxlag}
says till how many lags of \(Y\) should be included in the test.

The null hypothesis is: the series in the second column, does not
Granger-cause the series in the first. If the p-values are less than a
significance level then you can reject the null hypothesis and conclude
that the said lag of \(X\) is indeed useful.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{grangercausalitytests}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{n}{gc\PYZus{}res} \PY{o}{=} \PY{n}{grangercausalitytests}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{milan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Granger Causality
number of lags (no zero) 1
ssr based F test:         F=5.5471  , p=0.0193  , df\_denom=257, df\_num=1
ssr based chi2 test:   chi2=5.6119  , p=0.0178  , df=1
likelihood ratio test: chi2=5.5522  , p=0.0185  , df=1
parameter F test:         F=5.5471  , p=0.0193  , df\_denom=257, df\_num=1

Granger Causality
number of lags (no zero) 2
ssr based F test:         F=2.5467  , p=0.0803  , df\_denom=254, df\_num=2
ssr based chi2 test:   chi2=5.1938  , p=0.0745  , df=2
likelihood ratio test: chi2=5.1424  , p=0.0764  , df=2
parameter F test:         F=2.5467  , p=0.0803  , df\_denom=254, df\_num=2

Granger Causality
number of lags (no zero) 3
ssr based F test:         F=1.6183  , p=0.1856  , df\_denom=251, df\_num=3
ssr based chi2 test:   chi2=4.9901  , p=0.1725  , df=3
likelihood ratio test: chi2=4.9425  , p=0.1761  , df=3
parameter F test:         F=1.6183  , p=0.1856  , df\_denom=251, df\_num=3

Granger Causality
number of lags (no zero) 4
ssr based F test:         F=2.0213  , p=0.0920  , df\_denom=248, df\_num=4
ssr based chi2 test:   chi2=8.3788  , p=0.0786  , df=4
likelihood ratio test: chi2=8.2451  , p=0.0830  , df=4
parameter F test:         F=2.0213  , p=0.0920  , df\_denom=248, df\_num=4
    \end{Verbatim}

    \hypertarget{time-series-forecasting}{%
\section{Time Series Forecasting}\label{time-series-forecasting}}

Forecasting is the next natural step in the analysis of a time series,
where you want to predict the future values the series is going to take.

Forecasting a time series (like demand and sales) is often of tremendous
commercial value. In most manufacturing companies, it drives the
fundamental business planning, procurement and production activities.
Any errors in the forecasts will ripple down throughout the supply chain
or any business context for that matter. So it's important to get the
forecasts accurate in order to save on costs and is critical to success.

Beyond this example, the techniques and concepts behind time series
forecasting are applicable in any business.

Forecasting a time series can be broadly divided into two types. If you
use only the previous values of the time series to predict its future
values, it is called \emph{Univariate Time Series Forecasting}. If
instead you use predictors other than the series (a.k.a exogenous
variables) to forecast it is called
\emph{Multi Variate Time Series Forecasting}.

In the following we are going to, briefly, show various classical time
series forecasting models.

\hypertarget{autoregression-ar}{%
\subsubsection{Autoregression (AR)}\label{autoregression-ar}}

In the previous Chapter we have seen that an autoregression (AR) method
models the next step in the sequence as a linear function of the
observations at prior time steps.

The usage of an AR model is suitable for univariate time series without
trend and seasonal components.

The correlation statistics can also help to choose which lag variables
will be useful in a model and which will not. Interestingly, if all lag
variables show low or no correlation with the output variable, then it
suggests that the time series problem may not be predictable. This can
be very useful when getting started on a new dataset.

We could calculate the linear regression model manually using the
LinearRegession class in scikit-learn and manually specify the lag input
variables to use. Alternately, the
\textbackslash{}texttt\{statsmodels.tsa.ar\_model\} library provides an
autoregression model where you must specify an appropriate lag value and
trains a linear regression model. It is provided in the \texttt{AutoReg}
class. We can use this model by first creating the model and then
calling \texttt{fit()} to train it on our dataset. This returns an
AutoRegResults object.

Once fit, we can use the model to make a prediction by calling the
\texttt{predict()} function for a number of observations in the future.
This creates 1 7-day forecast, which is different from the persistence
example above.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{ar\PYZus{}model} \PY{k}{import} \PY{n}{AutoReg}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}\PY{p}{,} \PY{n}{seed}

\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}

\PY{n}{model} \PY{o}{=} \PY{n}{AutoReg}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[100.55071444] 99.56135786477837
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{ar\PYZus{}model} \PY{k}{import} \PY{n}{AutoReg}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
\PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{sqrt}

\PY{n}{series} \PY{o}{=} \PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daily\PYZhy{}min\PYZhy{}temperatures.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{squeeze}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{X} \PY{o}{=} \PY{n}{series}\PY{o}{.}\PY{n}{values}
\PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{:}\PY{p}{]}
\PY{c+c1}{\PYZsh{} train autoregression}
\PY{n}{model} \PY{o}{=} \PY{n}{AutoReg}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{29}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficients: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{params}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make predictions}
\PY{n}{predictions} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{start}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{dynamic}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}\PY{p}{)}\PY{p}{:}
	\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{, expected=}\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{predictions}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{rmse} \PY{o}{=} \PY{n}{sqrt}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test RMSE: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{rmse}\PY{p}{)}
\PY{c+c1}{\PYZsh{} plot results}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/ar\_model.py:252: FutureWarning: The parameter names
will change after 0.12 is released. Set old\_names to False to use the new names
now. Set old\_names to True to use the old names.
  FutureWarning,
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Coefficients: [ 5.57543506e-01  5.88595221e-01 -9.08257090e-02  4.82615092e-02
  4.00650265e-02  3.93020055e-02  2.59463738e-02  4.46675960e-02
  1.27681498e-02  3.74362239e-02 -8.11700276e-04  4.79081949e-03
  1.84731397e-02  2.68908418e-02  5.75906178e-04  2.48096415e-02
  7.40316579e-03  9.91622149e-03  3.41599123e-02 -9.11961877e-03
  2.42127561e-02  1.87870751e-02  1.21841870e-02 -1.85534575e-02
 -1.77162867e-03  1.67319894e-02  1.97615668e-02  9.83245087e-03
  6.22710723e-03 -1.37732255e-03]
predicted=11.871275, expected=12.900000
predicted=13.053794, expected=14.600000
predicted=13.532591, expected=14.000000
predicted=13.243126, expected=13.600000
predicted=13.091438, expected=13.500000
predicted=13.146989, expected=15.700000
predicted=13.176153, expected=13.000000
Test RMSE: 1.225
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_46_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{moving-average-ma}{%
\subsubsection{Moving Average (MA)}\label{moving-average-ma}}

The moving average (MA) method models the next step in the sequence as a
linear function of the residual errors from a mean process at prior time
steps. These models are suitable to represent univariate time series
without trend and seasonal components.

We can use the \texttt{ARIMA} class to create an MA model and setting a
zeroth-order AR model (we must specify the order of the MA model in the
order argument). Like before we are going to define a dummy series and
try to fit a first-order MA model to it.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima}\PY{n+nn}{.}\PY{n+nn}{model} \PY{k}{import} \PY{n}{ARIMA}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}

\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}

\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.50521534] 0.6881898080477126
    \end{Verbatim}

    \hypertarget{autoregressive-moving-average-arma}{%
\subsubsection{Autoregressive Moving Average
(ARMA)}\label{autoregressive-moving-average-arma}}

The Autoregressive Moving Average (ARMA) method models the next step in
the sequence as a linear function of the observations and residual
errors at prior time steps. This model combines both Autoregression (AR)
and Moving Average (MA) models.

The notation for the model involves specifying the order for the
AR(\(p\)) and MA(\(q\)) models as parameters to an ARMA function,
e.g.~ARMA(\(p\), \(q\)). An ARIMA model can be used to develop AR or MA
models.

The method is suitable for univariate time series without trend and
seasonal components.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{92}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima}\PY{n+nn}{.}\PY{n+nn}{model} \PY{k}{import} \PY{n}{ARIMA}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}

\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}

\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[0.52629796] 0.4950722507160109
    \end{Verbatim}

    \hypertarget{autoregressive-integrated-moving-average-arima}{%
\subsubsection{Autoregressive Integrated Moving Average
(ARIMA)}\label{autoregressive-integrated-moving-average-arima}}

The Autoregressive Integrated Moving Average (ARIMA) method models the
next step in the sequence as a linear function of the differenced
observations and residual errors at prior time steps.

It combines both Autoregression (AR) and Moving Average (MA) models as
well as a differencing pre-processing step of the sequence to make the
sequence stationary, called integration (the I in the name).

Any `non-seasonal' time series that exhibits patterns and is not a
random white noise can be modeled with ARIMA models.

An ARIMA model is characterized by 3 terms: \(p\), \(d\), \(q\) where,

\begin{itemize}
\tightlist
\item
  \(p\) is the order of the AR term;
\item
  \(q\) is the order of the MA term;
\item
  \(d\) is the number of differencing required to make the time series
  stationary.
\end{itemize}

\hypertarget{determination-of-d-term}{%
\subsubsection{\texorpdfstring{Determination of \(d\)
Term}{Determination of d Term}}\label{determination-of-d-term}}

The first step to build an ARIMA model is to make the time series
stationary, because, the term `Auto Regressive' in ARIMA means it is a
linear regression model that uses its own lags as predictors. Linear
regression models, as you know, work best when the predictors are not
correlated and are independent of each other.

The most common approach to make a series stationary is to difference
it. That is, subtract the previous value from the current value.
Sometimes, depending on the complexity of the series, more than one
differencing may be needed.

The value of \(d\), therefore, is the minimum number of differencing
needed to make the series stationary. And if the time series is already
stationary, then \(d = 0\). On the other hand you need to be careful to
not over-difference the series. Because, an over differenced series may
still be stationary, which in turn will affect the model parameters.

The right order of differencing is the minimum differencing required to
get a near-stationary series which roams around a defined mean and the
ACF plot reaches to zero fairly quick.

If the autocorrelations are positive for many number of lags (10 or
more), then the series needs further differencing. On the other hand, if
the lag-1 autocorrelation itself is too negative, then the series is
probably over-differenced.

In the event, you can't really decide between two orders of
differencing, then go with the order that gives the least standard
deviation in the differenced series.

Let's see how to do it with an example.

First, I am going to check if the series is stationary using the
Augmented Dickey Fuller test because, you need differencing only if the
series is non-stationary.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{93}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{adfuller}
\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{log}

\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/wwwusage.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                 \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{result} \PY{o}{=} \PY{n}{adfuller}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ADF Statistic: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
ADF Statistic: -2.464240
p-value: 0.124419
    \end{Verbatim}

    Since p-value is greater than the significance level, let's difference
the series and see how the autocorrelation plot looks like

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{97}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Order Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2nd Order Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For the above series, the time series reaches stationarity with two
orders of differencing. But on looking at the autocorrelation plot for
the 2nd differencing the lag goes into the far negative zone fairly
quick, which indicates, the series might have been over differenced.

So, I am going to tentatively fix the order of differencing as 1 even
though the series is not perfectly stationary (weak stationarity).

    \hypertarget{determination-of-p-term}{%
\subsubsection{\texorpdfstring{Determination of \(p\)
Term}{Determination of p Term}}\label{determination-of-p-term}}

The next step is to identify if the model needs any AR terms. You can
find out the required number of AR terms by inspecting the Partial
Autocorrelation (PACF) plot.

Any autocorrelation in a stationarized series can be rectified by adding
enough AR terms. So, we initialy take the order of AR term to be equal
to as many lags that crosses the significance limit in the PACF plot.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{98}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{ylim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}pacf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_57_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    You can observe that the PACF lag-1 is quite significant since is well
above the significance line. Lag-2 turns out to be significant as well,
slightly managing to cross the significance limit (blue region). But I
am going to be conservative and tentatively fix the \(p\) as 1.

\hypertarget{determination-of-q-term}{%
\subsubsection{\texorpdfstring{Determination of \(q\)
Term}{Determination of q Term}}\label{determination-of-q-term}}

Just like how we looked at the PACF plot for the number of AR terms, you
can look at the ACF plot for the number of MA terms. An MA term is
technically, the error of the lagged forecast.

The ACF tells how many MA terms are required to remove any
autocorrelation in the stationarized series.

Let's see the autocorrelation plot of the differenced series.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{99}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{ylim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.2}\PY{p}{)}\PY{p}{)}
\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_59_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Couple of lags are well above the significance line. So, let's
tentatively fix \(q\) as 2. When in doubt, go with the simpler model
that sufficiently explains the \(Y\).

\hypertarget{how-to-handle-if-a-time-series-is-slightly-under-or-over-differenced}{%
\subsubsection{How to handle if a time series is slightly under or over
differenced}\label{how-to-handle-if-a-time-series-is-slightly-under-or-over-differenced}}

It may so happen that your series is slightly under differenced, that
differencing it one more time makes it slightly over-differenced.

If your series is slightly under differenced, adding one or more
additional AR terms usually makes it up. Likewise, if it is slightly
over-differenced, try adding an additional MA term.

\hypertarget{how-to-build-the-arima-model}{%
\subsubsection{How to build the ARIMA
Model}\label{how-to-build-the-arima-model}}

Now that you've determined the values of \(p\), \(d\) and \(q\), you
have everything needed to fit the ARIMA model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{108}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima\PYZus{}model} \PY{k}{import} \PY{n}{ARIMA}

\PY{c+c1}{\PYZsh{} 1,1,2 ARIMA Model}
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                             ARIMA Model Results
==============================================================================
Dep. Variable:                D.value   No. Observations:                   99
Model:                 ARIMA(1, 1, 2)   Log Likelihood                -253.790
Method:                       css-mle   S.D. of innovations              3.119
Date:                Wed, 27 Jan 2021   AIC                            517.579
Time:                        15:50:35   BIC                            530.555
Sample:                             1   HQIC                           522.829

================================================================================
=
                    coef    std err          z      P>|z|      [0.025
0.975]
--------------------------------------------------------------------------------
-
const             1.1202      1.290      0.868      0.385      -1.409
3.649
ar.L1.D.value     0.6351      0.257      2.469      0.014       0.131
1.139
ma.L1.D.value     0.5287      0.355      1.489      0.136      -0.167
1.224
ma.L2.D.value    -0.0010      0.321     -0.003      0.998      -0.631
0.629
                                    Roots
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.5746           +0.0000j            1.5746            0.0000
MA.1           -1.8850           +0.0000j            1.8850            0.5000
MA.2          544.9014           +0.0000j          544.9014            0.0000
-----------------------------------------------------------------------------
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/arima\_model.py:472: FutureWarning:
statsmodels.tsa.arima\_model.ARMA and statsmodels.tsa.arima\_model.ARIMA have
been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .
between arima and model) and
statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.

statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and
is both well tested and maintained.

To silence this warning and continue using ARMA and ARIMA until they are
removed, use:

import warnings
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARMA',
                        FutureWarning)
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARIMA',
                        FutureWarning)

  warnings.warn(ARIMA\_DEPRECATION\_WARN, FutureWarning)
    \end{Verbatim}

    The model summary reveals a lot of information. The table in the middle
is the coefficients table where the values under `coef' are the weights
of the respective terms.

Notice here the coefficient of the MA2 term is close to zero and the
p-Value in `P\textgreater{}\textbar{}z\textbar{}' column is highly
insignificant. It should ideally be less than 0.05 for the respective
term to be significant.

So, let's rebuild the model without the MA2 term.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{109}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                             ARIMA Model Results
==============================================================================
Dep. Variable:                D.value   No. Observations:                   99
Model:                 ARIMA(1, 1, 1)   Log Likelihood                -253.790
Method:                       css-mle   S.D. of innovations              3.119
Date:                Wed, 27 Jan 2021   AIC                            515.579
Time:                        15:50:50   BIC                            525.960
Sample:                             1   HQIC                           519.779

================================================================================
=
                    coef    std err          z      P>|z|      [0.025
0.975]
--------------------------------------------------------------------------------
-
const             1.1205      1.286      0.871      0.384      -1.400
3.641
ar.L1.D.value     0.6344      0.087      7.317      0.000       0.464
0.804
ma.L1.D.value     0.5297      0.089      5.932      0.000       0.355
0.705
                                    Roots
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.5764           +0.0000j            1.5764            0.0000
MA.1           -1.8879           +0.0000j            1.8879            0.5000
-----------------------------------------------------------------------------
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/arima\_model.py:472: FutureWarning:
statsmodels.tsa.arima\_model.ARMA and statsmodels.tsa.arima\_model.ARIMA have
been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .
between arima and model) and
statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.

statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and
is both well tested and maintained.

To silence this warning and continue using ARMA and ARIMA until they are
removed, use:

import warnings
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARMA',
                        FutureWarning)
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARIMA',
                        FutureWarning)

  warnings.warn(ARIMA\_DEPRECATION\_WARN, FutureWarning)
    \end{Verbatim}

    The model AIC has reduced, which is good. The p-values of the AR1 and
MA1 terms have improved and are highly significant
(\textless{}\textless{} 0.05). Let's plot the residuals to ensure there
are no patterns (that is, look for constant mean and variance).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{110}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{residuals} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{resid}\PY{p}{)}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{residuals}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuals}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{residuals}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The residual errors seem fine with near zero mean and uniform variance.
Let's plot the actuals against the fitted values using plot\_predict().

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{plot\PYZus{}predict}\PY{p}{(}\PY{n}{dynamic}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{how-to-do-find-the-optimal-arima-model-manually-using-out-of-time-cross-validation}{%
\subsubsection{How to do find the optimal ARIMA model manually using
Out-of-Time Cross
validation}\label{how-to-do-find-the-optimal-arima-model-manually-using-out-of-time-cross-validation}}

In Out-of-Time cross-validation, you take few steps back in time and
forecast into the future to as many steps you took back. Then you
compare the forecast against the actuals.

To do out-of-time cross-validation, you need to create the training and
testing dataset by splitting the time series into 2 contiguous parts in
approximately 75:25 ratio or a reasonable proportion based on time
frequency of series.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{112}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{acf}

\PY{n}{train} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{85}\PY{p}{]}
\PY{n}{test} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{[}\PY{l+m+mi}{85}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    You can now build the ARIMA model on training dataset, forecast and plot
it.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} model = ARIMA(train, order=(3,2,1))  }
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}  
\PY{n}{fitted} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  

\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{fc}\PY{p}{,} \PY{n}{se}\PY{p}{,} \PY{n}{conf} \PY{o}{=} \PY{n}{fitted}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 95\PYZpc{} conf}

\PY{c+c1}{\PYZsh{} Make as pandas series}
\PY{n}{fc\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fc\PYZus{}series}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{lower\PYZus{}series}\PY{p}{,} \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast vs Actuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/arima\_model.py:472: FutureWarning:
statsmodels.tsa.arima\_model.ARMA and statsmodels.tsa.arima\_model.ARIMA have
been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .
between arima and model) and
statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.

statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and
is both well tested and maintained.

To silence this warning and continue using ARMA and ARIMA until they are
removed, use:

import warnings
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARMA',
                        FutureWarning)
warnings.filterwarnings('ignore', 'statsmodels.tsa.arima\_model.ARIMA',
                        FutureWarning)

  warnings.warn(ARIMA\_DEPRECATION\_WARN, FutureWarning)
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the chart, the ARIMA(1,1,1) model seems to give a directionally
correct forecast. And the actual observed values lie within the 95\%
confidence band. That seems fine.

But each of the predicted forecasts is consistently below the actuals.
That means, by adding a small constant to our forecast, the accuracy
will certainly improve. So, there is definitely scope for improvement.

So, what I am going to do is to increase the order of differencing to
two, that is set \(d=2\) and iteratively increase \(p\) to up to 5 and
then \(q\) up to 5 to see which model gives least AIC and also look for
a chart that gives closer actuals and forecasts.

While doing this, I keep an eye on the p-values of the AR and MA terms
in the model summary. They should be as close to zero, ideally, less
than 0.05.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{114}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Build Model}
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}  
\PY{n}{fitted} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  
\PY{n+nb}{print}\PY{p}{(}\PY{n}{fitted}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{fc}\PY{p}{,} \PY{n}{se}\PY{p}{,} \PY{n}{conf} \PY{o}{=} \PY{n}{fitted}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 95\PYZpc{} conf}

\PY{c+c1}{\PYZsh{} Make as pandas series}
\PY{n}{fc\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fc\PYZus{}series}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{lower\PYZus{}series}\PY{p}{,} \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast vs Actuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                             ARIMA Model Results
==============================================================================
Dep. Variable:               D2.value   No. Observations:                   83
Model:                 ARIMA(3, 2, 1)   Log Likelihood                -214.248
Method:                       css-mle   S.D. of innovations              3.153
Date:                Wed, 27 Jan 2021   AIC                            440.497
Time:                        15:53:48   BIC                            455.010
Sample:                             2   HQIC                           446.327

================================================================================
==
                     coef    std err          z      P>|z|      [0.025
0.975]
--------------------------------------------------------------------------------
--
const              0.0483      0.084      0.577      0.564      -0.116
0.212
ar.L1.D2.value     1.1386      0.109     10.399      0.000       0.924
1.353
ar.L2.D2.value    -0.5923      0.155     -3.827      0.000      -0.896
-0.289
ar.L3.D2.value     0.3079      0.111      2.778      0.005       0.091
0.525
ma.L1.D2.value    -1.0000      0.035    -28.799      0.000      -1.068
-0.932
                                    Roots
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.1557           -0.0000j            1.1557           -0.0000
AR.2            0.3839           -1.6318j            1.6763           -0.2132
AR.3            0.3839           +1.6318j            1.6763            0.2132
MA.1            1.0000           +0.0000j            1.0000            0.0000
-----------------------------------------------------------------------------
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_73_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The AIC has reduced to 440 from 515. Good. The p-values of the \(Y\)
terms are less the \textless{} 0.05, which is great. So overall it's
much better.

Ideally, you should go back multiple points in time, like, go back 1, 2,
3 and 4 quarters and see how your forecasts are performing at various
points in the year.

\%Here's a great practice exercise: Try to go back 27, 30, 33, 36 data
points and see how the forcasts performs. \%The forecast performance can
be judged using various accuracy metrics discussed next.

\hypertarget{accuracy-metrics-for-time-series-forecast}{%
\subsubsection{Accuracy Metrics for Time Series
Forecast}\label{accuracy-metrics-for-time-series-forecast}}

The commonly used accuracy metrics to judge forecasts are:

Mean Absolute Percentage Error (MAPE) Mean Error (ME) Mean Absolute
Error (MAE) Mean Percentage Error (MPE) Root Mean Squared Error (RMSE)
Lag-1 Autocorrelation of Error (ACF1) Correlation between the Actual and
the Forecast (corr) Min-Max Error (minmax)

Typically, if you are comparing forecasts of two different series, the
MAPE, Correlation and Min-Max Error can be used. Because only the above
three are percentage errors that vary between 0 and 1. That way, you can
judge how good is the forecast irrespective of the scale of the series.

The other error metrics are quantities. That implies, an RMSE of 100 for
a series whose mean is in 1000's is better than an RMSE of 5 for series
in 10's. So, you can't really use them to compare the forecasts of two
different scaled time series.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{115}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Accuracy metrics}
\PY{k}{def} \PY{n+nf}{forecast\PYZus{}accuracy}\PY{p}{(}\PY{n}{forecast}\PY{p}{,} \PY{n}{actual}\PY{p}{)}\PY{p}{:}
    \PY{n}{mape} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{actual}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} MAPE}
    \PY{n}{me} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}             \PY{c+c1}{\PYZsh{} ME}
    \PY{n}{mae} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{p}{)}    \PY{c+c1}{\PYZsh{} MAE}
    \PY{n}{mpe} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{/}\PY{n}{actual}\PY{p}{)}   \PY{c+c1}{\PYZsh{} MPE}
    \PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{o}{.}\PY{l+m+mi}{5}  \PY{c+c1}{\PYZsh{} RMSE}
    \PY{n}{corr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{forecast}\PY{p}{,} \PY{n}{actual}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}   \PY{c+c1}{\PYZsh{} corr}
    \PY{n}{mins} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{amin}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{forecast}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{,} 
                              \PY{n}{actual}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{maxs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{forecast}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{,} 
                              \PY{n}{actual}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{minmax} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mins}\PY{o}{/}\PY{n}{maxs}\PY{p}{)}             \PY{c+c1}{\PYZsh{} minmax}
    \PY{n}{acf1} \PY{o}{=} \PY{n}{acf}\PY{p}{(}\PY{n}{fc}\PY{o}{\PYZhy{}}\PY{n}{test}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}                      \PY{c+c1}{\PYZsh{} ACF1}
    \PY{k}{return}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{mape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{me}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{me}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mae}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{mae}\PY{p}{,} 
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mpe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{mpe}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{rmse}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acf1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{acf1}\PY{p}{,} 
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{corr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{corr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{minmax}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{forecast\PYZus{}accuracy}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{test}\PY{o}{.}\PY{n}{values}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/stattools.py:660: FutureWarning: The default number of
lags is changing from 40 tomin(int(10 * np.log10(nobs)), nobs - 1) after 0.12is
released. Set the number of lags to an integer to  silence this warning.
  FutureWarning,
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/stattools.py:669: FutureWarning: fft=True will become
the default after the release of the 0.12 release of statsmodels. To suppress
this warning, explicitly set fft=False.
  FutureWarning,
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{115}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'mape': 0.022501338735867873,
 'me': 3.2307975842766514,
 'mae': 4.548327520525081,
 'mpe': 0.016421068924791414,
 'rmse': 6.373248791560426,
 'acf1': 0.5105503577711675,
 'corr': 0.967457637120179,
 'minmax': 0.021631569901959136\}
\end{Verbatim}
\end{tcolorbox}
        
    Around 2.2\% MAPE implies the model is about 97.8\% accurate in
predicting the next 15 observations.

    \hypertarget{seasonal-autoregressive-integrated-moving-average-sarima}{%
\subsubsection{Seasonal Autoregressive Integrated Moving-Average
(SARIMA)}\label{seasonal-autoregressive-integrated-moving-average-sarima}}

The Seasonal Autoregressive Integrated Moving Average (SARIMA) method
models the next step in the sequence as a linear function of the
differenced observations, errors, differenced seasonal observations, and
seasonal errors at prior time steps.

It combines the ARIMA model with the ability to perform the same
autoregression, differencing, and moving average modeling at the
seasonal level.

The notation for the model involves specifying the order for the AR(p),
I(d), and MA(q) models as parameters to an ARIMA function and AR(P),
I(D), MA(Q) and m parameters at the seasonal level, e.g.~SARIMA(p, d,
q)(P, D, Q)m where ``m'' is the number of time steps in each season (the
seasonal period). A SARIMA model can be used to develop AR, MA, ARMA and
ARIMA models.

The method is suitable for univariate time series with trend and/or
seasonal components.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} SARIMA example}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{sarimax} \PY{k}{import} \PY{n}{SARIMAX}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
\PY{c+c1}{\PYZsh{} contrived dataset}
\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
\PY{c+c1}{\PYZsh{} fit model}
\PY{n}{model} \PY{o}{=} \PY{n}{SARIMAX}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{seasonal\PYZus{}order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make prediction}
\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax}{%
\subsubsection{Seasonal Autoregressive Integrated Moving-Average with
Exogenous Regressors
(SARIMAX)}\label{seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax}}

The Seasonal Autoregressive Integrated Moving-Average with Exogenous
Regressors (SARIMAX) is an extension of the SARIMA model that also
includes the modeling of exogenous variables.

Exogenous variables are also called covariates and can be thought of as
parallel input sequences that have observations at the same time steps
as the original series. The primary series may be referred to as
endogenous data to contrast it from the exogenous sequence(s). The
observations for exogenous variables are included in the model directly
at each time step and are not modeled in the same way as the primary
endogenous sequence (e.g.~as an AR, MA, etc. process).

The SARIMAX method can also be used to model the subsumed models with
exogenous variables, such as ARX, MAX, ARMAX, and ARIMAX.

The method is suitable for univariate time series with trend and/or
seasonal components and exogenous variables.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} SARIMAX example}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{sarimax} \PY{k}{import} \PY{n}{SARIMAX}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
\PY{c+c1}{\PYZsh{} contrived dataset}
\PY{n}{data1} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
\PY{n}{data2} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}\PY{p}{]}
\PY{c+c1}{\PYZsh{} fit model}
\PY{n}{model} \PY{o}{=} \PY{n}{SARIMAX}\PY{p}{(}\PY{n}{data1}\PY{p}{,} \PY{n}{exog}\PY{o}{=}\PY{n}{data2}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{seasonal\PYZus{}order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make prediction}
\PY{n}{exog2} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{200} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{,} \PY{n}{exog}\PY{o}{=}\PY{p}{[}\PY{n}{exog2}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[100.35471348]
    \end{Verbatim}

    \hypertarget{vector-autoregression-var}{%
\subsubsection{Vector Autoregression
(VAR)}\label{vector-autoregression-var}}

The Vector Autoregression (VAR) method models the next step in each time
series using an AR model. It is the generalization of AR to multiple
parallel time series, e.g.~multivariate time series.

The notation for the model involves specifying the order for the AR(p)
model as parameters to a VAR function, e.g.~VAR(p).

The method is suitable for multivariate time series without trend and
seasonal components.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} VAR example}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{vector\PYZus{}ar}\PY{n+nn}{.}\PY{n+nn}{var\PYZus{}model} \PY{k}{import} \PY{n}{VAR}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
\PY{c+c1}{\PYZsh{} contrived dataset with dependency}
\PY{n}{data} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
    \PY{n}{v1} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
    \PY{n}{v2} \PY{o}{=} \PY{n}{v1} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
    \PY{n}{row} \PY{o}{=} \PY{p}{[}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{]}
    \PY{n}{data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{)}
\PY{c+c1}{\PYZsh{} fit model}
\PY{n}{model} \PY{o}{=} \PY{n}{VAR}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make prediction}
\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{y}\PY{p}{,} \PY{n}{steps}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{vector-autoregression-moving-average-varma}{%
\subsubsection{Vector Autoregression Moving-Average
(VARMA)}\label{vector-autoregression-moving-average-varma}}

The Vector Autoregression Moving-Average (VARMA) method models the next
step in each time series using an ARMA model. It is the generalization
of ARMA to multiple parallel time series, e.g.~multivariate time series.

The notation for the model involves specifying the order for the AR(p)
and MA(q) models as parameters to a VARMA function, e.g.~VARMA(p, q). A
VARMA model can also be used to develop VAR or VMA models.

The method is suitable for multivariate time series without trend and
seasonal components.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} VARMA example}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{varmax} \PY{k}{import} \PY{n}{VARMAX}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
\PY{c+c1}{\PYZsh{} contrived dataset with dependency}
\PY{n}{data} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
    \PY{n}{v1} \PY{o}{=} \PY{n}{random}\PY{p}{(}\PY{p}{)}
    \PY{n}{v2} \PY{o}{=} \PY{n}{v1} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
    \PY{n}{row} \PY{o}{=} \PY{p}{[}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{]}
    \PY{n}{data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{)}
\PY{c+c1}{\PYZsh{} fit model}
\PY{n}{model} \PY{o}{=} \PY{n}{VARMAX}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make prediction}
\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/tsa/statespace/varmax.py:163: EstimationWarning: Estimation
of VARMA(p,q) models is not generically robust, due especially to identification
issues.
  EstimationWarning)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[[0.52387293 1.02383584]]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/sani/anaconda3/envs/.python3/lib/python3.6/site-
packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood
optimization failed to converge. Check mle\_retvals
  ConvergenceWarning)
    \end{Verbatim}

    

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    How to automatically build SARIMA model in python The problem with plain
ARIMA model is it does not support seasonality.

If your time series has defined seasonality, then, go for SARIMA which
uses seasonal differencing.

Seasonal differencing is similar to regular differencing, but, instead
of subtracting consecutive terms, you subtract the value from previous
season.

So, the model will be represented as SARIMA(p,d,q)x(P,D,Q), where, P, D
and Q are SAR, order of seasonal differencing and SMA terms respectively
and `x' is the frequency of the time series.

If your model has well defined seasonal patterns, then enforce D=1 for a
given frequency `x'.

Here's some practical advice on building SARIMA model:

As a general rule, set the model parameters such that D never exceeds
one. And the total differencing `d + D' never exceeds 2. Try to keep
only either SAR or SMA terms if your model has seasonal components.

Let's build an SARIMA model on `a10' -- the drug sales dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Import}
\PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/a10.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Usual Differencing}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Usual Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Usual Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Seasinal Dei}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Seasonal Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Seasonal Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a10 \PYZhy{} Drug Sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    As you can clearly see, the seasonal spikes is intact after applying
usual differencing (lag 1). Whereas, it is rectified after seasonal
differencing.

Let's build the SARIMA model using pmdarima`s auto\_arima(). To do that,
you need to set seasonal=True, set the frequency m=12 for month wise
series and enforce D=1.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} !pip3 install pyramid\PYZhy{}arima}
\PY{k+kn}{import} \PY{n+nn}{pmdarima} \PY{k}{as} \PY{n+nn}{pm}

\PY{c+c1}{\PYZsh{} Seasonal \PYZhy{} fit stepwise auto\PYZhy{}ARIMA}
\PY{n}{smodel} \PY{o}{=} \PY{n}{pm}\PY{o}{.}\PY{n}{auto\PYZus{}arima}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{start\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{start\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
                         \PY{n}{test}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{n}{max\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{max\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,}
                         \PY{n}{start\PYZus{}P}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{seasonal}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                         \PY{n}{d}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{D}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{trace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                         \PY{n}{error\PYZus{}action}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  
                         \PY{n}{suppress\PYZus{}warnings}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
                         \PY{n}{stepwise}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{smodel}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    The model has estimated the AIC and the P values of the coefficients
look significant. Let's look at the residual diagnostics plot.

The best model SARIMAX(3, 0, 0)x(0, 1, 1, 12) has an AIC of 528.6 and
the P Values are significant.

Let's forecast for the next 24 months.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{n\PYZus{}periods} \PY{o}{=} \PY{l+m+mi}{24}
\PY{n}{fitted}\PY{p}{,} \PY{n}{confint} \PY{o}{=} \PY{n}{smodel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{n\PYZus{}periods}\PY{o}{=}\PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{return\PYZus{}conf\PYZus{}int}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{index\PYZus{}of\PYZus{}fc} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{date\PYZus{}range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{periods} \PY{o}{=} \PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{freq}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} make series for plotting purpose}
\PY{n}{fitted\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fitted}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fitted\PYZus{}series}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} 
                 \PY{n}{lower\PYZus{}series}\PY{p}{,} 
                 \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SARIMA \PYZhy{} Final Forecast of a10 \PYZhy{} Drug Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    How to build SARIMAX Model with exogenous variable The SARIMA model we
built is good. I would stop here typically.

But for the sake of completeness, let's try and force an external
predictor, also called, `exogenous variable' into the model. This model
is called the SARIMAX model.

The only requirement to use an exogenous variable is you need to know
the value of the variable during the forecast period as well.

For the sake of demonstration, I am going to use the seasonal index from
the classical seasonal decomposition on the latest 36 months of data.

Why the seasonal index? Isn't SARIMA already modeling the seasonality,
you ask?

You are correct.

But also, I want to see how the model looks if we force the recent
seasonality pattern into the training and forecast.

Secondly, this is a good variable for demo purpose. So you can use this
as a template and plug in any of your variables into the code. The
seasonal index is a good exogenous variable because it repeats every
frequency cycle, 12 months in this case.

So, you will always know what values the seasonal index will hold for
the future forecasts.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Import Data}
\PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/a10.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's compute the seasonal index so that it can be forced as a
(exogenous) predictor to the SARIMAX model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Compute Seasonal Index}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{seasonal} \PY{k}{import} \PY{n}{seasonal\PYZus{}decompose}
\PY{k+kn}{from} \PY{n+nn}{dateutil}\PY{n+nn}{.}\PY{n+nn}{parser} \PY{k}{import} \PY{n}{parse}

\PY{c+c1}{\PYZsh{} multiplicative seasonal component}
\PY{n}{result\PYZus{}mul} \PY{o}{=} \PY{n}{seasonal\PYZus{}decompose}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{36}\PY{p}{:}\PY{p}{]}\PY{p}{,}   \PY{c+c1}{\PYZsh{} 3 years}
                                \PY{n}{model}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{multiplicative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                \PY{n}{extrapolate\PYZus{}trend}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{seasonal\PYZus{}index} \PY{o}{=} \PY{n}{result\PYZus{}mul}\PY{o}{.}\PY{n}{seasonal}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{12}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{p}{)}
\PY{n}{seasonal\PYZus{}index}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{seasonal\PYZus{}index}\PY{o}{.}\PY{n}{index}\PY{p}{)}\PY{o}{.}\PY{n}{month}

\PY{c+c1}{\PYZsh{} merge with the base data}
\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{month}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{seasonal\PYZus{}index}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{on}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seasonal\PYZus{}index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{df}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{index}  \PY{c+c1}{\PYZsh{} reassign the index.}
\end{Verbatim}
\end{tcolorbox}

    The exogenous variable (seasonal index) is ready. Let's build the
SARIMAX model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pmdarima} \PY{k}{as} \PY{n+nn}{pm}

\PY{c+c1}{\PYZsh{} SARIMAX Model}
\PY{n}{sxmodel} \PY{o}{=} \PY{n}{pm}\PY{o}{.}\PY{n}{auto\PYZus{}arima}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{exogenous}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seasonal\PYZus{}index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                           \PY{n}{start\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{start\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
                           \PY{n}{test}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                           \PY{n}{max\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{max\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,}
                           \PY{n}{start\PYZus{}P}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{seasonal}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                           \PY{n}{d}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{D}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{trace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                           \PY{n}{error\PYZus{}action}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  
                           \PY{n}{suppress\PYZus{}warnings}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
                           \PY{n}{stepwise}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{sxmodel}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    So, we have the model with the exogenous term. But the coefficient is
very small for x1, so the contribution from that variable will be
negligible. Let's forecast it anyway.

We have effectively forced the latest seasonal effect of the latest 3
years into the model instead of the entire history.

Alright let's forecast into the next 24 months. For this, you need the
value of the seasonal index for the next 24 months.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{n\PYZus{}periods} \PY{o}{=} \PY{l+m+mi}{24}
\PY{n}{fitted}\PY{p}{,} \PY{n}{confint} \PY{o}{=} \PY{n}{sxmodel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{n\PYZus{}periods}\PY{o}{=}\PY{n}{n\PYZus{}periods}\PY{p}{,} 
                                  \PY{n}{exogenous}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{tile}\PY{p}{(}\PY{n}{seasonal\PYZus{}index}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} 
                                  \PY{n}{return\PYZus{}conf\PYZus{}int}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{index\PYZus{}of\PYZus{}fc} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{date\PYZus{}range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{periods} \PY{o}{=} \PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{freq}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} make series for plotting purpose}
\PY{n}{fitted\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fitted}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fitted\PYZus{}series}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} 
                 \PY{n}{lower\PYZus{}series}\PY{p}{,} 
                 \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SARIMAX Forecast of a10 \PYZhy{} Drug Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    Practice Exercises In the AirPassengers dataset, go back 12 months in
time and build the SARIMA forecast for the next 12 months.

Is the series stationary? If not what sort of differencing is required?
What is the order of your best model? What is the AIC of your model?
What is the MAPE achieved in OOT cross-validation? What is the order of
the best model predicted by auto\_arima() method?

    \hypertarget{persistence-model}{%
\subsubsection{Persistence Model}\label{persistence-model}}

Let's say that we want to develop a model to predict the last 7 days of
minimum temperatures in the dataset given all prior observations.

The simplest model that we could use to make predictions would be to
persist the last observation. We can call this a persistence model and
it provides a baseline of performance for the problem that we can use
for comparison with an autoregression model.

We can develop a test harness for the problem by splitting the
observations into training and test sets, with only the last 7
observations in the dataset assigned to the test set as ``unseen'' data
that we wish to predict.

The predictions are made using a walk-forward validation model so that
we can persist the most recent observations for the next day. This means
that we are not making a 7-day forecast, but 7 1-day forecasts.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{read\PYZus{}csv}
\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{DataFrame}
\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{concat}
\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
\PY{n}{series} \PY{o}{=} \PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daily\PYZhy{}min\PYZhy{}temperatures.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{c+c1}{\PYZsh{} create lagged dataset}
\PY{n}{values} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{series}\PY{o}{.}\PY{n}{values}\PY{p}{)}
\PY{n}{dataframe} \PY{o}{=} \PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{values}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{values}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{dataframe}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t\PYZhy{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t+1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{c+c1}{\PYZsh{} split into train and test sets}
\PY{n}{X} \PY{o}{=} \PY{n}{dataframe}\PY{o}{.}\PY{n}{values}
\PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{:}\PY{p}{]}
\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}y} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}

\PY{c+c1}{\PYZsh{} persistence model}
\PY{k}{def} \PY{n+nf}{model\PYZus{}persistence}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
	\PY{k}{return} \PY{n}{x}

\PY{c+c1}{\PYZsh{} walk\PYZhy{}forward validation}
\PY{n}{predictions} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{test\PYZus{}X}\PY{p}{:}
	\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}persistence}\PY{p}{(}\PY{n}{x}\PY{p}{)}
	\PY{n}{predictions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\PY{n}{test\PYZus{}score} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test MSE: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}score}\PY{p}{)}
\PY{c+c1}{\PYZsh{} plot predictions vs expected}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Test MSE: 3.423
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_106_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Running the example prints the mean squared error (MSE). The value
provides a baseline performance for the problem. The expected values for
the next 7 days are plotted (blue) compared to the predictions from the
model (red).


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
