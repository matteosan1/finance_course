\chapter{Stochastic Processes}

In financial engineering, simulation tools have been increasingly gaining importance due to both the continuous enhancement of computational efficiency and the development of very complex financial instruments, e.g. derivatives. 

\section{Markov Process}
 
A \emph{stochastic or random process} can be defined as a collection of random variables indexed by some mathematical set (usually time), meaning that each random variable of the stochastic process is uniquely associated with an element in that set. Figure~\ref{fig:random_process} shows a graphical representation of a random process.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/brownian_process}
\caption{Representation of a random process $X(t)$. Notice that, at each time $t$, the corresponding random variable has a peculiar probability distribution.}
\label{fig:random_process}
\end{figure}

A model of the dynamics of asset prices is a nice example of random process since it must reflect the stochastic nature of price movements. Stock prices are usually assumed to follow a \emph{Markov process}. Markov processes are a particular type of stochastic process without any history: past values and the way the present has emerged from the past are irrelevant. This unique characteristic of Markov processes makes them memory-less.

\subsubsection{Wiener Process}
The simplest example of Markov stochastic process is the \emph{Wiener process} ($W$) or elementary Brownian motion, which is characterized by stationary and independent increments that are normally distributed. Those increments can be expressed as

\begin{equation}
\Delta W = Z\sqrt{\Delta t}
\end{equation}
where $Z\approx \mathcal{N}(0, 1)$ and $\mathcal{N}$ represents the standard normal Gaussian. 
The mean of $\Delta W$ is zero and its variance is $\Delta t$, which means the standard deviation grows with the square root of time. It follows that $W(t)\approx \mathcal{N}(0, t)$ because each $\Delta W$ is independently distributed like standard Gaussian. Figure~\ref{fig:wiener_process} shows two realizations of Wiener process.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/wiener_process.png}
	\caption{Two Wiener process realizations.}
	\label{fig:wiener_process}
\end{figure}

\subsubsection{It$\hat{o}$ Processes} 
An It$\hat{o}$ process ($X$) is a stochastic process whose increments satisfy the following \emph{stochastic differential equation} (SDE) 
\begin{equation}
dX(t) = \mu(t, X(t)) dt + \sigma(t, X(t)) dW(t)
\end{equation}
A stochastic differential equation is a differential equation in which one or more of the terms is a stochastic process, resulting in a solution which is also a stochastic process.

In the special case where both \emph{drift} $\mu$ and the diffusion coefficient $\sigma$ are deterministic and independent of time $t$ and $X$, the process $X(t)$ is called an \emph{arithmetic Brownian motion}. The corresponding SDE is
\begin{equation}
dX(t) = \mu dt + \sigma dW(t) = \mu dt + \sigma \sqrt{t} \mathcal{N}(0,1) 
\end{equation}
thus $X(t)\approx \mathcal{N}(\mu t, \sigma^2 t)$.

Figure~\ref{fig:random_process} shows how the stochastic term grows linearly with time in an arithmetic Brownian motion.
Since such a process can take negative values, this type of process is apparently unsuitable for simulating asset prices. These are rather assumed to follow a \emph{geometric Brownian motion} (GBM) instead, which is going to be described in the Section. 

\section{Geometric Brownian Motion}
\label{derivation-of-log-normal-stochastic-differential-equation}

A stock price ($S(t)$ or $S_t$) deviates from a steady state as a result of random fluctuations given by the trades. 
Its relative change during a period $dt$ can be decomposed into two parts

\begin{itemize}
\tightlist
\item \emph{deterministic part}: represented by the expected return from the stock held during $dt$ and which can be expressed as $\mu S_tdt$ (with $\mu$ being the expected rate of return);
\item \emph{stochastic part}: reflects the random changes of the market (e.g. as a response to external effects such as unexpected news). A reasonable assumption is to consider this contribution proportional to the stock so $\sigma S_t dW_t$, where $dW_t$ is a Wiener process.
\end{itemize}
Putting the two contributions together the resulting SDE is:

\begin{equation}
dS_t = \mu S_t dt + \sigma S_t dW_t
\label{eq:differential}
\end{equation}
or dividing by $S_t$
\begin{equation}
\cfrac{dS_t}{S_t} = d\textrm{log}(S_t) = \mu dt + \sigma dW_t
\label{eq:differential_relative}
\end{equation}

The solution of Eq.~\ref{eq:differential_relative} can be derived by applying It$\hat{o}$'s formula~\cite{bib:ito_lemma}, which, roughly speaking, says that the derivative of a stochastic function with respect to time has an additional \emph{deterministic} term compared to the usual derivative we know from standard calculus. 

More formally it states that for any given continuous and differentiable function $G(S, t)$ where $S$ satisfies $dS=a\cdot dt +b\cdot dW_t$ holds

\begin{equation}
dG = \left(a\cfrac{\partial G}{\partial S} + \cfrac{\partial G}{\partial t} + \cfrac{1}{2}b^2\cfrac{\partial^2 G}{\partial S^2} \right)dt + b \cfrac{\partial G}{\partial S}dW
\label{eq:itos_lemma}
\end{equation}
which is essentially an extension of the \emph{Taylor series} expansion applicable to stochastic functions.

\begin{attention}
\subsubsection{Derivation of It$\hat{o}$'s Lemma}
Assume $X_t$ is a process that satisfies the stochastic differential equation
\begin{equation*}	
dX_{t}=\mu_{t}\,dt+\sigma_{t}\,dW_{t}
\end{equation*}
If $f(x, t)$ is a twice-differentiable scalar function, its expansion in a Taylor series is
	
\begin{equation*}
df={\cfrac {\partial f}{\partial t}}\,dt+{\cfrac {\partial f}{\partial x}}\,dx+{\cfrac {1}{2}}{\cfrac {\partial ^{2}f}{\partial x^{2}}}\,dx^{2}+\cdots 
\end{equation*}

Substituting $X_t$ for $x$ and therefore $\mu_t dt + \sigma_t dW_t$ for `$dx$ gives
	
\begin{equation*}
df={\cfrac {\partial f}{\partial t}}\,dt+{\cfrac {\partial f}{\partial x}}(\mu _{t}\,dt+\sigma _{t}\,dW_{t})+{\cfrac {1}{2}}{\cfrac {\partial ^{2}f}{\partial x^{2}}}\left(\mu _{t}^{2}\,dt^{2}+2\mu _{t}\sigma _{t}\,dt\,dW_{t}+\sigma _{t}^{2}\,dW_{t}^{2}\right)+\cdots 
\end{equation*}

In the limit $dt\rightarrow 0$, the terms $dt^2$ and $dt\cdot dW_t$ tend to zero faster than $dW_t^2$, which is $O(dt)$. Neglecting those terms, substituting $dt$ for $dW_t^2$ (it is a Wiener process), and collecting the $dt$ and $dW_t$ terms, we obtain
	
\begin{equation*}
df=\left({\cfrac {\partial f}{\partial t}}+\mu _{t}{\cfrac {\partial f}{\partial x}}+{\cfrac {\sigma _{t}^{2}}{2}}{\cfrac {\partial ^{2}f}{\partial x^{2}}}\right)dt+\sigma _{t}{\cfrac {\partial f}{\partial x}}\,dW_{t}
\end{equation*}
\end{attention}

If we set $G = \textrm{log}(S_t)$ we can compute the derivatives:

\begin{equation}
\begin{gathered}
\cfrac{\partial G}{\partial S} = \cfrac{1}{S_t}\\[5pt]
\cfrac{\partial G}{\partial t} = 0\\[5pt]
\cfrac{\partial^2 G}{\partial S^2} = -\cfrac{1}{S_t^{2}}
\end{gathered}
\end{equation}
Substituting back into Eq.~\ref{eq:itos_lemma} and taking the values of $a$ and $b$ from Eq.~\ref{eq:differential} we get:

\begin{equation}
\begin{gathered}
d(\textrm{log} S_t) = \left(\mu S_t \cfrac{1}{S_t} + \cfrac{1}{2}\sigma^2 S_t^2 (-\cfrac{1}{S_t^2})\right)dt + \sigma Z\sqrt{dt}\\[5pt]
\textrm{log} (S_t) - \textrm{log} (S_{t-1}) = \textrm{log} \cfrac{S_t}{S_{t-1}} = \left(\mu - \cfrac{1}{2}\sigma^2\right)dt + \sigma Z\sqrt{dt}\\[5pt]
S_t = S_{t-1}\cdot\textrm{exp}\left[\left(\mu - \cfrac{1}{2}\sigma^2\right)dt + \sigma Z\sqrt{dt}\right]
\end{gathered}
\label{eq:gbm_solution}
\end{equation}
As can be seen from the second of Eqs.~\ref{eq:gbm_solution} the variation in $\textrm{log} S_t$ has a constant (the \emph{drift} $\mu - \frac{1}{2}\sigma^2$) plus a Gaussian distributed random variable (remember that $Z$ is $\mathcal{N}(0,1)$). Therefore $\textrm{log} S_t$ at some time $T$ is 

\begin{equation}
\textrm{log}S_t \approx\mathcal{N}\left[\left(\mu-\frac{\sigma^2}{2}\right)T, \sigma^2 T\right]
\end{equation}
A random variable like $S_t$ whose logarithm is normally distributed is said to be \emph{log-normal}. 

The model we have just developed implies that the stock price at time $T$, given today's price, is log-normally distributed. One of the most important properties of a log-normal distribution is to be positive definite and that's why it is a good characteristic for stock prices. 

It is possible to check how a log-normal PDF looks like with the following lines of code. Figure~\ref{fig:log_normal} shows the resulting distribution.

\begin{ipython}
from matplotlib import pyplot as plt
from scipy.stats import lognorm
from numpy import arange

x = arange(-1, 5, 0.01)
plt.plot(x, lognorm.pdf(x, 1), color='blue', label='Log-normal PDF')
plt.grid(True)
plt.xlim(-1, 5)
plt.ylim(0, 0.7)
plt.legend()
plt.show()
\end{ipython}

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{figures/log_normal}
\caption{Log-normal PDF, it is apparent how such distribution doesn't have non-zero probabilities for $x<0$.}
\label{fig:log_normal}
\end{figure}

Actually this result was expected, in fact looking at the initial $dS$ expression (Eq.~\ref{eq:differential}) we had:

\begin{equation}
dS_t = \mu S_tdt + \sigma S_t dW_t
\end{equation}
which shows that the closer $S_t$ is to 0, the smaller is its variation $dS$, so it will never go negative.

\section {Simulating Stochastic Differential Equations}
Several kind of numerical schemes can be used to simulate SDEs, however we restrict our discussion to the two most fundamental ones, the \emph{Euler scheme} and the \emph{Milstein scheme}. 

As a starting point for both methods, consider an SDE of the form
\begin{equation}
dX(t) = \mu(t, X(t))dt + \sigma(t, X(t)) dW (t)
\label{eq:sde}
\end{equation}

%Note that both the drift $\mu$ and the diffusion $\sigma$ are functions of the process variable $X$ and the time $t$. 
Suppose that we are interested in simulating values of $X(T)$ without knowing an explicit solution of Eq.~\ref{eq:sde}. 

%Keep in mind that when we are simulating an SDE, we are actually simulating a 
%discretized version of an SDE. Therefore, a general hat notation $\hat{X}$ 
%is introduced to indicate that $\hat{X}$ is a time-discretized approximation of %the true value $X$.

\subsection{Euler Scheme}
The Euler scheme is the simplest and probably the most used. In this kind of simulation the process is discretized at constant time steps $\Delta t$.
% denoted by $b$  as $\{X_b, X_{2b},\ldots, X_{mb}\} = \{X(t_i), X(t_{i+1}),...,X(t_{i+n})\}$. Furthermore, the variable $m$ indicates the total number of simulated time steps, i.e. $mb = T$.

Consider $X$ at a time $t_i$, the value at $t_{i+1} = t_i +\Delta t$ can be determined directly applying the SDE of Eq.~\ref{eq:sde} sampling from the standard normal $Z_i \approx \mathcal{N}(0,1)$

\begin{equation}
X(t_{i+1}) = X(t_i) + \mu(t_i , X(t_i))\Delta t + \sigma(t_i , X(t_i))\sqrt{\Delta t}Z_{i+1}
\label{eq:euler_scheme}
\end{equation}
Therefore the implementation of this method is straightforward, at least if drift and diffusion are easy to evaluate.

\begin{attention}
\subsection{Milstein Scheme}
The accuracy of numerical solutions of ordinary differential equations may be improved by simple Taylor expansions. A very similar strategy also applies to SDEs. Yet it is crucial to account for the rules of It$\hat{o}$ calculus in order to maintain consistency.

Inspecting the Euler scheme~\ref{eq:euler_scheme} from the perspective of Taylor expansions leads to a possible inconsistency: a Taylor approximation expands the drift to the order $O(\Delta t)$ but the diffusion term only to $O(\sqrt{\Delta t})$. This suggests to focus on the diffusion term in order to improve the Euler scheme. And exactly there the Milstein scheme is applied.

The Milstein scheme is defined as an ordinary Euler scheme where the next order terms of the It$\hat{o}$ expansion of Equation~\ref{eq:sde} are included. This eventually leads to

\begin{equation}
\begin{aligned}
X(t_{i+1})= X(t_i) &+ \mu(t_i, X(t_i))\Delta t+\sigma(t_i,X(t_i))\sqrt{\Delta t}Z_{i+1} \\
&+\cfrac{1}{2}\sigma'(t_i ,X(t_i))\sigma (t_i ,X(t_i))\Delta t(Z^2_{i+1} -1)
\end{aligned}
\end{equation}

The Milstein approximation thus simply involves the addition of a term to the Euler scheme so that both the drift and diffusion terms are expanded to $O(\Delta t)$ in order to eliminate the inconsistency. %The Milstein scheme therefore provides a refinement of the Euler scheme based on expanding the diffusion term to $O(\Delta t)$ rather than just $O(\sqrt{\Delta t})$.

However, the question remains whether it is of great difference if one applies Euler’s or Milstein’s approximation for a particular diffusion equation. 

In conclusion, the approximations turn out to be very close in many simple cases. In more sophisticated models, the Milstein scheme appears to be a little better, which may be the additional benefit of the increased complexity. Of course, the two approximations do not differ at all if the diffusion coefficient $\sigma (t,X(t))$ does not depend on $X(t)$.
\end{attention}

\subsection{Simulation of a Stock Price}
%Note that the left-hand side of Eq.~\ref{eq:differential_relative} represents the 
%proportional change in the asset price in the interval $(t, t + dt)$ and additionally shows that $dS/S ∼ \mathcal{N}(\mu dt, \sigma^2 dt)$. 

Equation~\ref{eq:gbm_solution} allows to simulate realizations of possible asset price paths by Monte Carlo. Particularly, a random path can be generated by sampling iteratively from $\mathcal{N}(0,1)$ and substituting into Eq.~\ref{eq:gbm_solution}.
Below two implementations are proposed, the latter exploiting \texttt{numpy} vectorialization more concise and faster.

\begin{ipython}
from numpy.random import normal, seed, exp, sqrt

S = 100
mu = -0.01
sigma = 0.05
T=1
historical_series = [S]

seed(1)
for i in range(30):
    S = S * exp((mu - 0.5 * sigma * sigma) * T +
                 sigma * sqrt(T) * normal())
    historical_series.append(S)
\end{ipython}

\begin{ipython}
from numpy import exp, sqrt, cumprod
from numpy.random import normal, seed

def gbm(mu, sigma, dT, steps):
  return exp((mu - 0.5 * sigma**2)*dT + sigma * sqrt(T) * normal(size=steps))

S = 100
mu = -0.01
sigma = 0.05
dt = 1
seed(1)

historical_series = S*cumprod(gbm(mu, sigma, dt, 30))
\end{ipython}

It is important to note that both $\mu$ and $\sigma$ are variation in unit of time that refers to the $\Delta t$ used in the simulation. In other words if $\Delta t = 1~\textrm{day}$ then the two parameters should represents the daily variations of the stock price. Figure~\ref{fig:stock_price_sim} shows possible asset price paths following a geometric Brownian motion. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/asset_price_simulation}
\caption{Simulation with Monte Carlo method of various realization of a stock price, using geometric Brownian motion.}
\label{fig:stock_price_sim}
\end{figure}

\subsection{Pricing Vanilla European Options by Monte Carlo}
Monte Carlo simulation fits particularly well to the pricing of a derivative. In the risk-neutral world indeed the price of a derivative can be expressed as

\begin{equation}
V(S, t) = e^{-r(T-t)} \mathbb{E}[\textrm{payoff}(S_t)]
\end{equation}
So we could simulate different stock paths, calculate the average of the corresponding payoffs (i.e. the expectation), and finally discount the result.

Let's consider a vanilla European call option. This may seem a little pointless, as there is the Black-Scholes formula which provides an analytical method that easily delivers the true option price. Nevertheless, it is a good example of financial simulation. 

The payoff of a call option with strike $K$ and duration $T$ is 

\begin{equation}
(S(T)-K)^+ = \max[0,S(T)-K]
\end{equation}

The call option price $C$ is defined by

\begin{equation} 
C = e^{-rT} \mathbb{E}[(S(T)-K)^+ ]
\label{eq:call_payoff}
\end{equation}

However, it is impossible to price an option without any information about the distribution of its underlying price or its return. Therefore, we necessarily need to specify the particular distribution of the final asset price $S(T)$. 

Let's model $S(T)$ as a log-normal random variable and simulate one path for the asset price as it is done before. Then calculate, according to Eq.~\ref{eq:call_payoff}, the corresponding payoff. Repeat this step numerous times and finally compute the mean of the discounted payoffs. 

The fact that this Monte Carlo estimate is indeed consistent with the Black-Scholes formula results is illustrated in Fig.~\ref{fig:error_BS} where the relative pricing error, computed as $(C_n-C)/C$, as a function of the number of simulation $n$ is shown. Note that the pricing error is gradually decreasing and eventually converging to 0 as $n$ goes to infinity. This is another proof that Monte Carlo provides correct results in the limit $n\rightarrow\infty$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/call_pricing_error.png}
\caption{Development of the relative pricing error for a call between MC simulation and BS formula with growing samples.}
\label{fig:error_BS}
\end{figure}

\section{Markov Chain}
\label{sec:markov_chain}
A \emph{Markov chain} is a mathematical system usually defined as a collection of random variables, that transition from one state to another according to certain probabilistic rules. This set of transitions satisfies the Markov property, in other words the probability of transitioning to any particular state is dependent solely on the current state and not on the sequence of states that preceded it. 

Markov Chains are widely employed in economics, game theory, communication theory, genetics and finance. When it comes to real-world problems, they are used to study cruise control systems in motor vehicles, queues or lines of customers, exchange rates of currencies\ldots

%A Markov chain is then a random process and has either discrete state space (set of possible values of the random variables) and discrete index set (often representing time). 

\subsection{Discrete Time Markov Chain}
A discrete-time Markov chain involves a system which is in a certain state at each step, with the state changing randomly. The steps are often thought of as moments in time (but you might as well refer to physical distance or any other discrete measurement). This kind of Markov chain is a sequence of random variables $X_1, X_2, X_3,\ldots$ with the Markov property. Putting this is mathematical probabilistic formula:

\begin{equation}
P(X_{n+1}) = x | X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) = P( X_{n+1} = x | X_n = x_n)
\end{equation}

The probability of $X_{n+1}$ only depends on the probability of $X_n$ that precedes it. Which means the knowledge of the previous state is all that is necessary to determine the probability distribution of the current state.

\begin{attention}
Some interesting properties of Markov chains are:

\begin{itemize}
\tightlist
\item reducibility: a Markov chain is said to be irreducible if there exists a chain of steps between any two states that has positive probability;
\item periodicity: a state in a Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1. Otherwise the state is called aperiodic;	
\item transience and recurrence: a state $i$ is said to be transient if, given that we start in state $i$, there is a non-zero probability that we will never return to $i$. State $i$ is recurrent (or persistent) if it is not transient;
\item ergodicity: a state $i$ is said to be ergodic if it is aperiodic and recurrent. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic;
\item absorbing state: a state $i$ is called absorbing if it is impossible to leave this state. If every state can reach an absorbing state, then the Markov chain is an absorbing Markov chain.
\end{itemize}
\end{attention}

%The possible values of $X_i$ form a countable set $S$ called the state space of the chain. The state space can be anything: letters, numbers, basketball scores or weather conditions. While the time parameter is usually discrete, the state space of a discrete time Markov chain does not have any widely agreed upon restrictions, and rather refers to a process on an arbitrary state space. However, many applications of Markov chains employ finite or countably infinite state spaces, because they have a more straightforward statistical analysis.

\subsection{The Markov Model}
A Markov chain can be represented by a set of probabilities associated with the various possible transitions (the changes of state of the system). These probabilities are usually represented as a matrix (i.e. the \emph{transition matrix}).
Each element of the matrix $P(X_{n+1} = x | X_n = x_n)$ can be read as the probability of going to state $X_{n+1}$ given the current value of state $X_n$. 

If the Markov chain has $N$ possible states, the matrix will be $N \times N$, such that entry $(i, j)$ is the probability of transitioning from state $i$ to state $j$. Additionally, the transition matrix must be such that entries in each row must add up to exactly 1, since each row represents a state probability distribution.

%The model is characterized by a state space (the set of possible state of the system), a transition matrix describing the probabilities of particular transitions, and an initial state across the state space. Let's work out a real example. 

\subsection{Markov Chain to Predict Market Trends}
Markov chains and their respective diagrams can be used to model the probabilities of certain financial market climates and thus predicting the likelihood of future market conditions.
These conditions, also known as trends, are:
\begin{itemize}
\item bull markets: periods of time where prices generally are rising, due to the actors having optimistic hopes of the future;
\item bear markets: periods of time where prices generally are declining, due to the actors having a pessimistic view of the future;
\item stagnant markets: periods of time where the market is characterized by neither a decline nor rise in general prices.
\end{itemize}

The transition matrix is usually determined using historical data. For example, consider a hypothetical market with Markov properties where historical data has given us the patterns depicted in Fig.~\ref{fig:markov_chain}.
After a week characterized of a \emph{bull market} trend there is a 90\% chance that another bullish week will follow. Additionally, there is a 7.5\% chance that the bull week instead will be followed by a bearish one, or a 2.5\% chance that it will be a stagnant one. Similarly for the other states. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/markov_chain}
\caption{Graph showing the transition probabilities between three weekly market states: bull, bear and stagnant.}
\label{fig:markov_chain}
\end{figure}

By compiling these probabilities into a table, it is easy to derive the transition matrix $\Pi$

\[\Pi = 
\begin{bmatrix}
p_{11} & p_{12} & p_{13} \\
p_{12} & p_{22} & p_{23} \\
p_{13} & p_{32} & p_{33}
\end{bmatrix} =
\begin{bmatrix}
0.9 & 0.075 & 0.025 \\
0.15 & 0.8 & 0.05 \\
0.25 & 0.25 & 0.5
\end{bmatrix} 
\]

Let's indicate with a $1 \times 3$ vector $C$ the current state of the market; where column 1 represents a bull week, column 2 a bear week and column 3 a stagnant week. Finally suppose this week the market is in a bearish state, resulting in the vector $C_0  = (0, 1, 0)$.

To calculate the probabilities of a bull, bear or stagnant week for any number of $n$ weeks into the future it is enough to multiply $C$ by the transition matrix $n$ times ($C_n = C_0\cdot \Pi^n$)

\[C_1 = C_0 \cdot \Pi =  
\begin{bmatrix}
0 \\
1 \\
0 
\end{bmatrix}
\begin{bmatrix}
0.9 & 0.075 & 0.025 \\
0.15 & 0.8 & 0.05 \\
0.25 & 0.25 & 0.5
\end{bmatrix} = 
\begin{bmatrix}
0.15 \\
0.8 \\
0.05 
\end{bmatrix}
\]

The model predicts an 80\% probability for the market to stay in a bearish mode also in the next week, 15\% to become bullish and just a 5\% to be stagnant.

With \texttt{python} it is possible to determine longer term behavior up to the \emph{stationary distribution}. The stationary distribution is the fraction of time that the system spends in each state as the number of iterations approaches infinity.

\begin{ipython}
import numpy as np

n = 50
hist_C = np.zeros(shape=(n, 3))
C = np.array([0, 1, 0])
P = np.array([[0.9, 0.075, 0.025],[0.15, 0.8, 0.05],[0.25, 0.25, 0.5]])

for i in range(n):
    hist_C[i] = C
    C = C.dot(P)

print (C)
\end{ipython}
\begin{ioutput}
[0.62499979 0.31250019 0.06250002]
\end{ioutput}

From this example it can be concluded that as $n \rightarrow \infty$, the probabilities will converge to a steady state, 63\% of all weeks will be bullish, 31\% bearish and 6\% stagnant. Figure~\ref{fig:markov_chain_sim} reports the various probabilities at each iteration. The system converges to the stationary state after about 20 weeks.
It can also be shown, by changing $C_0$ that the steady-state probabilities of this Markov chain do not depend upon the initial state. 

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/markov_chain_sim}
	\caption{Simulation of weekly market trend for $n=50$. The system reaches the stationary state relatively quickly after about 20 weeks.}
	\label{fig:markov_chain_sim}
\end{figure}

These results can be used in various ways, some examples are: calculating the average time it takes for a bearish period to end or the risk that a bullish market turns bearish or stagnant.

\section*{Exercises}
\input{stochastic_ex_text}

\begin{thebibliography}{9}
\bibitem{bib:stochastic_calculus} P. Wilmott, \emph{Quantitative Finance (2nd edition)}, Elementary Stochastic Calculus (Ch. 4), John Wiley and Sons, 2006 
\bibitem{bib:ito_lemma}\href{https://en.wikipedia.org/wiki/It\%C3\%B4\%27s_lemma}{\emph{It$\hat{o}$'s Lemma}}, Wikipedia [Online]
\bibitem{bib:euler_scheme} \href{https://en.wikipedia.org/wiki/Euler_method}{\emph{Euler Scheme}}, Wikipedia [Online]
\end{thebibliography}









