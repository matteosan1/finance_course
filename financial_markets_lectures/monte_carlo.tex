\chapter{Monte Carlo Simulation}
\label{cap:montecarlo}

The modern version of the Monte Carlo method dates back to the application of the first computers during the years 1940-45 with the purpose of computing neutron diffusion in atomic bombs.

The Monte Carlo approach was primarily promoted by physics researchers Stanislaw Ulam, Nicholas Metropolis and John von Neumann. They were well aware of the potential of such technique but it was only with the first electronic computer, the ENIAC, which was able to solve differential equations at a tremendous, and so far inconceivable, speed, that Monte Carlo method was eventually triggered.

The name Monte Carlo refers to the city in Monaco where Ulam's uncle used to indulge his gambling passion. The characteristics of randomness and the repetitive nature of many processes correspond with the games played in a casino. Note that the famous roulette wheel is one of the simplest mechanical devices to generate random variables.

Monte Carlo tools find their application mostly in cases where the inherent complexity of a problem makes the use of other techniques impossible or where no analytically tractable solution exists.
They are are widely used in many fields: Engineering, Physics, Computational biology, Computer graphics, Applied statistics and Artificial intelligence for games. It started to be en vogue in financial mathematics in the 1980s, particularly when the theories of the random walk of asset prices came up.

In this Chapter the Monte Carlo technique is reviewed together with few applications.

\section{The Algorithm}
\label{whats-monte-carlo-simulation}

A Monte Carlo simulation is a mathematical technique that simulates the range of possible outcomes for an uncertain event.
The Monte Carlo method has been described as "faking it a billion times until the reality emerges." It relies on the assumption that many random samples mimic patterns in the total population.

The algorithm proceeds by simulating the process under study replacing every source of uncertainty by random numbers sampled from appropriate distributions (PDF).
The results is re-calculated many times, using a different set of random numbers within the estimated range. This process generates many probable outcomes, which become more accurate as the number of inputs grows, i.e. law of large numbers.

The main advantage of the technique is that predictions are based on an estimated range of values instead of a fixed set of values and evolve randomly.

Note that depending upon the number of uncertainties and their ranges, a Monte Carlo simulation could involve thousands or even millions of recalculations before it is completed.

Although they might vary from case to case, the general steps to a Monte Carlo simulation can be summarized as follows:

\begin{itemize}
\item Build the mathematical model.
\item Determine an appropriate probability distribution (PDF) for each random variable and their range of variability $\Omega$.
\item Run the random variables through the mathematical model to perform many iterations of the simulation.
\item Aggregate the results, and determine mean and standard deviation to determine if the result is as expected. Visualize the results on a histogram.
\end{itemize}

As an example imagine to simulate the results of rolling a die: the only variable is the die outcome, so $\Omega =1,2,3,4,5,6$ and the outcome probability is a uniform distribution (i.e. we are using a fair die and outcomes are equiprobable). Hence the simulation consists of sampling uniform distributed integers between 1 and 6.

\section{Pseudo-Random Numbers}
\label{pseudo-random-numbers}

The need of generating random inputs during a MC simulation requires large amounts of \emph{random numbers} to be generated. A random number is a "randomly chosen" number from a particular distribution.

Their use in Monte Carlo simulations spurred the development of \emph{pseudo-random number generators}. Nowadays every programming language has libraries that allow to produce huge series of random numbers. 

These series, which have very large periodicity (e.g. $2^{20000}$), are calculated by algorithms that take as input a \emph{seed} which determines them uniquely. \textbf{This means that choosing the same seed will produce the same set of numbers every time (which is great for debugging purposes though).}

In \texttt{python} one of the available modules to generate random numbers is \texttt{numpy.random} which has, among others, the following useful functions:
\begin{itemize}
\tightlist
\item \texttt{seed}: set the seed of the random number generator;
\item \texttt{uniform}: returns a random number between 0 and 1 (with uniform probability);
\item \texttt{randint(min,\ max)}: returns an integer random number between \texttt{min} and \texttt{max} (with uniform probability);
\item \texttt{choice(aList,\ k=n, replace=True)}: samples n elements from the list \texttt{aList} with or without replacement.
\item \texttt{normal}: throws random numbers sampled from a normal distribution ($\mathcal{N}(0, 1)$).
\end{itemize}

The following example show few simple applications.

\pythoncodenon{code/monte_carlo_1.py}
\begin{ioutput}
uniform distributed floats with seed 1
0.417022004702574, 0.7203244934421581
uniform distributed floats with seed 2
0.43599490214200376, 0.025926231827891333
uniform distributed floats with seed 1 again
0.417022004702574, 0.7203244934421581
random integer between 1-9: 6
choose two items among aList: ['c' 'd']
\end{ioutput}

\subsection{Sampling from a Distribution}

If we need to sample from a generic distribution it is convenient to use \texttt{scipy.stats}. In there a lot of useful distribution have been defined such that with the convenient method \texttt{rvs(size=n)} (random variable sample) allows to sample as many time as specified by the argument \texttt{size}. 

Below two examples which sample 50000 times from the discrete uniform distribution (\texttt{randint}) and from the normal distribution, the results are shown in Fig.~\ref{fig:gauss_dist}.

\pythoncodenon{code/monte_carlo_2.py}

\begin{figure}[hb]
\centering
\includegraphics[width=0.45\textwidth]{figures/uniform}
\includegraphics[width=0.45\textwidth]{figures/standard_normal}
\caption{Uniform distribution generated with \texttt{numpy.random.randint} function (left). Normal distribution generated with \texttt{scipy.stats.norm} function (right).}
\label{fig:gauss_dist}
\end{figure}

\section{Monte Carlo Simulation Examples}
\label{example-of-monte-carlo-simulation}

In this Section we go through some applications of the Monte Carlo method.

\subsection{Probability to Draw Two Kings From a Deck}

Imagine we would like to determine the probability of drawing consecutively two kings from a deck of cards, at the beginning there are 40 cards (i.e. the entire deck) with 4 possible kings. According to the frequentist approach~\ref{cap:bayes}, we can calculate the probability of an event as the ratio of the number of favourable outcomes ("successes") and all possible outcomes. 

So the probability to get the first king is $4/40=10\%$. Next, assuming we've got a king the first time, we are left with 39 cards and 3 kings only, so the probability to get the second king is $3/39=7.7\%$. Since we want both events to happen, the final probability is the product of the two:

\begin{equation*}
P_\textrm{two kings} = \frac{4}{40} \cdot \frac{3}{39} = \frac{1}{130} \approx 0.77\%
\end{equation*}

Let's now try to estimate the same probability with a MC simulation, following the steps outlined above.

\pythoncode{code/monte_carlo_4.py}

In this case the domain is a deck of cards; to simulate it we can define a list with the values of each card (actually we can create first the list of "cards" for a single suit and then "multiply" the list by four, see line 4). We pick up randomly, one million times, two cards from the virtual deck with uniform probability since the deck is fair (line 8); for debugging purpose the first ten draws are printed on screen. Check if the card pair is \texttt{['K', 'K']} in line 11 and in case increase the counter of successes (line 12). Finally just print \texttt{successes/trials} which is the sought probability.
In the example we are going to set the seed to 1 to make it reproducible.

\begin{ioutput}
['3' '2']
['2' '3']
['7' '3']
['Q' '4']
['7' '5']
['5' 'J']
['4' 'K']
['K' '2']
['5' 'Q']
['A' 'J']

The probability to draw two kings is 0.0076
\end{ioutput}

The result is in agreement with the theoretical expectations, further considerations about this result in Section~\ref{sec:confidence_interval}.

\subsection{Determine $\pi$}
\label{determine-pi}

We know what the result has to be: $\pi\approx 3.141592653589793\ldots$ and in order to get an estimate through MC simulation of this famous constant a geometric approach can be considered. Imagine a circle of radius $r$ which is \emph{inscribed} in a square with side length $2r$, see Fig.~\ref{fig:circle_inscribed}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.5\textwidth]{figures/circle_inscribed}
\caption{A circle of radius $r$ inscribed in a square of side length $2r$.}
\label{fig:circle_inscribed}
\end{figure}

Computing the ratio of the two figure areas and solving for $\pi$ we get

\begin{equation}
\frac{\text{Area Circle}}{\text{Area Square}} = \frac{\pi r^2}{4r^2} = \frac{\pi}{4} \implies \pi \approx \cfrac{4\cdot\text{Area Circle}}{\text{Area Square}}
\end{equation}

The algorithm to approximate $\pi$ should then be

\begin{itemize}
\item select 2 random numbers, $x_1$ and $x_2$, from the interval $[-r,r]$; 
\item determine if the point $(x_1, x_2)$ lies within or on the circle, keeping track both of the total number of tested points and of those satisfying the condition $\sqrt{x_1^2 + x_2^2}\leq r$; 
\item approximate the ratio of the areas by the number of points within or on the circle divided by the total number of tested points; 
\item multiply the approximated area by 4 to get $\pi$.
\end{itemize}

\pythoncodenon{code/monte_carlo_5.py}
\begin{ioutput}
Approx. pi: 3.1524
\end{ioutput}

\section{Accuracy of Monte Carlo Simulation}
\label{sec:confidence_interval}

A general consideration that we can make is that the lower is the probability we try to estimate with MC the higher has to be the number of simulations. The result precision indeed depends on the amount of "successes" and if the success probability is small, many trials are needed. 

This can be simply checked by playing with the number of simulations in the previous examples. We may run an entire set without getting two consecutive kings, nevertheless we cannot conclude there is zero probability of such event.

The only conclusion we can draw is that, despite its undoubted power, Monte Carlo simulation is not always the best approach to follow, especially for computationally heavy experiments. Many times indeed the simulation of a single experiment requires a lot of computing resources (and time) and it may not be practical to embark into such a large simulation.

In the following example it is apparent how increasing the number of simulations in a single experiment the approximation precision improves. In Fig.~\ref{fig:circle_approx} it is shown graphically the results with 100, 1000, 10000, 100000 and 1000000 simulations.

\pythoncodenon{code/monte_carlo_5_2.py}
\begin{ioutput}
pi=3.120000 0.0001s
pi=3.096000 0.0012s
pi=3.121200 0.0147s
pi=3.139560 0.1997s
pi=3.142136 1.0671s
\end{ioutput}

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{figures/mc_vs_n_experiments}
\caption{Graphical representation of geometrical approximation of $\pi$. Results are reported for $N = 100, 1000, 10000, 100000, 1000000$ simulations. It is clear how the result improves with $N$.}
\label{fig:circle_approx}
\end{figure}

In this case the simulation is so simple that even in the larger case ($N=1000000$) the program runs for only about a second.

Now let's go back to the card example and imagine that you didn't know how much is the probability of getting two consecutive K from a deck of 40 cards. 

\pythoncodenon{code/monte_carlo_4_2.py}
\begin{ioutput}
0.0084
\end{ioutput}

What can be concluded from the result of this single MC experiment ? Unfortunately not much, but a famous theorem come to rescue us.
The Central Limit Theorem~\cite{bib:central_limit} (CLT) states that if we have $X_1, X_2,\dots, X_n$ which are $n$ random samples from a distribution $X$ with true mean $\mu$ and variance $\sigma^{2}$, then with $n$ sufficiently large,

\begin{equation*} 
\mu_n = \cfrac{X_1 + X_2 +\ldots + X_n}{n} = \cfrac{1}{n}\sum_i^n X_i
\end{equation*}
has approximately a normal distribution $\mathcal{N}(\mu, \sigma^2/n)$.

This means that if one repeats the same MC experiment many times (changing the random number generator seed otherwise we are always repeating the \textbf{same} experiment) would obtain results normally distributed around the \textbf{true} value $\mu$. \emph{Also the best estimate $\mu_n$ of the unknown parameter $\mu$ is the mean of the MC experiment results.}

Another very important consequence of the CLT is that the standard deviation of our estimate $\mu_n$ is \emph{the standard deviation of the sample ($\sigma_{Y_i}$) divided by the square root of the sample size}. Indeed
\begin{equation*}
\begin{gathered}
\bar{X} = \cfrac{X_1 + X_2 +\ldots +X_n}{n}\\
\var(\bar{X}) = \var\left(\cfrac{X_1 + X_2 +\ldots +X_n}{n}\right)=\cfrac{\var(X_1)}{n^2}+\cfrac{\var(X_1)}{n^2}+\ldots+\cfrac{\var(X_n)}{n^2} = \\
=\cfrac{1}{n^2}[\sigma^2+\sigma^2+\ldots+\sigma^2] = \cfrac{\sigma^2}{n}
\end{gathered}
\end{equation*}

This is sometimes called the standard deviation of the sample mean (contrary to the standard deviation of a sample). Hence, the sample mean is much less volatile than the sample values themselves. Consequently, the error reduces at the rate of 1 over the square root of the sample size. 

Notice what this means, for increasing the sample accuracy by increasing the sample size. Suppose $\sigma$ is the standard deviation of the sample. We first conduct a Monte Carlo simulation using $n_1$ experiments. The standard deviation of our estimate of the unknown value is $\cfrac{\sigma}{\sqrt{n_1}}$. Now suppose we wanted to reduce that uncertainty in half. How much larger must the sample be? Let this new sample size be $n_2$. Then the standard deviation of the estimate becomes $\cfrac{\sigma}{\sqrt{n_2}}$.
Finally note that,
\begin{equation*}
  \cfrac{1}{2}\cfrac{\sigma}{\sqrt{n_1}}  = \cfrac{\sigma}{\sqrt{n_2}} \implies n_2 = 4 n_1
\end{equation*}

Thus, to achieve a 50\% reduction in error, i.e., a 50\% increase in accuracy, we must \textbf{quadruple} the number of random experiments.

\subsection{Central Limit Theorem in the Kings Example}

We can check the Central Limit Theorem by repeating many times the MC experiment with our virtual deck and see how the distribution of $\mu_n$ behaves.

\pythoncodenon{code/monte_carlo_4_3.py}
\begin{ioutput}
mu_true: 0.007692
mu: 0.007640 +- 0.000151
\end{ioutput}

Looking at Fig.~\ref{fig:repeated_MC} it is clear that the various results are distributed as a Gaussian (in this example the distribution is still a bit asymmetric but it will eventually become Gaussian running more simulations). Also playing with the parameter experiments it easy to see how the precision of the estimate increase always staying within the assigned error interval.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/experiment_distribution}
\caption{Distribution of the results of 1000 experiments.}
\label{fig:repeated_MC}
\end{figure}

\subsection{Confidence Interval}

From the Central Limit Theorem we know that: 

\begin{equation}
\mu_n - \mu \approx \mathcal{N}(0, \sigma^2/n)
\end{equation}

From the previous equation and remembering the definition of quantiles~\ref{sec:quantile-function} it is then possible to define an interval such that there is a certain probability to find $\mu$ in there. Referring to Fig.~\ref{fig:confidence_interval} we can write:

\begin{equation}
P\left(\mu_n - 1.96\frac{\sigma}{\sqrt{n}}\le \mu \le \mu_n + 1.96\frac{\sigma}{\sqrt{n}}\right) = 0.95
\end{equation}
(which corresponds to the pink area).

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/confidence_interval}
\caption{Confidence interval for a MC experiment.}
\label{fig:confidence_interval}
\end{figure}

This interval is called \textbf{95\% confidence interval} because it covers 95\% of the total area under the Gaussian. It can be interpreted like the following: if you repeat many times a simulation, the fraction of calculated confidence intervals that contains the true parameter $\mu$ would tend toward 95\%.

The most commonly used intervals are 99\% and 95\% confidence level and are respectively defined as \(\pm \cfrac{2.57\sigma}{\sqrt{n}}\) and \(\pm \cfrac{1.96\sigma}{\sqrt{n}}\).

To construct an interval with a custom confidence level $\alpha$, we have to find a number $A$ such that
\begin{equation}
\Phi(A) = 1 - \frac{1-\alpha}{2}\quad\implies\quad A = \Phi^{-1}\left(\cfrac{1+\alpha}{2}\right)
\label{for:A}
\end{equation}
then the $\alpha$-confidence interval guarantees
\begin{equation}
P(\mu - A\sigma \le X \le \mu+ A\sigma) = \alpha 
\end{equation}

Below an example of how to compute a confidence level in \texttt{python} given a set of simulation results (the inverse of the Gaussian CDF $\Phi^{-1}$, used in Eq.~\ref{for:A}, is computed with the method \texttt{norm.ppf()}, see Section~\ref{sec:quantile-function}).

\pythoncodenon{code/monte_carlo_6.py}
\begin{ioutput}
95% confidence int.: 4.437 +- 0.812
\end{ioutput}

\section{Integration by Monte Carlo}
\label{sec:integration}

\begin{finmarkets}
Function integration in \texttt{python} can be performed using \texttt{scipy.integrate.quad}. This method takes in input: the function to integrate, the integration limits and, if needed, optional arguments for the integrand (see Section~\ref{sec:kwargs_args}). Since it is a numerical integration the result will be a tuple with the actual result and its associated error.

For example imagine to integrate
\begin{equation*}
f(x) = \int_{0}^{\infty}\frac{e^{-x}}{\left(a + (x-1)^2\right)}~dx
\end{equation*}
for $a=3$.

\pythoncodebox{code/monte_carlo_7.py}
\begin{ioutput}
(0.2802620835548487, 1.5887625490132047e-09)
\end{ioutput}
\end{finmarkets}

Sometimes it may be useful to calculate the approximate result of an integral of a function $f$ through Monte Carlo simulation. The technique relies on the fact that the average of $f$ calculated in the integration interval can be expressed as

\begin{equation}
<f(x)> = \frac{1}{b-a} \int_{a}^{b} f(x) \,dx
\end{equation}
Rearranging the terms
\begin{equation}
\int_{a}^{b} f(x) \,dx = 
(b-a) <f(x)> \approx (b-a) \frac{1}{N} \sum_{i=1}^{N} f(x_i)
\end{equation}
Then the integral can be computed by sampling values of $x$ from a uniform distribution between $[a,b]$, average the evaluations of $f(x)$ and then multiplying the result by $(b-a)$.

Considering the previous integral

\pythoncode{code/monte_carlo_8.py}
\begin{ioutput}
0.28061306413101306
\end{ioutput} 
 
Although there are various improvements that can be added to our simple MC implementation, the result that we obtained is in good agreement with the one from the \texttt{python} implementation.  
 
\section*{Exercises}
\input{monte_carlo_ex_text}









