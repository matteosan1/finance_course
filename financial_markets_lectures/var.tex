\chapter{Risk Measures}
\label{var-and-credit-risk}

In quantitative finance arguably one of the most important challenge is the financial risk measurement and assessment.
The majority of the financial engineer’s task is connected to this field.
%Risk Measurement for a financial institution is very data intense and computational expensive process. Using the adequate, state of the art technologies is key to keep up with the speed of the financial market innovation, and to be able to meet the regulatory requirements.

There is a wide range of risk measures that spread through the Market: volatility ($\sigma$), Value at Risk (VaR), Expected Shortfall (ES) (Conditional Value at Risk), Average Drawdown, Maximum Drawdown, $\ldots$
In this Chapter we will concentrate specifically on two of them: VaR and ES.

\section{Portfolios}

A \emph{portfolio} is a collection of financial investments like stocks, bonds, commodities,\ldots
It is fully described by the \emph{weights} $w_i$ which represent the fraction of the total wealth that has been invested in asst $i$. Clearly it must hold $\sum_i w_i = 1$. 
Some notations that will be used later on are

\begin{itemize}
	\tightlist
	\item
	portfolio expected return: 
	\begin{equation} 
		\mathbb{E}(R_{p}) = \sum _{i}w_{i} \mathbb{E}(R_{i}) = \mathbf{w}\cdot \mathbb{E}(\mathbf{R}) = \mathbf{w}^T \mathbb{E}(\mathbf{R})=
		\begin{bmatrix}
			w_1 \\ 
			w_2 \\ 
			\vdots \\
			w_n
		\end{bmatrix}
		\begin{bmatrix}
			\mathbb{E}(R_1) & \mathbb{E}(R_2) & \cdots & \mathbb{E}(R_n)
		\end{bmatrix}
	\end{equation} 
	where \(R_{p}\) is the return on the portfolio, \(R_{i}\) is the return on asset \(i\) and \(w_{i}\) is the weighting of component asset \(i\) (that is, the proportion of asset \(i\) in the portfolio) and \(\sum_{i}w_i = 1\) and \(0 \le w_i \le 1\);
	\item
	portfolio return variance:
	\begin{equation}
		\begin{aligned}
			\sigma _{p}^{2} = &\sum _{i}\sum _{j}w_{i}w_{j}\sigma _{ij} = \mathbf{w}^T\Sigma\mathbf{w} =
			\begin{bmatrix}
				w_1 \\ 
				w_2 \\ 
				\vdots \\
				w_n
			\end{bmatrix}
			\begin{bmatrix}
				\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
				\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
				\vdots & & \\
				\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn} \\
			\end{bmatrix}
			\begin{bmatrix}
				w_1 & w_2 & \cdots & w_n
			\end{bmatrix} =\\ 
			&\begin{bmatrix}
				\sigma_{11} *w_1 + \sigma_{12} *w_2 + \cdots + \sigma_{1n}*w_n \\
				\sigma_{21} *w_1 + \sigma_{22} *w_2  +\cdots + \sigma_{2n}*w_n \\
				\vdots \\
				\sigma_{n1} *w_1 + \sigma_{n2}*w_2 + \cdots + \sigma_{nn}*w_n \\
			\end{bmatrix}
			\begin{bmatrix}
				w_1 & w_2 & \cdots & w_n
			\end{bmatrix}
		\end{aligned}
	\end{equation}
	where \(\sigma\) is the (sample) standard deviation of the periodic returns on an asset, and \(\rho _{ij}\) is the correlation coefficient between the returns on assets \(i\) and \(j\). For a brief introduction to matrices see Chapter~\ref{sec:matrices};
	\item
	portfolio return volatility (standard deviation):
	\begin{equation}
		\sigma _{p}= \sqrt{\sigma _{p}^{2}}
	\end{equation}
\end{itemize}

\pythoncodenon{code/var_portfolio.py}
\begin{ioutput}
Portfolio return 0.3003
Portfolio variance 0.0452
Portfolio std. dev. 0.2126
\end{ioutput}

\section{VaR and Expected Shortfall}
\label{value-at-risk}

\emph{Value-at-risk} (VaR) is defined as the loss level that will not be exceeded with a certain confidence level during a certain period of time.
\begin{equation}
\text{VaR}_{\alpha}(X) = -F^{-1}_X(\alpha)
\label{eq:var}
\end{equation}
Mathematically it is the loss corresponding to the $(100-X)\textrm{th}$ percentile of the portfolio returns over the next $N$ days (e.g. in Figure~\ref{fig:var_loss} the graphical representation of the VaR, with $N=1$ and $X=95$, is shown; in the example a normal distribution for the changes in value is assumed). By definition it is a function of two parameters: the time horizon (i.e. $N$ days usually set to 1) and the confidence level (usually 95\%), see Fig.~\ref{fig:var_loss}. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\linewidth]{figures/95_var}
\caption{Example of $VaR$ on the distribution of changes in value.}
\label{fig:var_loss}
\end{figure}

For example, if a bank's 10-day 99\% VaR is 3 million USD, there is considered to be only a 1\% chance that losses will exceed 3 million USD in 10 days. One problem with VaR is that, when used in an attempt to limit the risks taken by a trader, it can lead to undesirable results. Suppose a bank tells a trader that the one-day 99\% VaR of the trader's portfolio must be kept less than 10 million USD. There is a danger that the trader will construct a portfolio where there is a 99\% chance that the daily loss is less than 10 million USD and a 1\% chance that it is 500 million USD. The trader is satisfying the risk limits imposed by the bank, but is clearly taking unacceptable risks. 
The problem is summarized in Figure~\ref{fig:var_vs_badvar}. It shows the probability distribution for the gain or loss on a portfolio over a specified period of time. Both portfolios have the same VaR. However, the portfolio on the right is much riskier than the portfolio on the left because potential losses are much larger. 

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\linewidth]{figures/var_badvar}
\caption{Probability distribution for the gain or loss on a portfolio over a specified period of time. Both portfolios have the same VaR. However, the portfolio on the right is much riskier than the portfolio on the left because potential losses are much larger.}
\label{fig:var_vs_badvar}
\end{figure}

A measure that produces better incentives for traders than VaR is \emph{expected shortfall} (ES). This is also sometimes referred to as conditional VaR. Where VaR asks the question 'how bad can things get?', expected shortfall asks 'if things do get bad, what is our expected loss?'. It is the expected loss during an N-day period, conditional that the loss is greater than the $X^{th}$ percentile of the loss distribution
\begin{equation}
\text{ES}_{\alpha}(X) = \frac{1}{\alpha}\int_0^\alpha \text{VaR}_p(X) dp
\label{eq:es}
\end{equation}

For example, with $X = 99$ and $N = 10$, the expected shortfall is the average amount that is lost over a 10-day period, assuming that the loss is greater than the $99^{th}$ percentile of the loss distribution.  Clearly, the expected shortfall is much higher in Figure~\ref{fig:var_vs_badvar} (right) than (left). 

Expected shortfall is the negative of the expected value of the tail beyond the VaR, hence it is always a larger number than the corresponding VaR, see Figure~\ref{fig:es_loss}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\linewidth]{figures/es}
\caption{Example of 95\% VaR on the distribution of changes in value.}
\label{fig:es_loss}
\end{figure}

Regulators make extensive use of VaR and its importance as a risk measure is therefore unlikely to diminish. However, expected shortfall has a number of advantages over VaR, and this has led many financial institutions to use it as a risk measure internally. 

\section{How to Compute Risk Measures}
\label{how-to-estimate-the-var}

Throughout this Section the historical series of following tickers AAPL, CSCO, GOOG, INTC, NVDA are used to create an equally weighted portfolio. The dataset with their historical series between January 2023 and September 2024 is available in \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/input_files/historical_data.csv}{historical\_data.csv}.

\subsection{Parametric VaR, ES}

The historical performance of the portfolio can be determined by the series of the constituents. It has been checked that the return distributions are all approximately Gaussian, see Fig.~\ref{fig:asset_return_distro}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/asset_return_distro}
	\caption{Stacked histogram showing the return distributions of the asset involved in the studied portfolio. The overall distribution is indeed a scaled version of the historical performance histogram of the portfolio. All the returns are approximately distributed as a Gaussian.}
	\label{fig:asset_return_distro}
\end{figure}

\pythoncode{code/var_var_continuous.py}

Once the dataframe has been loaded the portfolio returns are calculated, lines 6-7.

Many influential theories in finance assume the normality of stock returns: portfolio theory and CAPM (where normality of returns implies that only the mean and variance are relevant for portfolio optimization), option pricing theory (Black and Scholes model assumes (log-)returns follow a Brownian motion, i.e., are normally distributed over any given time interval).
Unfortunately normality is not a realistic assumption as can be seen by performing a Gaussian fit to the return distribution of the portfolio.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/parametric_var}
	\caption{Return distribution obtained from the historical data used in the example. The red line shows the 99\% VaR. The distribution has also been fitted with a Gaussian and a t-student and the results are shown with the dashed lines.}
	\label{fig:paramtric_var}
\end{figure}

Red line in Figure~\ref{fig:paramtric_var} shows the resulting fit (line 23, see~\ref{sec:how_to_fit}) which clearly doesn't follow well the underlying distribution, so returns are \textbf{not} normally distributed. Much better results can be obtained with a t-student distribution (line 27), the fit is shown by the green line. 

\begin{finmarkets}
The \texttt{finmarkets} module has two function to compute VaR and ES in the continuous case. Since the functional form of the returns has been determined independently we can apply directly the definitions of  Eqs.~\ref{eq:var} and~\ref{eq:es}. 
For details about how to perform the integral in Eq.~\ref{eq:es} refer to Section~\ref{sec:integration}.
\end{finmarkets}

Below are reported the resulting VaR and ES using the normal and the t-student models, computed passing in turn the two functions the fitted model. Of course the better the model used to describe the actual histogram the more precise will be the resulting risk measurement.
  
\begin{ioutput}
Gaussian
1d-95% VaR: 0.022
1d-95% ES: 0.02801

t-student
1d-95% VaR: 0.02115
1d-95% ES: 0.029971
\end{ioutput}

\subsection{Historical Simulation}
\label{historical-simulation}

When it is not obvious which distribution the portfolio returns belong to, it is possible to avoid any assumption by random sampling on the historical returns directly. The obvious advantage of this technique is that no parameter estimation is required, the drawback being that such kind of simulation heavily relies on the assumption that past behaviors are indicative of what might happen in the future. That's why it is important that out historical series was as large as possible otherwise our results will be affected by lack of statistics. 

\begin{finmarkets}
The \texttt{finmarkets} module has also two functions to compute VaR and ES for the discrete case. They rely on the sampling from the original historical series which has been implemented in a separated utility function. The fastest way to generate random sample of historical returns is to assign an integer for each observation, and, using a random sampling algorithm, we can generate the set of simulated returns. Then the risk measures are calculated as percentiles of the the generated distributions.
\end{finmarkets}

\pythoncodenon{code/var_var_historical.py}
\begin{ioutput}
1d-95% VaR (discrete): 0.0201
1d-95% ES (discrete): 0.0319
\end{ioutput}

Figure~\ref{fig:historical_var} shows the results. Notice again that the generated return distributions suffers from the same limitation of the original historical series (e.g. missing data in specific regions).

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{figures/historical_var}
\caption{Distribution of the return random sampling. The corresponding $VaR$ and $ES$ are also shown.}
\label{fig:historical_var}
\end{figure}

\section{Credit VaR}
\label{credit-var-cr-var}

%exposure at any given future time is the
%larger between zero and the market value of the portfolio of derivative
%positions with a counterparty that would be lost if the counterparty
%were to default with zero recovery at that time.

\emph{Credit VaR} is defined in the usual way other risk measures are (i.e. as percentile of a loss distribution). In this case we are concerned with the default risk associated to one or multiple counter-parties in a specific portfolio instead with to the market risk.

To derive the loss distribution we need to consider the exposure $\text{EE}(\tau)$ defined as the sum of the discounted cash flows at the default date $\tau$. The corresponding loss is then given by

\begin{equation}
L_{\tau} = (1 - R) \cdot \textrm{EE}(\tau)
\end{equation}
where $L$ is non-zero only in scenarios of early counter-party default. 

Given the above definitions we can express the Credit VaR as the (100-X)-quantile of $L_{\tau}$. With respect to the Value at Risk, the time horizon is usually set to one year and the percentile to $99.9^{th}$, so it returns the loss that is exceeded only in 1 case out of 1000. 

Note that in this case quantile has to be computed on the "positive" tail of the loss distribution, because $L$ represents the loss value (i.e. it is a positive quantity).

Credit VaR is actually either the difference of the percentile from the mean, or the percentile itself. There is more than one possible definition, anyway we  will use the latter.

\subsection{Credit VaR and MC Simulation}
\label{sec:credit-var-sim}

Credit VaR can be calculated through a simulation of the evolution of a portfolio up to the risk horizon; the simulation must of course include possible defaults of the counter-parties. In each experiment the portfolio is priced obtaining a number of scenarios to draw the loss distribution. It is then straightforward to derive the Credit VaR.

%Consider, for example, a portfolio of five fixed coupon bonds (0.1) each one with the same default probability (a Poisson process with $\lambda=0.5$) and the same face value (100 EUR). The recovery rate in case of default is $R=40\%$ and the discount curve is the one stored in \href{https://github.com/matteosan1/finance_course/raw/master/input_files/discount_factors_2022-10-05.xlsx}{discount\_curve.xlsx}.
%
%In order to better simulate the evolution of the portfolio a helper function \texttt{Bond} is implemented: it is just able to compute the bond NPV at a given date. Also a set of default times has been calculated using \texttt{PoissonProcess} distribution.
%Notice that in this simple example there is no need to run multiple scenarios since any market parameters of the portfolio is changing and no evolution is required (e.g. interest rate is fixed).
% 
%\begin{ipython}
%from finmarkets import generate_dates, maturity_from_str
%
%class Bond:
%    def __init__(self, face_value, start_date, tenor, maturity):
%        self.FV = face_value
%        self.tau = maturity_from_str(tenor, "y")
%        self.payment_dates = generate_dates(start_date, maturity, tenor)
%
%    def npv(self, d, dc, fc):
%        val = 0
%        for i in range(1, len(self.payment_dates)):
%            C = fc.forward_rate(self.payment_dates[i-1], self.payment_dates[i])
%            if self.payment_dates[i-1] <= d < self.payment_dates[i]:
%                rateo = (self.payment_dates[i] - d).days/
%                (self.payment_dates[i] - self.payment_dates[i-1]).days
%                val += C*rateo*self.tau*dc.df(d)
%            elif self.payment_dates[i] > d:
%                val += C*self.tau*dc.df(self.payment_dates[i])
%                val += dc.df(self.payment_dates[-1])
%        return self.FV*val
%\end{ipython}
%\begin{ipython}
%import pandas as pd
%import numpy as np
%from datetime import date
%from dateutil.relativedelta import relativedelta
%from finmarkets import DiscountCurve, CreditCurve, ForwardRateCurve
%from finmarkets import PoissonProcess, generate_dates
%    
%start_date = date.today()
%df = pd.read_excel("discount_curve.xlsx")
%pillars = [start_date + relativedelta(months=i) for i in df['months']]
%dc = DiscountCurve(pillars, df['dfs'])
%
%bonds = []
%maturity = "2y"
%
%fc_dates = generate_dates(start_date, maturity, "1y")
%rates = [0.1 for _ in range(len(fc_dates))]
%fc = ForwardRateCurve(start_date, fc_dates, rates)
%
%for i in range(len(coupons)):
%    bonds.append(Bond(100, start_date, coupons[i], 1, maturity))
%
%np.random.seed(1)
%N = 100000
%R = 0.4
%horizon = 1
%
%Q = PoissonProcess(l=0.5)
%cov = np.ones(shape=(len(bonds), len(bonds)))*rho
%np.fill_diagonal(cov, 1)
%g = GaussianCopula(len(bonds), cov)
%taus = Q.ppf(g.sample(N))
%
%L = []
%for n in range(N):
%    EE = 0
%    for i, b in enumerate(bonds):
%        if taus[n, i] <= horizon:
%            d = start_date + relativedelta(days=365*taus[n, i])
%            EE += b.npv(dc, d)
%    if EE != 0:
%        L.append((1-R)*EE)
%  
%print (np.percentile(L, [95.0]))
%\end{ipython}
%\begin{ioutput}
%338.0718924883006
%\end{ioutput}
%\noindent
%Figure~\ref{fig:credit_var} shows the loss distribution of the portfolio. Each peak corresponds to scenarios where there is an increasing number of defaults and is located roughly at multiple of 60 since each bond value is around 100 which gives a loss of that size (i.e. $1-R$ with $R\simeq$40\%). 
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.5\textwidth]{figures/credit_var}
%\caption{Distribution of losses in a portfolio made of five fixed coupon bonds. In the zoom the tail of the plot is shown to highlight the Credit VaR value.}
%\label{fig:credit_var}
%\end{figure}

\subsection{Credit VaR and One Factor Copula Model}
Consider a portfolio made of similar assets. As an approximation assume that the probability of default is the same for each counter-party and that the correlation between each pair is the same and equal to $\rho$. Under these assumption the One Factor Copula model can be used to describe the default correlations (see Eq.~\ref{eq:gaussian_one_factor_copula})

\begin{equation}
\mathcal{Q}_M(T) = \Phi\Big(\cfrac{\Phi^{-1}[Q(T)]-M\sqrt{\rho}}{\sqrt{1-\rho}}\Big)
\label{eq:conditional_default_prob}
\end{equation}
where $\Phi$ is the cumulative distribution function of the standard normal.

Since there are $n$ counter-parties with the same default probability $\mathcal{Q}_M(T)$ the percentage of defaults at time $T$ is $\mathcal{Q}_M$ itself ($\textrm{\% of defaults} = \textrm{n\_defaults}/n = n\cdot \mathcal{Q}_M/n$). Hence Eq.~\ref{eq:conditional_default_prob} gives the percentage of defaults by time $T$ given $M$. 

Since $M$ is distributed according to a standard Gaussian we can be $X\%$ certain that its value will be \emph{greater} than $\hat{m} = \Phi^{-1}(1-X)=-\Phi^{-1}(X)$, where the equality holds due to the symmetry of the Gaussian distribution (see Figure~\ref{fig:certain_for_X}).

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/certain_for_X}
\caption{$X\%$ probability to get an higher value a threshold for a normally distributed random variable.}
\label{fig:certain_for_X}
\end{figure} 

Once the time $T$ has been chosen the only random variable appearing in the conditional default probability expression of Eq.~\ref{eq:conditional_default_prob} is $M$, therefore we can be $X\%$ certain that the percentage of defaults over $T$ years on a large portfolio will be \textbf{less} than $V(X,T)$ where

\begin{equation*}
V(X,T)= \Phi\Big(\cfrac{\Phi^{-1}[Q(T)]-\hat{m}\sqrt{\rho}}{\sqrt{1-\rho}}\Big) = \Phi\Big(\cfrac{\Phi^{-1}[Q(T)]+\Phi^{-1}(X)\sqrt{\rho}}{\sqrt{1-\rho}}\Big)
\end{equation*}

When the confidence level is $X\%$ and the time horizon is $T$, a rough estimate of the Credit VaR is therefore $P(1-R)V(X,T)$, where $P$ is the portfolio size and $R$ is the recovery rate.

Suppose that a bank has a total of \euro{100} million of retail exposures. The 1-year probability of default averages to 2\% and the recovery rate averages to 60\%. The copula correlation parameter is estimated as 0.1.
The 1-year 99.9\% Credit VaR is therefore \euro{5.13} million.

\subsection{CreditMetrics}
Another popular approach to estimate Credit VaR is \emph{CreditMetrics}. It involves computing a probability distribution of credit losses by carrying out Monte Carlo simulations of the counter-party credit rating changes.

Imagine we would like to determine the probability distribution of losses over 1-year period. On each simulation, we are going to determine the credit rating of each counter-party using the estimated probability of migration between one rate to another (or to default). Since the portfolio value depends on its asset ratings we can determine the eventual losses. 

As an example consider Table~\ref{tab:credit_ratings} which shows the percentage probability of a bond moving from one category to another during a 1-year period.

\begin{table}[htb]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
Initial rating & AAA & AA & A & BBB & BB & B & CCC & default \\
\hline
\hline
AAA & 90.81 & 8.33 & 0.68 & 0.06 & 0.08 & 0.02 & 0.01& 0.01 \\ 
\hline
AA & 0.70 & 90.65 & 7.79 & 0.64 & 0.06 & 0.13 & 0.02 & 0.01 \\ 
\hline
A & 0.09 & 2.27 & 91.05 & 5.52 & 0.74 & 0.26 & 0.01 & 0.06 \\ 
\hline
BBB & 0.02 & 0.33 & 5.95 & 85.93 & 5.30 & 1.17 & 1.12 & 0.18 \\
\hline
BB & 0.03 & 0.14 & 0.67 & 7.73 & 80.53 & 8.84 & 1.00 & 1.06 \\
\hline
B & 0.01 & 0.11 & 0.24 & 0.43 & 6.48 & 83.46 & 4.07 & 5.20 \\
\hline
CCC & 0.21 & 0 & 0.22 & 1.30 & 2.38 & 11.24 & 64.86 & 19.79 \\		
\hline
\end{tabular}
\caption{Example of table with transition probabilities (in percent) between different credit rating categories.}
\label{tab:credit_ratings}
\end{table}

For a correct implementation of this technique credit rate changes cannot be assumed independent, hence a copula approach could be implemented.
Another possibility is the application of Markov chains~\ref{sec:markov_chain}, with the transition matrix which can be deduced by the numbers in Table~\ref{tab:credit_ratings}.

In fact the main difficulty in this application is precisely the determination of the transition matrix. These probabilities could be estimated by analyzing historical data from credit rating agencies, such as Standard\&Poor, Moody's and Fitch. But this could lead to unreliable numbers in case the future does not develop as smoothly as the past. It can therefore be more reliable to base the estimates on a combination of empirical data and more subjective, qualitative data such as opinions from experts. This is because the market view is a mixture of beliefs determined by both historical ratings and a more extreme view of the ratings. 

Another problem with deciding the transition matrix is that maybe it is not appropriate to use a \emph{homogeneous} Markov chain to model credit risk over time. In this kind of chain the transition matrix is considered constant, but it is clearly a crude approximation of reality which doesn't capture the time-varying behavior of the default risk. A non-homogeneous model could be more realistic, but on the other hand more complicated to use. 

\section{Credit Valuation Adjustment}
\label{credit-valuation-adjustment}

Suppose you have a portfolio of derivatives. If a counter-party defaults and the present value of the portfolio at default is positive to the surviving party (you), then the actual gain is only given by the recovery fraction of the value. If however the present value is negative to you, you have to pay it in full to the liquidators of the defaulted entity.

This behavior creates an asymmetry which can be corrected by changing the definition of the deal value as the value without counter-party risk minus a positive adjustment, called \emph{Credit Valuation Adjustment} (CVA).

The CVA can be expressed in the following way:

\begin{equation}
\text{CVA} = (1-R) \int_0^T D(t) \cdot \textrm{EE}(t) dQ(t)
\label{eq:cva}
\end{equation}
where $T$ is the latest maturity in the portfolio, $D$ is the discount factor, EE is the expected exposure or $\mathbb{E}[\max(0, NPV_{portfolio})]$, and $dQ$ is the probability of default between $t$ and $t+dt$.

For an easier computation it is natural to discretize the above integral and use a time grid going from 0 to the portfolio maturity:

\begin{equation}
\text{CVA} = (1-R) \sum_i^n D(t_i) \cdot \mathrm{EE}(t_i) Q(t_{i-1}, t_i)
\label{eq:cva_discrete}
\end{equation}

It is important to note the difference between Credit VaR and CVA. While the former measures the risk of losses faced due to the possible default of some counter-party, the latter measures the pricing component of this risk, i.e. the price adjustment of a contract due to this risk.

\subsection{Debit Valuation Adjustment}

The adjustment seen from the point of view of our counter-party is positive, and is called Debit Valuation Adjustment, DVA. It is positive because the early default of the client itself would imply a discount on its payment obligations, and this means a gain. So the client marks an adjustment over the risk free price by adding the positive amount called DVA. 

When both parties have non-null default probabilities, they consistently include both defaults into the valuation. So they will mark a positive CVA to be subtracted and a positive DVA to be added to the default risk free price of the deal. The CVA of one party will be the DVA of the other one and vice versa.

\begin{equation*}
\textrm{price}=\textrm{default risk free price + DVA - CVA}
\end{equation*}
Now, since
\begin{equation*}
\begin{gathered}
\textrm{default risk free price(A)} = - \textrm{default risk free price(A)} \\
\textrm{DVA(A)} = \textrm{CVA(B)} \\
\textrm{DVA(B)} = \textrm{CVA(A)} 
\end{gathered}
\end{equation*}
we get that eventually
\begin{equation*}
\textrm{price(A)} = -\textrm{price(B)}
\end{equation*}
so that both parties agree on the price, or, we could say, there is money conservation.

\subsection{CVA Computation}

The computation of the CVA is carried on with Monte Carlo simulation. 
First calculate the portfolio value at each time point for each MC scenario. Then estimate the CVA using either Eq.~\ref{eq:cva} or its discrete form in Eq.~\ref{eq:cva_discrete}. Finally average the CVA of all the scenarios to get its value.

\subsubsection{Example}
Compute the CVA of a 10-years receiver interest rate swap on a notional of 1M€ exchanging 3.5\% against 6m LIBOR.
Your counterparty has a non zero default probability modeled according to the following credit curve:
\begin{table}
	\begin{center}
	\begin{tabular}{l|c}
Years & $\Psur$ \\
\hline
1 & 90\% \\
\hline
2 & 80\% \\
\hline
3 & 70\% \\
\hline
4 & 60\% \\
\hline
$\ge5$ & 50\% \\
\end{tabular}
\end{center}
\end{table}

The recovery rate is 40\%. The risk free rate is instead 3\% flat.

\begin{finmarkets}
Up to now we have always assumed that the valuation date and the start date of the contract are the same.
Since the CVA simulation requires to compute the swap value at various date it is instructive to look again at the \texttt{InterestRateSwap} class implementation, in particular at the annuity method:

\begin{ipythonbox}
...
    def annuity(self, dc, current_date=None):
        if current_date is None:
            current_date = self.fix_dates[0]

        a = 0
        for i in range(1, len(self.fix_dates)):
            if current_date > self.fix_dates[i]:
                continue
            tau = (self.fix_dates[i]-self.fix_dates[i-1]).days/360
            a += tau*dc.df(self.fix_dates[i])
        return a
\end{ipythonbox}

Recalling the definition of the annuity ($\sum_{i=1}^{n}D(d, d_{i}^{\mathrm{fixed}}$), we pass to the method an optional parameter (\texttt{current\_date}) which specifies the evaluation date (by default this is assumed to be the start date of the contract). In such a way we exclude from the summation all the dates in the past to give the correct valuation when this those not happen at the start date.
\end{finmarkets}

That said the following code reports the entire simulation
\pythoncode{code/var_cva.py}

\begin{ioutput}
CVA: 732.29
CVA as fraction of notional: 0.00
\end{ioutput}

\emph{Expressing the CVA as a fraction of the notional can be a useful way to compare the credit risk of different contracts.}
This percentage represents the expected loss due to credit risk as a proportion of the contract's nominal value.
Figure~\ref{fig:cva_exposure} reports the evolution of the exposure in the swap as time flows.

\begin{figure}[htb]
	\centering
 	\includegraphics[width=0.65\textwidth]{figures/cva_exposure}
 	\caption{Swap exposure as a function of time.}
 	\label{fig:cva_exposure}
\end{figure} 

Of course this example offers an over-simplification since we are assuming a constant flat risk-free rate and that the 6m forward rate is actually going to be realized. A more accurate simulation would imply to simulate the evolution of the interest rates.

\section*{Exercises}
\input{var_ex_text}
