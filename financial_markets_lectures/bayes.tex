\chapter{Bayesian Statistics}
\label{cap:bayes}

\emph{Bayesian statistics} is a particular approach to applying probability to statistical problems. It provides us with mathematical tools to update our beliefs about random events in light of seeing new data or evidence about those events.

In particular Bayesian inference interprets probability as a measure of confidence that an individual may possess about the occurrence of a particular event.

We may have a \emph{prior belief} about an event, but our beliefs are likely to change when \emph{new evidence} is brought to light. Bayesian statistics gives a solid mathematical means of incorporating our prior beliefs, and evidence, to produce \emph{new posterior beliefs}.

This is in contrast to another form of statistical inference, known as \emph{frequentist statistics}, which assumes that probabilities are the frequency of particular random events occurring in a long run of repeated trials.
For example, as we roll a fair six-sided die repeatedly, we would see that each number on the die tends to come up 1/6 of the time.

\section{Frequentist vs Bayesian}
When carrying out statistical inference, that is, inferring statistical information from probabilistic systems, the two approaches, frequentist and Bayesian, have very different philosophies.

Frequentist statistics tries to eliminate uncertainty by providing estimates. Bayesian statistics tries to preserve and refine uncertainty by adjusting individual beliefs in light of new evidence.

In order to make clear the distinction between the two differing statistical philosophies, consider the probabilistic system of flipping a coin: what is the probability of an unfair coin coming up head ?
\begin{itemize}
	\item \textbf{frequentist}: the probability of seeing a head when the unfair coin is flipped is the long-run relative frequency of seeing a head when repeated flips of the coin are carried out. That is, as we carry out more coin flips the number of heads obtained as a proportion of the total flips tends to the "true" probability of the coin coming up as head. In particular the individual running the experiment does not incorporate her own beliefs about the fairness of the coin.
	\item \textbf{Bayesian}: prior to any flip of the coin an individual may believe that the coin is fair. After a few flips the coin continually comes up head. Thus the prior belief about fairness of the coin is modified to account for the fact that three heads have come up in a row and thus the coin might not be fair. After 500 flips, with 400 heads, the individual believes that the coin is very unlikely to be fair. The posterior belief is heavily modified from the prior belief of a fair coin.
\end{itemize}

Thus in the Bayesian interpretation a probability is a summary of an individual's opinion. A key point is that different (intelligent) individuals can have different opinions (and thus different prior beliefs), since they have differing access to data and ways of interpreting it. However, as both of these individuals come across new data that they both have access to their (potentially differing) prior beliefs will lead to posterior beliefs that will begin converging towards each other under the rational updating procedure of Bayesian inference.

In the Bayesian framework an individual would apply a probability of 0 when there is no confidence in an event occurring, while she would apply a probability of 1 when she is absolutely certain of an event occurring. A probability assigned between 0 and 1 allows weighted confidence in other potential outcomes.

In order to carry out Bayesian inference, we need to utilize a famous theorem in probability known as \emph{Bayes' rule} and interpret it in the correct way.

\section{Conditional Probability}
\label{sec:conditional_prob}

We begin by considering the definition of \emph{conditional probability}, which gives us a rule for determining the probability of an event $A$, given the occurrence of another event $B$. 

To derive the general formula let's start with an example. A fair die is rolled. Let $A$ be the event set that the outcome is an odd number ($A={1,3,5}$). Also let $B$ be the event set that the outcome is less than or equal to $3$ ($B={1,2,3}$). What is the probability of $A$ ($P(A)$) ? And what is the probability of $A$ given $B$, the probability we get $A$ conditioned to having also $B$ and denoted with $P(A|B)$ ?

Being a simple example we can compute the result by hand:

\begin{equation}
P(A) = \cfrac{|A|}{|S|} = \cfrac{|\{1,3,5\}|}{|\{1,2,3,4,5,6\}|} = \cfrac{1}{2}\qquad\textrm{($S$ is the entire sample space)}
\end{equation}

Now let's find the conditional probability of $A$ given that $B$ occurred. If we know $B$ has occurred, the outcome must be among $\{1,2,3\}$. For $A$ to also happen the outcome must be in $A\cap B = \{1,3\}$. Since all die rolls are equally likely, we argue that $P(A|B)$ must be equal to

\begin{equation}
P(A|B) = \cfrac{|A\cap B|}{|B|} = \cfrac{2}{3}
\end{equation}

To find the mathematical definition we can rewrite the calculation by dividing the numerator and denominator by the entire space of the events $|S|$ hence:

\begin{equation}
  P(A|B) = \cfrac{|A\cap B|}{|B|} = \cfrac{\cfrac{|A\cap B|}{|S|}}{\cfrac{|B|}{|S|}} = \cfrac{P(A\cap B)}{P(B)}
  \label{eq:conditional_prob}
\end{equation}

This simply states that the probability of $A$ occurring given that $B$ has occurred is equal to the probability that they have both occurred, relative to the probability that $B$ has occurred. See Figure~\ref{fig:conditional_prob} for a graphical interpretation of conditional probability.

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/conditional_b}
  \caption{Graphical representation of conditional probability (the grey shaded area).}
  \label{fig:conditional_prob}
\end{figure}

\section{Bayes' Rule}

If we multiply both sides of Eq.~\ref{eq:conditional_prob} by $P(B)$ we get:
\begin{equation}
P(B)P(A|B)=P(A\cap B)
\end{equation}

What if we repeat the same reasoning applied to $P(B|A)$ instead, which is akin to asking "what is the probability to roll a die and get a number that is lower or equal than 3 given that is odd."

\begin{equation}
 P(B|A)=\frac{P(B\cap A)}{P(A)}
\end{equation}
Note that $P(A\cap B) = P(B\cap A)$, i.e. having $A$ and $B$ is the same as having $B$ and $A$; so by substituting the above and multiplying by $P(A)$, we get:
\begin{equation}
 P(A)P(B|A)=P(A\cap B)
\end{equation}
We are now able to set the two expressions for $P(A\cap B)$ equal to each other:
\begin{equation}
  P(A)P(B|A)=P(B)P(A|B)
\end{equation}
If we now divide both sides by $P(B)$ we arrive at the celebrated Bayes' rule:
\begin{equation}
  P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}

However, it will be helpful for later usage to modify the denominator, $P(B)$ on the right hand side to be written in terms of $P(B|A)$. From the \emph{law of total probability} we can actually write:
\begin{equation}
P(B)=\sum_{a\in A}P(B\cap a)
\end{equation}
this is possible if the events $A$ are an exhaustive partition of the sample space. So that by substituting the definition of conditional probability we get:
\begin{equation}
P(B)=\sum_{a\in A}P(B\cap a) = \sum_{a\in A}P(B|a)P(a)  
\end{equation}

Finally, we can substitute this into Bayes' rule from above to obtain an alternative version, which is used heavily in Bayesian inference:
\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{\sum_{a\in A}P(B|a)P(a)}
\end{equation}

\section{Applying Bayes' Rule for Bayesian Inference}
The basic idea of Bayesian inference is to continually update our prior beliefs about events as new evidence is presented. This is a very natural way to think about probabilistic events. As more and more evidence is accumulated our prior beliefs are steadily "washed out" by any new data.

In order to demonstrate a concrete numerical example of Bayesian inference it is necessary to introduce some new notation.

Firstly, we need to consider the concept of parameters and models. A \emph{parameter} could be the weighting of an unfair coin, which we could label as $\theta$. Thus $\theta = P(H)$ would describe the probability distribution of our beliefs that the coin will come up as head ($H$) when flipped. The \emph{model} is the actual means of encoding this flip mathematically. In this instance, the coin flip can be modelled as a \emph{Bernoulli trial}. A Bernoulli trial is a random variable with only two outcomes, usually labelled as "success" or "failure", in which the probability of success is exactly the same every time the trial is carried out. This probability is given by $\theta$, and is a number between 0 and 1 $\theta\in[0,1]$.

Over the course of carrying out some coin flip experiments (repeated Bernoulli trials) we will generate some data, $D$, about heads or tails. A natural question to ask is "what is the probability of seeing 3 heads in 8 flips (8 Bernoulli trials), given a fair coin ($\theta=0.5$) ?".

The probability of seeing data $D$ under a particular value of $\theta$ is given by $P(D|\theta)$. However, if you consider it for a moment, we are actually interested in the alternative question, "what is the probability that the coin is fair (or unfair), given that I have seen a particular sequence of heads and tails ?".

Thus we are interested in the \emph{probability distribution} which reflects our belief about different possible values of $\theta$, given that we have observed some data $D$, denoted by $P(\theta|D)$. Notice that this is the converse of $P(D|\theta)$: the Bayes' rule is the link that allows to go between the two.
\begin{equation}
  P(\theta|D) = P(D|\theta)P(\theta)/P(D)
\end{equation}
where:
\begin{itemize}
\item $P(\theta)$ is the \emph{prior}. This is the strength in our belief of $\theta$ without considering the evidence. Our prior view on the probability of how fair the coin is.
\item $P(\theta|D)$ is the \emph{posterior}. This is the (refined) strength of our belief of $\theta$ once the evidence $D$ has been taken into account.
\item $P(D|\theta)$ is the \emph{likelihood}. This is the probability of seeing the data $D$ as generated by a model with parameter $\theta$. If we knew the coin was fair, this tells us the probability of seeing a number of heads in a particular number of flips.
\item $P(D)$ is the \emph{evidence}. This is the probability of the data as determined by summing (or integrating) across all possible values of $\theta$, weighted by how strongly we believe in those particular values of $\theta$. If we had multiple views of what the coin fairness is (but didn't know for sure), then this tells us the probability of seeing a certain sequence of flips for all possibilities of our belief.
\end{itemize}

The entire goal of Bayesian inference is to provide us with a rational and mathematically sound procedure for incorporating our prior beliefs, with any evidence at hand, in order to produce an updated posterior belief. What makes it such a valuable technique is that posterior beliefs can themselves be used as prior beliefs under the generation of new data. Hence Bayesian inference allows us to continually adjust our beliefs under new data by repeatedly applying Bayes' rule.

%Note that in the derivation of the Bayes Theorem we have used the conditional probability definition. Indeed Bayes's formula actually connects two different conditional probabilities $P(A|B)$ and $P(B|A)$, and is essentially a formula for "turning the conditioning around". The Reverend Thomas Bayes referred to this in terms of "inverse probability".

Let's see now a concrete example using the age-old tool of statisticians: the coin-flip.

\subsection{Coin-Flipping Example}

Consider multiple flips of a coin with unknown fairness. We will use Bayesian inference to update our beliefs on the fairness of the coin as more data, i.e. more coin flips, becomes available. The coin will actually be fair, but we won't learn this until the trials are carried out. At the start we have no prior belief on the fairness of the coin, that is, we can say that any level of fairness is equally likely (uniform distribution).

In statistical language we are going to perform $N$ repeated Bernoulli trials with $\theta=0.5$. We are going to use a Bayesian updating procedure to go from our prior beliefs to posterior beliefs as we observe new coin flips. This process can be quite complicated in the general case but for this particular example the concept of \emph{conjugate priors} can make things substantially simpler.

\subsection{Conjugate Priors}
We have just outlined Bayes' rule and have seen that we must specify a likelihood function, a prior belief and the evidence, i.e. a normalizing constant.

We are interested in the probability of the coin coming up head as a function of the underlying fairness parameter $\theta$.
If we denote by $k$ the random variable that describes the result of the coin toss, which is drawn from the set $\{1, 0\}$, where $k=1$ represents a head and $k=0$ represents a tail, then the probability of seeing a head, with a particular fairness of the coin, is given by:
\begin{equation}
 P(k=1|\theta) = \theta 
\end{equation}
And the probability of coming up tail as:
\begin{equation}
P(k=0|\theta) = 1-\theta 
\end{equation}
This can also be written as:
\begin{equation}
P(k|\theta) = \theta^k(1-\theta)^{1-k} 
\end{equation}

This is known as the \emph{Bernoulli distribution}, in essence it tells us the probability of a coin coming up head or tail depending on how fair the coin is.

We can also look at the above function from a different perspective. If we consider a fixed observation, i.e. a known coin flip outcome, $k$, and the fairness parameter $\theta$ as a continuous variable then it tells us the probability of a fixed outcome $k$ given some particular value of $\theta$. As we adjust $\theta$ (e.g. change the fairness of the coin), we will start to see different probabilities for $k$.

This is known as the \emph{likelihood function} of $\theta$. Note that the likelihood function is not actually a probability distribution in the true sense since integrating it across all values of the fairness parameter $\theta$ does not actually equal 1, as is required for a probability distribution.

With the Bernoulli likelihood function we can determine the probability of seeing a particular sequence of flips, given by the set $\{k_1,\ldots,k_N\}$. Since each of these flips is independent of any other, the probability of the sequence occurring is simply the product of the probability of each flip occurring.

If we have a particular fairness parameter $\theta$, then the probability of seeing this particular stream of flips, given $\theta$, is:
\begin{equation}
P(\{k\}|\theta)=\prod_{i}P(k_i|\theta)=\prod_i \theta^{k_i}(1-\theta)^{1-k_i}
\end{equation}

What about if we are interested in the number of heads, say, in $N$ flips? If we denote by $z$ the number of heads appearing, then the formula above becomes:
\begin{equation}
P(z,N|\theta) = \binom{N}{z}\theta^z(1-\theta)^{N-z}
\end{equation}
where $\binom{N}{z}$ is the binomial coefficient which accounts for all the possible arrangements $z$ heads and $N-z$ tails can appear.

\subsubsection{Quantifying our Prior Beliefs}

In this example we are interested in our prior beliefs on the fairness of the coin. That is, we wish to quantify our uncertainty in how biased the coin is. $\theta=0$ indicates a coin that always comes up tail, while $\theta=1$ implies a coin that always comes up head. A fair coin is denoted by $\theta=0.5$.

The question then becomes, which probability distribution do we use to quantify our beliefs about the coin ?

In this instance we are going to choose the \emph{beta distribution}. The probability density function of the beta distribution is given by:
\begin{equation}
P(\theta|\alpha,\beta)=\theta^{\alpha -1}(1-\theta)^{\beta-1}/B(\alpha,\beta)
\end{equation}
where the term in the denominator, $B(\alpha,\beta)$ is present to act as a normalizing constant so that the area under the PDF actually sums to 1.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.7\textwidth]{figures/beta_distro}
  \caption{Different realizations of the beta distribution for various parameters $\alpha$ and $\beta$.}
  \label{fig:beta_distro}
\end{figure}

Essentially, as $\alpha$ becomes larger the bulk of the probability distribution moves towards the right (a coin biased to come up heads more often), whereas an increase in $\beta$ moves the distribution towards the left (a coin biased to come up tails more often).
However, if both parameters increase then the distribution begins to narrow. If they both increase equally, then the distribution will peak over $\theta=05$, i.e. the coin is fiar. Note also that the \emph{uniform distribution} is a particular case of a beta distribution, i.e. $\alpha = \beta = 1$.

Among various interesting properties we have chosen the beta function as our prior because it is a conjugate prior for the Bernoulli distribution.

\subsubsection{Conjugate Prior}
In Bayes' rule we can see that the posterior distribution is proportional to the product of the prior distribution and the likelihood function:
\begin{equation}
P(\theta|D)\propto P(D|\theta)P(\theta)
\end{equation}

A \emph{conjugate prior} is a choice of prior distribution, that when coupled with a specific type of likelihood function, provides a posterior distribution that is of the \emph{same family} as the prior distribution, i.e. the prior and posterior both have the same probability distribution, but with differing parameters.

Conjugate priors are extremely convenient from a calculation point of view as they provide closed-form expressions for the posterior, thus negating any complex numerical integration.

In our case, if we use a Bernoulli likelihood function and a beta distribution as prior, we immediately know that the posterior will also be a beta distribution.
As mentioned above, the probability density function of a beta distribution, for our particular parameter $\theta$, is given by:
\begin{equation}
P(\theta|\alpha,\beta)=\theta^{\alpha -1}(1-\theta)^{\beta-1}/B(\alpha,\beta)
\end{equation}

You can see that the form of the beta distribution is similar to the Bernoulli likelihood. In fact, if we multiply the two together, we get:
\begin{equation}
\theta^{\alpha -1}(1-\theta)^{\beta-1}/B(\alpha,\beta)\times \theta^k(1-\theta)^{1-k} \propto \theta^{\alpha+k-1}(1-\theta)^{\beta+k}
\end{equation}

The last issue we need to address is how to connect $\alpha$ and $\beta$ parameters to more intuitive parameters like mean and variance. It turns out that we can use the following relationships:
\begin{equation}
  \begin{gathered}
    \mu = \frac{\alpha}{\alpha+\beta} \\
    \sigma = \sqrt{\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}}
  \end{gathered}
\end{equation}

Re-arrange these formul\ae 
\begin{equation}
  \begin{gathered}
    \alpha = \left(\frac{1-\mu}{\sigma^2} - \frac{1}{\mu} \right) \mu^2\\
    \beta = \alpha\left(\frac{1}{\mu}-1 \right)
  \end{gathered}
\end{equation}

So suppose I think the fairness of the coin is around 0.5, but I'm not particularly certain, I may specify a standard deviation of about 0.1. Plugging in the numbers into the above formulae gives us $\alpha = 12$ and $\beta = 12$ and the beta distribution in this instance looks like the following:
\begin{figure}[htb]
  \centering
  \includegraphics[width=.7\textwidth]{figures/beta_12_12}
  \caption{A beta distribution with $\alpha=12$ and $\beta=12$.}
  \label{fig:beta_12_12}
\end{figure}

Notice how the peak is centred around 0.5 but that there is significant uncertainty in this belief, represented by the curve width.

\subsection{Using Bayes' Rule to Calculate a Posterior}

We are now finally in a position to be able to calculate our posterior beliefs using Bayes' rule for repeated coin-flips
\begin{equation}
P(\theta|z,N)=P(z,N|\theta)P(\theta)/P(z,N)
\end{equation}

If we substitute in the values for the likelihood function calculated above, as well as our prior belief beta distribution, we get:
\begin{equation}
P(\theta|z,N)\propto P(z,N|\theta)P(\theta)\propto \theta^{z+\alpha - 1}(1-\theta)^{N-z+\beta -1} 
\end{equation}
so the posterior is given by $\textrm{beta}(\theta|z+\alpha,N-z+\beta)$ .
All we need to do is specify the mean and standard deviation of our prior beliefs, carry out $N$ flips and observe the number of heads $z$ and we automatically have a rule for how our beliefs should be updated.

In the top-left plot of Fig.~\ref{fig:bayes} we have carried out no trials and our probability density function (in this case our original prior density) is the uniform distribution.
Originally we were not able to assume anything about the coin fairness, so the result states that we have equal belief in all values of $\theta$.

The next plot shows 2 trials carried out and they both come up heads. Our Bayesian procedure allows us to update to a posterior density. Notice how the density weight is now shifted to the right hand side of the chart. This indicates that our prior belief of equal likelihood of fairness of the coin, coupled with 2 new data points, leads us to believe that the coin is more likely to be unfair (biased towards heads than it is tails).

The following two plots show 10 and 20 trials respectively. Since we have seen 6 tails in 10 trials we are now more convinced that the coin is likely to be unfair and biased towards tails. After 20 trials, we have seen a few more heads appear. The density of the probability has now shifted closer to $\theta = P(H) = 0.5$. Hence we are now starting to believe that the coin is possibly fair.

After 50 and 500 trials respectively, we begin to believe that the coin fairness is very likely to be around $\theta=0.5$. This is indicated by the shrinking width of the probability density, which is now clustered tightly around $\theta=0.5$ in the final panel. Were we to carry out another 500 trials (since the coin is actually fair) we would see this probability density become even tighter and centred closer to $\theta=0.5$.

Thus it can be seen that Bayesian inference gives us a rational procedure to go from an uncertain situation with limited information to a more certain situation with significant amounts of data.

\begin{finmarkets}
Below code implements the coin fairness example described in the text.
\end{finmarkets}

\pythoncodenon{code/bayes_coin_fairness.py}

\begin{figure}[htb]
  \centering
  \includegraphics[width=.7\textwidth]{figures/bayes}
  \caption{Bayesian update procedure using the Beta-Binomial model.}
  \label{fig:bayes}
\end{figure}

%\section*{Exercises}
%\input{monte_carlo_ex_text}









