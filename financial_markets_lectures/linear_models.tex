\chapter{Linear Models}
\label{linear-models}

\section{Simple Linear Regression Analysis}
\label{sec:linear-regression}
Regression Analysis is a form of predictive analysis. It can be used to find the relation of a company’s performance to the industry performance or competitor business.

The single (or simple) linear regression model expresses the relationship between the dependent variable (target) and one independent variable. Regression attempts to find the strength of that relationship.

We use it to analyze the statistical relationship between sets of variables. Regression models usually show a regression equation representing the dependent variable as a function of the independent variable.
A simple linear model can be described by the following equation:
\begin{equation}
y = \beta x + \alpha + \epsilon
\label{eq:linear_regression}
\end{equation}
where $y$ is the dependent variable (the target), $\alpha$ the intercept, $\beta$ the slope, $x$ the independent variable (the predictor), and $\epsilon$ the residual (error).

The purpose is to estimate the underlying relationship so that we can predict the target variable based on the other (predictor).

%We can plot the function on a graph, where a is the intercept and b is the slope. It shows us the measure of the change in the target variable due to changes in other variables. We can use it when we attempt to identify the variables that affect a certain measure, like a stock price.

\subsection{Ordinary Least Squares}
We need to solve a problem when running the regression model, and this is to fit a straight line to a set of pairs of observations of the dependent and independent variables. The line of best fit is where the sum of the squares of the vertical deviations (distances) between observation points and the line is at its minimum. This is the method of ordinary least squares (OLS) and the one we most commonly apply to a linear regression model.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/linear_regression}
	\caption{Example of linear regression.}
	\label{fig:linear_regression_example}
\end{figure}

The regression equation gives no exact prediction of the target value for any predictor variable. The regression coefficients we calculate from our sample data observations are only the best estimate of the real population variables.
This is why we introduce $\epsilon$ (residual/error) to the model, it covers the element of chance that an independent variable can experience variations.

\subsubsection{Covariance}
One of the measures we get from a regression analysis is the covariance. It calculates the relationship between two variables.

\begin{equation}
\textrm{cov}(x, y) = \mathbb{E}[(x - \mathbb{E}[x])(y - \mathbb{E}[y])]
\end{equation}
where: $x$ and $y$ are the values of the independent and dependent variables at each observation, and $\mathbb{E}$ represents the mean.

The formula shows the direction of the relationship. If one variable is going up when the other is going down, then the covariance will be negative, and vice versa.

\subsubsection{Correlation}
That’s where correlation, another measure of regression analysis, comes in. It helps us to standardize the covariance to be able to better understand and use it in forecasting.

Correlation takes the covariance and divides it over the product of the standard deviations of the variables. This makes sure we get a correlation coefficient between -1 and +1.
\begin{equation}
\textrm{cor}(x, y) = \frac{\textrm{cov}(x, y)}{\textrm{std}(x)\textrm{std}(y)}
\end{equation}

A correlation of +1 suggests the two variables are perfectly positively correlated, and a value of -1 suggests an entirely negative correlation.

From the definition of the $\beta$ coefficient (i.e. the slope of the linear regression) through least squares we have
\begin{equation}
\hat{\beta} = \textrm{cor}(x, y)\cdot \frac{\textrm{std}(y)}{\textrm{std}(x)}
\end{equation}
therefore the two only coincide when $\textrm{std}(y) = \textrm{std}(x)$ That is, they only coincide when the two variables are, in some sense, on the same scale. 

Correlation and slope at some extent give the same information, i.e. they each tell the linear relationship strength between $x$ and $y$. But, they also do each give distinct information:
\begin{itemize}
\item the correlation gives you a bounded measurement that can be interpreted independently of the two variables scale. The closer the estimated correlation is to $\pm1$, the closer the two are to a perfect linear relationship. The regression slope, in isolation, does not tell you that piece of information.
\item the regression slope gives a useful quantity interpreted as the estimated change in the expected value of $y$ for a given value of $x$. Specifically, $\hat{\beta}$ tells you the change in the expected value of $y$ corresponding to a 1-unit increase in $x$. This information can not be deduced from the correlation coefficient alone.
\end{itemize}

%Model Assumptions
%Whenever we are setting up a regression model, we need to work with some inherent assumptions. The most notable amongst those are:
%
%The variables exhibit a linear relationship between the slope and intercept;
%The independent variable (predictor) is not random;
%The values of the residuals (errors) follow a standard normal distribution.

\subsection{Multiple Linear Regression}
Simple regression is usually not enough in a real-life scenario, as targets (dependent variables) are rarely impacted only by a single predictor.
The multiple linear regression model is almost the same as the simple one; the only difference being it can have two or more independent variables (predictors).
The function to represent the regression equation is:
\begin{equation}
y = \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \alpha + \epsilon
\label{eq:multiple_regression}
\end{equation}

It is crucial to keep in mind that the multiple regression model requires \emph{non-collinearity}. This means the independent variables should have a minimal correlation between them. Otherwise, it is difficult to assess the real relationship between the dependent (target) and the independent (predictors) variables.

%\subsection{Linear Regression Example}
%
%The Chinese Yuan (CNY) was pegged to the US Dollar (USD) prior to July 2005. Then, China announced that the exchange rate would be set with reference to a basket of other currencies, allowing for a movement of up to 0.3\% within any given day. The actual currencies and their basket weights are unannounced by China. Figure~\ref{fig:yuan_rate} reports the Yuan exchange rate to USD between 1999 and 2013.
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=0.7\textwidth]{figures/yuan_exchange_rate}
%\caption{Yuan exchange rate to USD from January 1999 to December 2013.}
%\label{fig:yuan_rate}
%\end{figure}
%
%From an empirical point of view, there are several important questions:
%\begin{itemize}
%\item for any given period, what is the implicit reference basket for the Chinese currency ?
%\item has the reference basket changed over time ?
%\item has the Chinese currency depreciated with respect to the dollar? If so, how much and when?
%\end{itemize}
%
%A possible approach for evaluating the implicit exchange rate regime of the Yuan involves the regression of the changes in the target currency on changes in the values of possible currencies in the reference basket.
%
%Consider the dataset \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/exchange_rates.csv}{exchange\_rates.csv} containing daily currencies exchange rates in the 1999 to 2013 period (data has been fetched from the Federal Reserve Archive~\cite{bib:fred}).
%
%To apply this methodology the original dollar-based exchange rates have been converted using the Swiss Franc. This allows currency moves of the dollar to be be used to explain moves in the Yuan. The choice of Swiss Franc is consistent with evaluations with respect to a stable currency. The dataframe already has columns with daily variations.
%
%\begin{ipython}
%import pandas as pd
%
%data = pd.read_csv("exchange_rates.csv", index_col="Date")
%print (data.head())
%\end{ipython}
%\begin{ioutput}
%            DEXCHUS  DEXJPUS  DEXKOUS  DEXMAUS  DEXUSEU  DEXUSUK  DEXTHUS  \
%Date                                                                        
%1999-01-05   8.2795   111.15   1166.0      3.8   1.1760   1.6566    36.18   
%1999-01-06   8.2795   112.78   1160.0      3.8   1.1636   1.6547    36.50   
%1999-01-07   8.2798   111.69   1151.0      3.8   1.1672   1.6495    36.30   
%1999-01-08   8.2796   111.52   1174.0      3.8   1.1554   1.6405    36.45   
%...
%                ret_THB_SFR      ret_USD_SFR  
%Date                                          
%1999-01-05        -0.002599        -0.002047  
%1999-01-06        -0.002666        -0.011472  
%1999-01-07        -0.006288        -0.000794  
%1999-01-08        -0.003565        -0.007689  
%
%[5 rows x 24 columns]
%\end{ioutput}
%
%%Figure~\ref{fig:rate_variation} shows CNY and USD daily variation with respect to Swiss Franc (SFR).
%%\begin{figure}[htbp]
%%\centering
%%\includegraphics[width=0.7\textwidth]{figures/log_variation_exch}
%%\caption{Daily variation of CNY and USD exchange rates from January 1999 to December 2013 with respect to Swiss Franc.}
%%\label{fig:rate_variation}
%%\end{figure}
%
%To implement the linear regression model it can be used the \texttt{statsmodel} package. It is necessary to define the $X$ vector (i.e. the independent exchange rate variations) and $y$ (i.e. the target Yuan rate to SFR). Then add a constant to the model which represents the rate variation not correlated to other currency variations (i.e. $\alpha$).
%
%First, we fit the regression model for the period prior to July 2005 when the Chinese currency was pegged to the US dollar. 
%
%\begin{ipython}
%import statsmodels.api as sm
%
%X = df.loc[df.index < '2005-06-30' ,
%           ['ret_YEN_SFR', 'ret_EUR_SFR', 
%            'ret_GBP_SFR', 'ret_USD_SFR']]
%y = df.loc[df.index < '2005-06-30' ,'ret_CNY_SFR']
%X = sm.add_constant(X)
%
%est = sm.OLS(y, X).fit()
%print(est.summary())
%\end{ipython} 
%\begin{ioutput}
%                            OLS Regression Results                            
%==============================================================================
%Dep. Variable:            ret_CNY_SFR   R-squared:                       1.000
%Model:                            OLS   Adj. R-squared:                  1.000
%Method:                 Least Squares   F-statistic:                 3.647e+06
%Date:                Tue, 08 Nov 2022   Prob (F-statistic):               0.00
%Time:                        13:46:53   Log-Likelihood:                 13749.
%No. Observations:                1692   AIC:                        -2.749e+04
%Df Residuals:                    1687   BIC:                        -2.746e+04
%Df Model:                           4                                         
%Covariance Type:            nonrobust                                         
%===============================================================================
%                  coef    std err          t      P>|t|      [0.025      0.975]
%-------------------------------------------------------------------------------
%const       -1.921e-07   1.74e-06     -0.110      0.912   -3.61e-06    3.23e-06
%ret_YEN_SFR    -0.0001      0.000     -0.477      0.633      -0.001       0.000
%ret_EUR_SFR    -0.0003      0.001     -0.379      0.705      -0.002       0.001
%ret_GBP_SFR    -0.0001      0.000     -0.224      0.823      -0.001       0.001
%ret_USD_SFR     1.0002      0.000   2545.320      0.000       0.999       1.001
%==============================================================================
%Omnibus:                      758.491   Durbin-Watson:                   2.824
%Prob(Omnibus):                  0.000   Jarque-Bera (JB):           889538.472
%Skew:                           0.456   Prob(JB):                         0.00
%Kurtosis:                     115.324   Cond. No.                         486.
%==============================================================================
%\end{ioutput} 
%
%The most interesting (for us) part of the summary is: 
%\begin{itemize}
%\item R-squared: the closer to 1 the higher is the linear correlation between $y$ and $X$;
%\item coef: the estimate of this model parameter (the weight assigned to this feature);
%\item std err: the standard error of our estimate. From these two values we can compute the t-score of our estimate:
%\begin{equation*}
%t = \frac{\textrm{coeff}}{\textrm{std err}}
%\end{equation*}
%which we can find on the third column ($t$); 
%\item t: this value essentially provides us with a metric of how small the error is with respect to the estimated value: the larger the t-score, the smaller the error and the more confident we can be in our estimate.
%\item P>|t|: to better quantify our confidence, it’s usual to compute the p-value associated to the t-score. Under relatively common assumptions, we expect the t-score to follow a standard distribution and the probability of obtaining results at least as extreme as the value observed is simply the area under the curve to the right of the t-score value. This area is known as the p-value.
%There are some nuances to interpreting p-values but briefly, the smaller the p-value, the stronger the evidence that the value we’re estimating is different than zero (if the coefficient of a given feature is indistinguishable from zero then that feature is not relevant for our model).
%
%Typical thresholds for the p-value are:
%\begin{itemize}
%	\item $p<0.05$: moderate evidence;
%	\item $p<0.01$: strong evidence;
%	\item $p<0.001$: very strong evidence.
%\end{itemize}
%\end{itemize}
%
%In our example R-squared is 1, and the only largely significant predictor is USD (p-value 0) confirming that indeed CNY was anchored to US dollar prior July 2005.
%
%Second, we fit the regression model for the first six months following the announcement of the change in currency policy.
%
%\begin{ipython}
%X = df.loc[(df.index > '2005-07-01') & (df.index < '2005-12-31'),
%           ['ret_YEN_SFR', 'ret_EUR_SFR', 
%            'ret_GBP_SFR', 'ret_USD_SFR',
%            'ret_WON_SFR', 'ret_MYR_SFR', 
%            'ret_THB_SFR']]
%y = df.loc[(df.index > '2005-07-01') & (df.index < '2005-12-31'),
%           'ret_CNY_SFR']
%
%X = sm.add_constant(X)
%est = sm.OLS(y, X).fit()
%print(est.summary())
%\end{ipython}
%\begin{ioutput}
%                            OLS Regression Results                            
%==============================================================================
%Dep. Variable:            ret_CNY_SFR   R-squared:                       0.949
%Model:                            OLS   Adj. R-squared:                  0.946
%Method:                 Least Squares   F-statistic:                     326.3
%Date:                Tue, 08 Nov 2022   Prob (F-statistic):           8.41e-76
%Time:                        13:53:28   Log-Likelihood:                 667.58
%No. Observations:                 130   AIC:                            -1319.
%Df Residuals:                     122   BIC:                            -1296.
%Df Model:                           7                                         
%Covariance Type:            nonrobust                                         
%===============================================================================
%                  coef    std err          t      P>|t|      [0.025      0.975]
%-------------------------------------------------------------------------------
%const          -0.0001      0.000     -0.920      0.359      -0.000       0.000
%ret_YEN_SFR    -0.0097      0.037     -0.262      0.794      -0.083       0.063
%ret_EUR_SFR     0.0583      0.092      0.636      0.526      -0.123       0.240
%ret_GBP_SFR    -0.0306      0.044     -0.688      0.493      -0.119       0.057
%ret_USD_SFR     0.2122      0.148      1.433      0.154      -0.081       0.505
%ret_WON_SFR     0.1790      0.036      5.041      0.000       0.109       0.249
%ret_MYR_SFR     0.7373      0.142      5.178      0.000       0.455       1.019
%ret_THB_SFR    -0.0655      0.059     -1.110      0.269      -0.182       0.051
%==============================================================================
%Omnibus:                      195.464   Durbin-Watson:                   2.331
%Prob(Omnibus):                  0.000   Jarque-Bera (JB):            15756.757
%Skew:                          -5.874   Prob(JB):                         0.00
%Kurtosis:                      55.639   Cond. No.                     1.57e+03
%==============================================================================
%\end{ioutput}
%
%R-squared is quite close to 1 so there is some correlation between $y$ and $X$.
%During this six-month period, there is evidence of the Yuan departing from a US Dollar peg. The exchange rates with the statistically significant regression parameters (lowest p-values) are for the Korean Won (WON) and the Malaysian Ringgit (MYR).
%
%Similar regressions can be performed to examine for further changes in the implicit reference basket from 2006 through 2013.
%
%Finally it is possible to measure the annualized trend in the Yuan exchange rate relative to the other currencies in the studied period. The annualization is performed on the $\alpha$ coefficient which is the only one not related to other currency variations (i.e. the idiosyncratic part of the rate variation).
%
%\begin{ipython}
%print (f"{(1+est.params[0])**126-1:.4f}")
%\end{ipython}
%\begin{ioutput}
%-0.0150
%\end{ioutput}
%\noindent
%So the Yuan depreciated in the studied period.

\section{Capital Asset Pricing Model}
\label{sec:capm}
The Capital Asset Pricing Model (CAPM) describes the relationship between asset expected returns and \emph{systematic risk} of the market. No measure of unsystematic risk appears in the risk premium for in the world of CAPM diversification has already eliminated it.

Sharpe~\cite{bib:capm_sharpe} and Lintner~\cite{bib:capm_lintner} developed the Capital Asset Pricing Model whose central insight is that the riskiness of an asset is not measured by the standard deviation of its return but by its beta. In particular, there is a linear relationship between the expected return of any security (or portfolio) and the expected return of the market portfolio. It is given by

\begin{equation}
r_i = r_f + \beta_i(r_m-r_f)
\label{eq:capm}
\end{equation}
where:
\begin{itemize}
\item $r_i$ is the expected return of the $i^{th}$ security;
\item $r_f$ is the risk-free rate with zero standard deviation (e.g. risk-free asset includes Treasury Bills as they are backed by the U.S. government);
\item $r_m - r_f$ is the risk premium, $r_m$ denotes the market return including all securities in the market, whose proxy can be an index like SP500;
\item $\beta_i$ is a measure of $i^{th}$ asset volatility in relation to the overall market. $\beta$ is used in the CAPM to describe the relationship between market risk, and expected return.
\end{itemize}

The relationship between risk ($\beta$) and expected return is called \emph{Security Market Line} (SML). An example of this line is shown in Fig.~\ref{fig:sml}.

In the freely competitive financial markets described by CAPM, no security can sell for long at prices low enough to yield more than its appropriate return on the SML (\emph{undervalued} asset). The security would then be very attractive compared with other securities of similar risk, and investors would bid its price up until its expected return fell to the appropriate position on the SML. Conversely, investors would sell off any stock selling at a price high enough to put its expected return below its appropriate position (\emph{overvalued} asset). The resulting reduction in price would continue until the stock’s expected return rose to the level justified by its systematic risk.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/sml}
	\caption{Security market line.}
	\label{fig:sml}
\end{figure}

The key point in CAPM is the determination of $\beta$. This can be achieved with the measurement of the \emph{regression line} slope, in the market vs individual stock return plot.

\subsection{Regression in CAPM}

The regressed coefficient estimates can be expressed as 

\begin{equation}
\beta \approx \cfrac{\textrm{cov}(X,y)}{\textrm {var}(X)}
\end{equation}

In the case of CAPM the line estimates the stock returns $y$ given the global market returns $X$ and so provides insights about how \emph{volatile}, or how risky, a stock is relative to the rest of the market.

In CAPM $\beta$ calculation is used to help investors understand whether a stock moves in the same direction as the rest of the market but for it to provide any useful clue, the market proxy should be related to the stock.

If $\beta$ of an individual stock = 1.0, means its price is perfectly correlated with the market, if $\beta < 1.0$, which is referred to as "defensive", indicates the security is theoretically less volatile than the market (provides lower returns, so it is less risky), while if $\beta > 1.0$, or "aggressive", indicates the assets price is more volatile than the market.

Those who use CAPM pick individual stocks or portfolios, and compare them to different indexes. The point is to find stocks that have high $\beta$, and portfolios that have high $\alpha$. High $\beta$ means the stock fares better than index with positive market and performs worse for negative market (contrary low $\beta$ gives lower performance for positive market and "better" returns in negative market), so those stocks have a chance at beating the market. $\alpha$ values above zero mean that a portfolio outperforms the market whatever it does.

\subsection{CAPM Example}
\label{sec:linear_regression}
Let's apply CAPM model to a couple of securities, the market proxy is the SP500 index while the risk free rate is approximated by the 3 months Treasury rate.
Input data can be downloaded with \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/capm.csv}{capm.csv}.

Consider the General Electrics (GE) stock and apply CAPM to the series of its returns. Figure~\ref{fig:ge_returns} reports the historical series of closing price for GE and SP500.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_ge}
\caption{Historical series of closing price for General Electrics stock (left) and SP500 (right).}
\label{fig:ge_returns}
\end{figure}

To implement the linear regression model it can be used the \texttt{statsmodel} package. It is necessary to define the $X$ vector (i.e. the independent exchange rate variations) and $y$ (i.e. the target Yuan rate to SFR). Then add a constant to the model which represents the rate variation not correlated to other currency variations (i.e. $\alpha$).

So to determine $\beta$ and $\alpha$ for GE stock the model is made of its excess of returns ($y$) and market excess of returns ($X)$ 

\begin{ipython}
X = capm['ret_SP500']
y = capm['ret_GE']
	
X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython}
\begin{ioutput}
OLS Regression Results                                
===========================================================================
Dep. Variable:                 ret_GE   R-squared :                   0.572
Model:                            OLS   Adj. R-squared :              0.572
Method:                 Least Squares   F-statistic:                  4469.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):            0.00
Time:                        08:51:58   Log-Likelihood:              9655.8
No. Observations:                3340   AIC:                     -1.931e+04
Df Residuals:                    3339   BIC:                     -1.930e+04
Df Model:                           1                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ret_SP500      1.1867      0.018     66.854      0.000       1.152       1.222
==============================================================================
Omnibus:                      900.145   Durbin-Watson:                   1.973
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            66833.895
Skew:                           0.275   Prob(JB):                         0.00
Kurtosis:                      24.908   Cond. No.                         1.00
==============================================================================
\end{ioutput}

The most interesting (for us) part of the summary is: 
\begin{itemize}
\item R-squared: the closer to 1 the higher is the linear correlation between $y$ and $X$;
\item coef: the estimate of this model parameter (the weight assigned to this feature);
\item std err: the standard error of our estimate. From these two values we can compute the t-score of our estimate:
\begin{equation*}
	t = \frac{\textrm{coeff}}{\textrm{std err}}
\end{equation*}
which we can find on the third column ($t$); 
\item t: this value essentially provides us with a metric of how small the error is with respect to the estimated value: the larger the t-score, the smaller the error and the more confident we can be in our estimate.
\item P>|t|: to better quantify our confidence, it’s usual to compute the p-value associated to the t-score. Under relatively common assumptions, we expect the t-score to follow a standard distribution and the probability of obtaining results at least as extreme as the value observed is simply the area under the curve to the right of the t-score value. This area is known as the p-value.
There are some nuances to interpreting p-values but briefly, the smaller the p-value, the stronger the evidence that the value we’re estimating is different than zero (if the coefficient of a given feature is indistinguishable from zero then that feature is not relevant for our model).
	
Typical thresholds for the p-value are:
\begin{itemize}
	\item $p<0.05$: moderate evidence;
	\item $p<0.01$: strong evidence;
	\item $p<0.001$: very strong evidence.
\end{itemize}
\end{itemize}

From the summary results that $\beta$ is statistically significant and equal to 1.187. Fig.~\ref{fig:capm_fit} reports the dataset and the linear regression fit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_fit}
\caption{Linear regression of market return vs stock return for GE security.}
\label{fig:capm_fit}
\end{figure}

If you imagine to have a portfolio, and that we have the $\beta$s of each individual stock to apply CAPM it is enough to perform a weighted sum of the expected return according to the model of each stock.

\subsection{Criticism to CAPM}
As we have seen the whole model is about plotting a line in a scatter plot, it’s not a very complex model. Assumptions under the model are even more simplistic. For example:
\begin{itemize}
\tightlist
\item expect that all investors are rational and they avoid risk;
\item everyone have full information about the market;
\item everyone have similar investment horizons and expectations about future movements;
\item stocks are all correctly priced.
\end{itemize}

Moreover, this is a model from the 1960s. Market dynamics were different back then. And of course, this is a retrospective model. We cannot know how future stock prices move and how the market behaves.
Interesting extension of CAPM involves \emph{Bayesian regression}\cite{bib:bayesian_regression} but it will not be discussed here.

\section{Multifactor Models}

In its original formulation the Capital Asset Pricing Model (CAPM) treats the market return as the only factor. Nevertheless a stock’s return can depend also on other macro-economic factors, such commodity prices, interest rates, economic growth (GDP). In this case we talk about \emph{multifactor models} which can be used for: return forecasting, risk modeling, transaction cost analysis, and performance attribution. 
By decomposing asset returns into different components (\emph{factors}), allows a better understanding of the sources of portfolio return and risk. 

Factor models are based on the idea that security returns consist of two components:
\begin{itemize}
\tightlist
\item the part that is driven by a set of common factors;
\item and the part unexplained by these factors hence idiosyncratic in nature. 
\end{itemize}
\begin{equation}
r_{t} = \sum_{k=1}^{K} X_{kt} f_{kt} + \epsilon_{t} = \mathbf{Xf} + \boldsymbol{\epsilon}
\label{eq:multifactor}
\end{equation}
where: $r_{t}$ is the security return over period $t$, $X_{kt}$ its \emph{exposure} to k$^{th}$ factor at the beginning of period $t$, $f_{kt}$ is the \emph{factor return} to k$^{th}$ factor over period $t$ and $\epsilon_{t}$ is the residual return over period $t$ (i.e. there are $K$ factors, and $T$ time periods).

\subsection{Types of Factor Models}

In practice, there are three common classes of factor models depending on the statistical techniques used to estimate factor returns and/or factor exposures.

\begin{center}
\scriptsize
\begin{tabularx}{\linewidth}{>{\hsize=.3\hsize}X>{\hsize=.4\hsize}X>{\hsize=.4\hsize}X>{\hsize=.5\hsize}X>{\arraybackslash}X}
Type & Inputs & Outputs & Estimation Method & \textcolor{ansi-green}{Adv.}/\textcolor{ansi-red}{Disadv.} \\
\toprule
Explicit & security returns, factor returns & factor exposures &\begin{tabular}[t]{@{}l@{}}time-series\\ regression\end{tabular}
& \textcolor{ansi-green}{Accommodates any time series as factors, linked to macroeconomic fundamentals.}
\textcolor{ansi-red}{Possible non-intuitive factor exposures.} \\
\midrule
Implicit & security returns, factor exposures & factor returns &\begin{tabular}[t]{@{}l@{}}cross-sectional\\ regression\end{tabular} & \textcolor{ansi-green}{Intuitive exposures linked to asset characteristics, incorporates security fundamental data, responsive to asset characteristics changes.} \textcolor{ansi-red}{Data intensive.} \\
\midrule
Statistical & security returns, factor exposures & factor returns &\begin{tabular}[t]{@{}l@{}}principal component\\analysis\end{tabular} & \textcolor{ansi-green}{Requires only asset-level price data, might capture new risk sources, relatively easy to build.} \textcolor{ansi-red}{Lack of interpretability.} \\
\end{tabularx}
\end{center}
\normalsize

\subsection{Risk Decomposition and Estimation with Factor Models}

Once factor exposures are calculated, one can carry out risk decomposition and forecast at both the individual security and the aggregate portfolio levels.

Let $R_P$ denote the return of some portfolio $P$ over period $t$
\begin{equation}
R_P = \mathbf{w}_P^T \mathbf{r}
\label{eq:portfolio_return}
\end{equation}
where $\mathbf{w}_P$ are the portfolio weights and $\mathbf{r}$ denotes the vector of security returns.

Substituting~\ref{eq:multifactor} into Eq.~\ref{eq:portfolio_return} gives
\begin{equation}
R_P = \mathbf{w}^T_P \mathbf{Xf} + \mathbf{w}^T_P\boldsymbol{\epsilon}
\end{equation}
where $\mathbf{X}$ is a $N\times K$ matrix specifying each security’s exposures to each model factors ($\mathbf{f}$). 

The risk $\sigma_P$ of the portfolio can be decomposed into the factor and the idiosyncratic components: 
\begin{equation}
\sigma^2_P = \mathbf{w}_P^T\mathbf{\Omega}\mathbf{w}_P = \mathbf{w}_P^T \mathbf{XFX}^T \mathbf{w}_P + \mathbf{w}_P^T \mathbf{\Delta} \mathbf{w}_P
\end{equation}

$\mathbf{\Omega}$ represents the $N \times N$ asset covariance matrix that without a factor model the calculation of $\mathbf{\Omega}$ would involve computing the pairwise sample covariance for each element in the matrix (e.g. $N$ is usually very large, hundreds of assets, relative to the number of time periods $T$ of the return time series, which makes the estimated covariance matrix unstable and ill-conditioned).

The factor model framework allows to reduce the dimensionality of the problem significantly. In this approach, we can express the asset covariance matrix in terms of factor model ingredients:
\begin{equation}
\mathbf{\Omega} = \mathbf{XFX}^T + \mathbf{\Delta}
\end{equation}
where $\mathbf{X}$ is the factor exposure matrix, $\mathbf{F}$ is the $K\times K$ factor covariance matrix, and $\mathbf{\Delta}$ is the $N\times N$ idiosyncratic covariance matrix (which is usually assumed to be diagonal). 

\subsubsection{Example}

Consider the example of previous Section and improve the model by adding the crude oil price as a second factor. Perform again the linear regression

\begin{ipython}
X = capm[['ret_SP500', 'ret_Brent']]
y = capm['ret_GE']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython} 
\begin{ioutput}
OLS Regression Results                                
==============================================================================
Dep. Variable:                 ret_GE   R-squared (uncentered):          0.574
Model:                            OLS   Adj. R-squared (uncentered):     0.574
Method:                 Least Squares   F-statistic:                     2251.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):               0.00
Time:                        09:05:11   Log-Likelihood:                 9662.9
No. Observations:                3340   AIC:                        -1.932e+04
Df Residuals:                    3338   BIC:                        -1.931e+04
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ret_SP500      1.2004      0.018     66.376      0.000       1.165       1.236
ret_Brent     -0.0371      0.010     -3.768      0.000      -0.056      -0.018
==============================================================================
Omnibus:                      878.756   Durbin-Watson:                   1.974
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63119.110
Skew:                           0.231   Prob(JB):                         0.00
Kurtosis:                      24.292   Cond. No.                         1.90
==============================================================================
\end{ioutput}

The regression coefficient for the oil factor (ret\_Brent) is statistically significant and negative. Over the analysis period, price changes in GE stock are negatively related to the price changes in oil.
Let's apply the same model now to Exxon (XOM) stock.

\begin{ipython}
X = capm[['ret_SP500', 'ret_Brent']]
y = capm['ret_XOM']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython} 
\begin{ioutput}
OLS Regression Results                                
==============================================================================
Dep. Variable:                ret_XOM   R-squared (uncentered):          0.549
Model:                            OLS   Adj. R-squared (uncentered):     0.549
Method:                 Least Squares   F-statistic:                     2031.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):               0.00
Time:                        09:07:25   Log-Likelihood:                 10402.
No. Observations:                3340   AIC:                        -2.080e+04
Df Residuals:                    3338   BIC:                        -2.079e+04
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ret_SP500      0.8129      0.014     56.082      0.000       0.785       0.841
ret_Brent      0.1453      0.008     18.391      0.000       0.130       0.161
==============================================================================
Omnibus:                      477.069   Durbin-Watson:                   2.075
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6852.309
Skew:                           0.036   Prob(JB):                         0.00
Kurtosis:                      10.017   Cond. No.                         1.90
==============================================================================
\end{ioutput}

The R-squared for XOM is slightly lower than for GE. Its relationship to the market index is less strong (lower t value).
The regression coefficient for the oil factor (ret\_Brent) is statistically significant and, unlike GE, positive.

\subsection{Fama-French Three-Factor Model}
This model was proposed in 1993 by Eugene Fama and Kenneth French to describe stock returns~\cite{bib:fama_french}. The three-factor model is
\begin{equation}
r=\alpha + \beta_m \textrm{MKT} +\beta_s \textrm{SMB}+ \beta_h \textrm{HML}
\end{equation}
where
\begin{itemize}
\tightlist
\item MKT: is the excess return of the market (e.g. the value-weighted return of all firms listed on the NYSE, AMEX, or NASDAQ minus the 1-month Treasury Bill rate);
\item SMB (Small Minus Big): measures the excess return of stocks with small market cap over those with larger market cap;
\item HML (High Minus Low): measures the excess return of value stocks over growth stocks. Value stocks have high book to price ratio (B/P) than growth stocks.
\end{itemize}

This setting represents the original Fama-French model. This very same approach has been also extended with the Fama-French Five-Factor model which comprises two more factors:
\begin{itemize}
\tightlist
\item RMW (Robust Minus Weak): measures the excess returns of firms with high operating profit margins over those with lower profits;
\item CMA (Conservative Minus Aggressive): measures the excess returns of firms investing less over those investing more.
\end{itemize}
Finally, momentum is another commonly used factor. It captures excess returns of stocks with highest returns over those with lowest returns.

The application of such models goes along the lines of the CAPM with the only difference being the calculation of a multidimensional regression.

%* The Fama and French model has three factors: 
%* the size of firms (**SMB**, small minus big), 
%* book-to-market values (**HML**, high minus low), 
%* **excess return on the market** (portfolio's return less the risk-free rate of return). 
%
%* SMB accounts for publicly traded companies with small market caps that generate higher returns, while HML accounts for value stocks with high book-to-market ratios that generate higher returns in comparison to the market.
%
%* Represent each characteristic of interest ($f_k(t)$) by a portfolio: 
%1. sort all assets by that characteristic;
%2. build the portfolio by going long the top quantile of assets and short the bottom quantile. 
%
%* **The factor corresponding to this characteristic is the return on this portfolio**. 
%* The $X_{nk}(t)$ are estimated for each asset $n$ by regressing over the historical values of $r_n(t)$ and of the factors.

\section{Principal Components Analysis}
\label{sec:pca}

Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the complexity of large datasets, by transforming the original set of variables into a smaller one that still contains most of the initial information.

Reducing the number of variables of a dataset naturally comes at the expense of accuracy. Since smaller datasets are easier to explore and visualize, and make analyzing data simpler and faster without extraneous variables to process, the trick is to trade less accuracy for more simplicity

Let's see how this kind of technique can be implemented starting from 
a multidimensional initial dataset 
\begin{equation}
X=\begin{bmatrix}
x^{(1)}_1 &x^{(2)}_1&\cdots &x^{(m)}_1 \\
x^{(1)}_2 &x^{(2)}_2&\cdots &x^{(m)}_2 \\
\vdots &\vdots &\vdots &\vdots \\
x^{(1)}_n &x^{(2)}_n&\cdots &x^{(m)}_n 
\end{bmatrix}
\end{equation}

To allow an easier representation in the examples of this Section we will use the following two-dimensional dataset.

\begin{ipython}
import numpy as np
from matplotlib import pyplot as plt
	
np.random.seed(123)
x = 5*np.random.rand(100)
y = 2*x + 1 + np.random.rand(100)

y = np.hstack([x, y])
\end{ipython}

Left plot of Fig.~\ref{fig:pca_dataset} shows the dataset.

\subsection{Standardization}
The first step to implement PCA is to standardize the range of the variables so that each one of them contributes equally.

More specifically, the reason why it is critical to perform \emph{standardization} prior to PCA, is that the latter is sensitive to the variances of the initial variables. That is, if there are large differences between the ranges of the variables, those with larger ranges will dominate over those with small ranges (e.g. a variable that goes from 0 and 100 will dominate over another one that ranges between 0 and 1), which will lead to biased results. 

Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each component of each variable

\begin{equation}
z = \cfrac{x - \textrm{mean}}{\textrm{std dev.}}
\end{equation}

Once the standardization is done, all the variables will be transformed to the same scale.

\begin{ipython}
def centerData(X):
    X = X.copy()
    X = (X - np.mean(X, axis=0))/np.std(X, axis=0)
    return X
    
X_centered = centerData(X)
\end{ipython}

Right plot of Fig.~\ref{fig:pca_dataset} shows the standardized dataset used in the following.

Left plot of Fig.~\ref{fig:pca_dataset} shows the dataset.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/pca_dataset_both}
	%\includegraphics[width=0.45\linewidth]{figures/pca_dataset_centered}
	\caption{The dataset used in the PCA example (left), and the same dataset centered in the origin (right).}
	\label{fig:pca_dataset}
\end{figure}

\subsection{Covariance Matrix Computation}
The covariance matrix of our dataset helps us to understand if there is any relationship between the variables (i.e. sometimes, they are so correlated that they contain redundant information). So, in order to identify these correlations, we compute the covariance matrix.

The covariance matrix is a $n\times n$ (where $n$ is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. Since the covariance of a variable with itself is its variance ($\textrm{cov}(a, a)= \textrm{var}(a)$), in the main diagonal (top-left to bottom-right) we actually have the variances of each initial variable. And since the covariance is commutative ($\textrm{cov}(a, b) = \textrm{cov}(b, a)$), the entries of the covariance matrix are symmetric with respect to diagonal.

Once we have centered our data around 0, $[X^T][X]$ is the covariance matrix.

\begin{equation}
[\Sigma]=[X^T][X] =
\begin{bmatrix}
\textrm{var}(x_1) & \textrm{cov}(x_2, x_1) & \cdots & \textrm{cov}(x_n, x_1) \\
\textrm{cov}(x_1, x_2) & \textrm{var}(x_2) & \cdots & \textrm{cov}(x_n, x_2) \\
\vdots & \vdots & \vdots & \vdots \\
\textrm{cov}(x_1, x_n) & \textrm{cov}(x_2, x_n) & \cdots & \textrm{var}(x_n)
\end{bmatrix}
\end{equation}

\subsection{Principal Components}

Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e. principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. 

So, the idea is that a $n$-dimensional dataset gives you $n$ principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.

Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables.
%An important thing to realize here is that, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.

Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has.

It is possible to demonstrate that the eigenvectors of the covariance matrix are actually the directions of the axis where there is the most variance (i.e. most information) and that we call \emph{principal components}. Eigenvalues are simply the coefficients, attached to eigenvectors, which give the amount of variance carried in each principal component.

As we have seen in Sec.~\ref{find-eigenvalues-and-eigenvectors-in-python} the eigenvectors and eigenvalues of our dataset covariance are obtained as follows

\begin{ipython}
eigVals, eigVecs = np.linalg.eig(X_centered.T.dot(X_centered))
print (eigVecs)
\end{ipython}
\begin{ioutput}
[[-0.70710678 -0.70710678]
 [ 0.70710678 -0.70710678]]
\end{ioutput}

\begin{ipython}
print (eigVals)
\end{ipython}
\begin{ioutput}
[  7.46600865 192.53399135]
\end{ioutput}

If we rank the eigenvalues in descending order, we get $\lambda_2>\lambda_1$, which means that the eigenvector that corresponds to the first principal component (PC1) is  $v_2$ (i.e. the second column of the previous matrix).

After having the principal components, to compute the percentage of variance (information) accounted for by each component, we divide the eigenvalue of each component by the sum of eigenvalues. 

\begin{ipython}
for l in eigVals:
    print (f"{l/np.sum(eigVals)*100:.1f}")
\end{ipython}
\begin{ioutput}
3.7
96.3
\end{ioutput}

According to these results PC2 and PC1 carry respectively 96\% and 4\% of the variance of the data.

\subsection{Feature Vector}
Given the list of eigenvectors and eigenvalues we need to choose whether to keep all these components or discard those of lesser significance (i.e. of low eigenvalues), and form with the remaining ones a matrix called \emph{feature vector}.

So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only $p$ eigenvectors (components) out of $n$, the final dataset will have only $p$ dimensions.

Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors $v1$ and $v2$, or discard the eigenvector $v_2$, which is the one of lesser significance, and form a feature vector with $v_1$ only.

Discarding the eigenvector $v_1$ will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set. But given that $v_1$ was carrying only 4\% of the information, the loss will be therefore not important and we will still have 96\% of the information that is carried by $v_2$.

Figure~\ref{fig:pca_result} shows the two principal components.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/pca_result}
	\caption{The directions corresponding to the first two principal 
		components of our example dataset.}
	\label{fig:pca_result}
\end{figure}

\subsection{Recast Data Along Principal Components}
At this point we can use the feature vector to reorient data from the original axis to the ones represented by the principal components. 
This can be done by multiplying the transpose of the original dataset by the transpose of the feature vector

\begin{equation}
[Y] = [X^T][F^T]
\end{equation}

\begin{ipython}
Y = eigVecs.T.dot(X_centered.T)
\end{ipython}

The rotation transformed our dataset that have now the more variance on one of the basis axis (i.e. the Cartesian axis). 
You could keep only this dimension and have a fairly good representation of the data. Figure~\ref{fig:pca_rotated} shows the effect of such a rotation to our sample.
\clearpage
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.5\linewidth]{figures/pca_rotated}
	\caption{The dataset rotated so that the usual Cartesian coordinates
		correspond to the first two principal components.}
	\label{fig:pca_rotated}
\end{figure}

\subsection{Impact of Covid19}
Using PCA analysis it can be assessed the impact of the Covid19 pandemic on different stocks, clustering together assets that were most/least affected without any prior knowledge of their fundamental.

In this analysis we are going to use the equities forming the \emph{Dow Jones 30} index, so the first step is to download their returns with \texttt{yfinance} package (the data availability is from March 2019 to today).

\begin{ipython}
import yfinance as yf

tickers = ["BA","CAT","CVX","CSCO","KO",
	       "DOW","XOM","GS","HD","INTC","IBM",
	       "JNJ","JPM","MCD","MRK","MSFT","NKE",
	       "PFE","PG","RTX","TRV","UNH","VZ",
	       "V","WBA", "WMT", "^DJI"]
	
proxy = yf.Tickers(tickers)           
df = proxy.history(start='2020-01-02',
                   end='2020-08-16')['Close'].pct_change().dropna()
\end{ipython}

After downloading data the PCA analysis is performed
\begin{ipython}
import numpy as np
	
equities = df.iloc[:, :-1]
cov = equities.cov()
eigVals, eigVecs = np.linalg.eig(cov)
\end{ipython}

Let's look at the first principal component and select the corresponding stocks that have the best and the worst PCA weights, as shown in Fig.~\ref{fig:pca_covid_neg}.

\begin{figure}[htb]
\centering
\includegraphics[width=.6\textwidth]{figures/pc1_weights_covid}
\caption{Ten most negative (top) and least negative (bottom) weights of the PC1 portfolio.}
\label{fig:pca_covid_neg}
\end{figure}

Notice from Figure~\ref{fig:pca_covid_neg} how the most negative stocks are Boing and Chevron, connected to travel and energy sector. This makes sense since Covid19 heavily impacted the traveling business, as well as the energy companies that provide fuels for those business. On the other hand, the least impacted companies are Verizon, Wallmart, Pfizer and Johnson \& Johnson, which belong to sectors boosted during the quarantine measures and the pandemic in general.

In addition we can check the winning portfolio that is long the top 3 companies according to the PCA weights.

\begin{ipython}
# normalize weights
norm_eigVecs = abs(eigVecs.T[0])/sum(abs(eigVecs.T[0]))
	
# market portfolio
pc_daily_ret = equities.dot(norm_eigVecs)
pc_cum_ret = pc_daily_ret.cumsum()
	
# worst portfolio
worst_idx = np.argsort(eigVecs.T[0])[:3]
eq_worst = equities.iloc[:, worst_idx]
worst_eigv = norm_eigVecs[worst_idx]
worst_ret = eq_worst.dot(worst_eigv).cumsum()
print ("Worst :", df.columns[worst_idx])
	
# best portfolio         
best_idx = np.argsort(eigVecs.T[0])[-3:]
eq_best = equities.iloc[:, best_idx]
best_eigv = norm_eigVecs[best_idx]
best_ret = eq_best.dot(best_eigv).cumsum()
print ("Best :", df.columns[best_idx])
\end{ipython}
\begin{ioutput}
Worst : Index(['BA', 'CVX', 'DOW'], dtype='object')
Best : Index(['JNJ', 'VZ', 'WMT'], dtype='object')
\end{ioutput}

As shown in Figure~\ref{fig:pca_covid_return}, the resulting portfolio would have performed significantly better than the market, since it invested in companies that actually didn't suffer to much from the pandemic. 

\begin{figure}[bhtp]
\centering
\includegraphics[width=.7\textwidth]{figures/pc1_weights_covid_returns}
\caption{Three most negative and least negative weights portfolios compared to the market portfolio.}
\label{fig:pca_covid_return}
\end{figure}

Note that this portfolio is formed with look-ahead bias, indeed portfolio weights are computed using future dates that were not available at the time of market downturn. The PCA used in this way is therefore a backward looking analytics tool.
































\subsection{Dimension Reduction}
In statistics, dimensionality reduction is the transformation of data such that the number of variables is reduced by obtaining principal variables. Dimensionality reduction can help us see the key structure of original data. For example, if the data lies in a low dimensional subspace, it is conceivable that one could restrict our learning problem to this low dimensional subspace and thereby simplify it.

There are two techniques of dimensionality reduction that can be applied to the feature
space:
\begin{itemize}
\item feature selection: is the process of selection of a subset of relevant features, which can be done manually based on domain knowledge or using statistical tools
\item feature projection: converts data from high-dimensional space to low-dimensional space. The data transformation can be linear or nonlinear.
\end{itemize}

The basic linear technique for dimensionality reduction, the principal component analysis (PCA), performs a linear mapping of the data into a smaller space such that the variance of the data in the low-dimensional representation is maximized. 

In practice, a covariance matrix (and sometimes a correlation matrix) of the data is constructed, and the eigenvectors of this matrix are calculated. The eigenvectors corresponding to the largest eigenvalues (principal components) can now be used to recover most of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behaviour of the system. 

The original space (with a dimension equal to the number of points) is reduced (with loss of data, but with the hope that the most important variance remains) to a space stretched over a few eigenvectors.

\subsection{Principal Component Analysis}
The goal of PCA is to find an alternative representation of data $X$ in the transformed space so that $X$ can be approximated by variables of smaller dimensions while maintaining a given level of the original information.
Let us denote a zero mean $p$-dimensional feature space $X = (X_1, \ldots, X_p)$. We aim to obtain a \textbf{linear} transformation $V$ such that the $p$-dimensional transformed feature variable $Z := XV$ consists of components orthogonal to each other, and the variances of those components are in descending order. Alternatively, we are aiming to find the representation of $\tilde{X} = ZV^{-1}$ can be seen as a projection onto a new feature space, on which a dimensionality reduction approach can be built.

Figure~\ref{fig:pca_dataset} illustrates the main idea of PCA: to find such orthogonal vectors (principal components) $Z_k$ for $k = 1,\ldots, p$, and to reduce the dimensionality of the original data $X$ by mapping this data to the space generated by first $l$, $l = 1,\ldots, p - 1$ vectors. % (on this figure $p = 3$ and $l = 2$).

%Right plot of Fig.~\ref{fig:pca_dataset} shows the standardized dataset used in the following.

Left plot of Fig. shows the dataset.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/pca_dataset_both}
	%\includegraphics[width=0.45\linewidth]{figures/pca_dataset_centered}
	\caption{The dataset used in the PCA example (left), and the same dataset centered in the origin (right).}
	\label{fig:pca_dataset}
\end{figure}


\subsection{Principal components and covariance}
Let’s recall some basic definitions from probability theory and statistics:
\begin{itemize}
\item for the vector $X = (X_1, X_2, \ldots, X_k)$ of $k$ jointly distributed random variables with finite second moments, its auto-covariance matrix $\SigmaX$ is defined as follows:
\begin{equation}
\Sigma_X = \text{Cov}(X, X) = E[(X − E[X])(X − E[X])^T] = E[XX^T]–E[X]E[X]^T
\end{equation}
\item let $X$ be a random vector with covariance matrix $\Sigma_X$, and let $A$ be a matrix that can
act on $X$ on the left. The covariance matrix of the matrix-vector product $AX$ is:
\begin{equation}
\text{Cov}(AX, AX) = E[(AX)(AX)^T]–E[AX]E[(AX)^T] = E[AXX^T A^T]–E[AX]E[A^T X^T] = AE[XX^T]A^T − AE[X]E^T A^T =
= A(E[XX^T]–E[X]E[X]^T)A^T = A\Sigma_XA^T
\end{equation}
\item The empirical covariance matrix $X$ can be written as 
\begin{equation}
\Sigma_X = \cfrac{1}{N − 1}XX^T
\end{equation}
\item For every $n\times n$ real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen real and orthonormal. Thus a real symmetric matrix $A$ can be decomposed as
\begin{equation}
A = Q\Lambda Q^T
\end{equation}
where $Q$ is an orthogonal matrix whose columns are (the above chosen, real and orthonormal) eigenvectors of $A$, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$.
By using the \emph{singular value decomposition} we can write this empirical matrix:
\begin{equation}
\Sigma_X = \cfrac{1}{N-1}XX^T = \cfrac{1}{N-1}(UDV^T)(UDV^T)^T = V\left(\cfrac{1}{N-1}D^2\right) V^T
\end{equation}

Additionaly as the covariance matrix $\Sigma_X$ is a real non-negative symmetric matrix we can use an \emph{eigenvalue decomposition}:
\begin{equation}
\Sigma_X = Q\Lambda Q^T
\end{equation}

It is clear that the right singular vectors in equation ... are the eigenvectors in ...p: $V = Q$, and the diagonal matrix $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$ is $\left(\frac{1}{N-1}D^2\right)$, i.e componentwise:
\begin{equation}
\lambda_j = \cfrac{d_j}{N − 1}
\end{equation}

That means that the new coordinates $V$ are given by the eigenvectors of the covariance matrix.
Now finally the empirical covariance matrix of the principal components $Z$ can be rewritten
\begin{equation}
\Sigma_Z = \cfrac{1}{N-1}Z^T Z = \cfrac{1}{N-1}(XV)^T(XV)=V^T\Sigma_X V = \cfrac{D^2}{N-1} = \Lambda
\end{equation}
\end{itemize}

To summarize:
\begin{enumerate}
\item The principal components $Z_1, \ldots, Z_p$ are orthogonal (i.e. $Z^{-1}=Z^T$), and the variances of $Z_1,\ldots, Z_p$ are the eigenvalues of the covariance matrix of $X$ in descending order.
\item With the representation of the covariance of the data using SVD, we can apply PCA to reduce the dimensionality of the data.

Once we have the principal components and the loading matrix, the representation of $X$ becomes
\begin{equation}
X = ZV^{−1} = ZV^T = \sum_{j=1}^{p}Z_j v_j^T
\end{equation}

Thus an approximating sequence of feature variables $X$ is obtained. In section (2.5) we have shown that $\hat{X˜}(k)$ including the first $k$ principal components of $X$:
\begin{equation}
\hat{X}^{(k)} = \sum_{j=1}^{k}Z_j v_j^T
\end{equation}
covers such proportion
\begin{equation}
\cfrac{\sum_j=1}^k \lambda_j}{\sum_j=1}^p \lambda_j}
\end{equation}
of the variance of $X$. From the equation above, it is obvious that if we take $k = p$, we recover all the original data of $X$.

Assuming, therefore, that the information is incorporated in the covariance, given the data set $X$ and the information preservation criterion in the form of the percentage of variance explained, we can find an approximation of $X$ with reduced dimensionality.
The residual from the approximation using $k$ PCs is 
\begin{equation}
R^{(k)} = X − \tilde{X}˜^{(k)} = \sum_{j=k+1}^{p}Z_j v^T_j
\end{equation}






\subsection{Practical problems}
PCA is not invariant to scaling. Before applying PCA, we should consider whether or not to standardize the data $X$, besides removing of the mean from the data before applying PCA. In practice, it depends on the context of the data and the objective, but there are two general tips regarding this problem:
\begin{enumerate}
\item If features are measured in different units that are not comparable, we should first standardize them before applying PCA.
\item If features are measured in comparable units, then we can keep them unscaled to preserve the original variability information.
\end{enumerate}

Furthermore PCA can not be used if the data is not Gaussian. PCA is based on the assumption that the data is normally distributed, that is why we can represent all the information from the covariance. In the case of non-Gaussian data, independent component analysis (ICA) is often used.

### Application: Term Structure Analysis Using PCA

#### Bond yield and yield curve

A bond’s yield is the interest income received by an investor from investing in debt securities. Bonds with a fixed coupon have an interest rate that accrues on the face value.

Current yield is the ratio of annual coupon payments to the current
market value of the bond:
$$
\text{Current Yield} = \cfrac{\text{Annual Coupon Payment}}{\text{Bond Price}}
$$

The yield curve represents the time structure of interest rates and shows the relationship between the yields of financial instruments and their maturity.

%Using this tool, an investor gets an idea of several market properties of traded bonds and can also predict the potential behaviour of the security’s price under the influence of market factors.

%By analysing graphical and tabular data, it is possible to assess the current state of the market, calculate fair premiums and calculate bond prices under forecasted interest rate movements.

#### Treasuries bond yield curve and recession
The Treasuries bond yield curve is not only used as a benchmark to assess the value of other debt instruments but is also considered one of the most important economic indicators to watch as a precursor to a future cyclical downturn in the US economy.

Usually, yields of longer-dated securities are higher than those of shorter-dated securities (this is a compensation for risk).
Such a yield curve is considered ”normal” because the market usually requires more compensation for more risk under normal conditions. Long-term bonds are subject to greater risks, such as changes in interest rates and increased potential default risk.
However, the short-term yield sometimes exceeds the long-term yield, and the spread turns negative. The result is a *concave* curve.

%If short securities turn out to be more profitable than long ones, investors do not believe in the long term, i.e. they are afraid to hold long bonds.

The inversion (concavity) of the Treasuries curve in the US is considered one of the proxies for an imminent recession or downturn in the economy.
%At least, as history shows, in seven out of ten cases, it has indeed become a clear harbinger of an American recession.

In the following example, we analyse the data of Treasury bonds between 1st January 2017 and 31st December 2018 taken using the Federal Reserve Bank of St. Louis API.

The US economy has been growing steadily in the period under review since after a rebound in 2016, it grew by 3.1% in 2018 after climbing by 2.5% in 2017

### Factor Model
Let $X$ be the yield curve consisting of bonds of $p$ maturities with $N$ observations. We will model $X$ as a $k$ factor model.

The $k$ factor model for the yield curve $X$ can then be presented in the following way

$$
X = \mu_ X + Zf + e
$$
where $\mu_X = (\mu_1,\ldots, \mu_p)$ is the mean vector of $X$, $e = (e_1,\ldots, e_N )$ is the residuals and

$$
Z =
\begin{bmatrix}
z_{1,1} & z_{1,2} & \cdots & z_{1,k} \\
z_{2,1} & z_{2,2} & \cdots & z_{2,k} \\
\vdots & \vdots & \ddots & \vdots \\
z_{N,1} & z_{N,2} & \cdots & z_{N,k} \\
\end{bmatrix}
$$
consist of $k$ factors,
$$
f =
\begin{bmatrix}
f_{1,1} & f_{1,2} & \cdots & f_{1,p} \\
f_{2,1} & f_{2,2} & \cdots & f_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
f_{k,1} & f_{k,2} & \cdots & f_{k,p} \\
\end{bmatrix}
$$
the factor loading matrix; i.e., the observation $i$ of feature $j$ is

$$
x_{i,j} = \mu_j + \sum_{l=1}^k z_{i,l}f_{l,j} + e_{i,j}
$$

The difference with the previous studies is that instead of identifying external factors, we will focus on self-contained independent drivers, i.e., factors related to the curve itself, such as slope and curvature.

Through this approach, we will recognize the key structure of the yield curve dynamics and will be able to answer how 2Y, 10Y, and 30Y bond yields move together. Moreover, PCA will help us to naturally reduce the dimensionality of the factors based on the coefficient of explained variance.


### Data and observation
The yield/rate time series of different maturities are shown in Figure 3.1. It is clear that all curves are moving in the same direction, which indicates the idea that some common factors drive the yield dynamic.

In Figure 3.2 we can see the yield curve term structure on different dates. In contrast to the previous figure, here one can distinctly see the inconsistencies in the dynamics and it is obvious that in some periods the movement is quite the opposite.


Let us consider in more detail the different types of changes between the dates:
1. Level change: The level shock modifies the interest rates of all maturities in nearly equal amounts, inducing a parallel shift that changes the level of the entire yield curve. An example of level change can be seen by observing the green (2018-02-05) and orange (2018-05-17) curves. Both are moving in the same direction with almost identical slopes, and the only difference is the level: the green curve is shifted compared to the orange one.
2. Slope change: The shock to the slope factor increases short-term interest rates by much more than long-term interest rates so that the yield curve becomes less abrupt and its slope decreases.
For example, from 2018-05-17 (orange) to 2018-12-10 (pink), the slope of the curve changed as the short-term yield increased while the long-term yield decreased (DGS2 to DGS5).
3. Curvature change: The change in curvature reflects how the difference between the medium-long-term premium (i.e., the long-term minus the medium-term rate level) and the shortmedium-term premium (i.e., the medium-term minus the short-term rate level) changes from each day. In other words, the ”belly” of the curve shifts relatively to variations in short-term and long-term levels daily. Moreover, we can say that the orange (2018-05-17) and green (2018-02-05) curves show us a classic example of the so-called \emph{normal yield curve}, a concave curve with a positive slope.







%\subsection{Dimension Reduction}
%In statistics, dimensionality reduction is the transformation of data such that the number of variables is reduced by obtaining principal variables. Dimensionality reduction can help us see the key structure of original data. For example, if the data lies in a low dimensional subspace, it is conceivable that one could restrict our learning problem to this low dimensional subspace and thereby simplify it.
%
%There are two techniques of dimensionality reduction that can be applied to the feature
%space:
%\begin{itemize}
%	\item feature selection: is the process of selection of a subset of relevant features, which can be done manually based on domain knowledge or using statistical tools
%	\item feature projection: converts data from high-dimensional space to low-dimensional space. The data transformation can be linear or nonlinear.
%\end{itemize}
%
%The basic linear technique for dimensionality reduction, the principal component analysis (PCA), performs a linear mapping of the data into a smaller space such that the variance of the data in the low-dimensional representation is maximized. 
%
%In practice, a covariance matrix (and sometimes a correlation matrix) of the data is constructed, and the eigenvectors of this matrix are calculated. The eigenvectors corresponding to the largest eigenvalues (principal components) can now be used to recover most of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behaviour of the system. 
%
%The original space (with a dimension equal to the number of points) is reduced (with loss of data, but with the hope that the most important variance remains) to a space stretched over a few eigenvectors.
%
%\subsection{Principal Component Analysis}
%The goal of PCA is to find an alternative representation of data $X$ in the transformed space so that $X$ can be approximated by variables of smaller dimensions while maintaining a given level of the original information.
%Let us denote a zero mean $p$-dimensional feature space $X = (X_1, \ldots, X_p)$. We aim to obtain a \textbf{linear} transformation $V$ such that the $p$-dimensional transformed feature variable $Z := XV$ consists of components orthogonal to each other, and the variances of those components are in descending order. Alternatively, we are aiming to find the representation of $\tilde{X} = ZV^{-1}$ can be seen as a projection onto a new feature space, on which a dimensionality reduction approach can be built.
%
%Figure~\ref{fig:pca_dataset} illustrates the main idea of PCA: to find such orthogonal vectors (principal components) $Z_k$ for $k = 1,\ldots, p$, and to reduce the dimensionality of the original data $X$ by mapping this data to the space generated by first $l$, $l = 1,\ldots, p - 1$ vectors. % (on this figure $p = 3$ and $l = 2$).
%
%%Right plot of Fig.~\ref{fig:pca_dataset} shows the standardized dataset used in the following.
%
%Left plot of Fig. shows the dataset.
%\begin{figure}[htb]
%	\centering
%	\includegraphics[width=0.9\linewidth]{figures/pca_dataset_both}
%	%\includegraphics[width=0.45\linewidth]{figures/pca_dataset_centered}
%	\caption{The dataset used in the PCA example (left), and the same dataset centered in the origin (right).}
%	\label{fig:pca_dataset}
%\end{figure}
%
%
%\subsection{Principal components and covariance}
%Let’s recall some basic definitions from probability theory and statistics:
%\begin{itemize}
%	\item for the vector $X = (X_1, X_2, \ldots, X_k)$ of $k$ jointly distributed random variables with finite second moments, its auto-covariance matrix $\Sigma X$ is defined as follows:
%	\begin{equation}
%		\Sigma_X = \text{Cov}(X, X) = E[(X − E[X])(X − E[X])^T] = E[XX^T]–E[X]E[X]^T
%	\end{equation}
%	\item let $X$ be a random vector with covariance matrix $\Sigma_X$, and let $A$ be a matrix that can
%	act on $X$ on the left. The covariance matrix of the matrix-vector product $AX$ is:
%	\begin{equation}
%		\text{Cov}(AX, AX) = E[(AX)(AX)^T]–E[AX]E[(AX)^T] = E[AXX^T A^T]–E[AX]E[A^T X^T] = AE[XX^T]A^T − AE[X]E^T A^T =
%		= A(E[XX^T]–E[X]E[X]^T)A^T = A\Sigma_XA^T
%	\end{equation}
%	\item The empirical covariance matrix $X$ can be written as 
%	\begin{equation}
%		\Sigma_X = \cfrac{1}{N − 1}XX^T
%	\end{equation}
%	\item For every $n\times n$ real symmetric matrix, the eigenvalues are real and the eigenvectors can be chosen real and orthonormal. Thus a real symmetric matrix $A$ can be decomposed as
%	\begin{equation}
%		A = Q\Lambda Q^T
%	\end{equation}
%	where $Q$ is an orthogonal matrix whose columns are (the above chosen, real and orthonormal) eigenvectors of $A$, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$.
%	By using the \emph{singular value decomposition} we can write this empirical matrix:
%	\begin{equation}
%		\Sigma_X = \cfrac{1}{N-1}XX^T = \cfrac{1}{N-1}(UDV^T)(UDV^T)^T = V\left(\cfrac{1}{N-1}D^2\right) V^T
%	\end{equation}
%	
%	Additionaly as the covariance matrix $\Sigma_X$ is a real non-negative symmetric matrix we can use an \emph{eigenvalue decomposition}:
%	\begin{equation}
%		\Sigma_X = Q\Lambda Q^T
%	\end{equation}
%	
%	It is clear that the right singular vectors in equation ... are the eigenvectors in ...p: $V = Q$, and the diagonal matrix $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$ is $\left(\frac{1}{N-1}D^2\right)$, i.e componentwise:
%	\begin{equation}
%		\lambda_j = \cfrac{d_j}{N − 1}
%	\end{equation}
%	
%	That means that the new coordinates $V$ are given by the eigenvectors of the covariance matrix.
%	Now finally the empirical covariance matrix of the principal components $Z$ can be rewritten
%	\begin{equation}
%		\Sigma_Z = \cfrac{1}{N-1}Z^T Z = \cfrac{1}{N-1}(XV)^T(XV)=V^T\Sigma_X V = \cfrac{D^2}{N-1} = \Lambda
%	\end{equation}
%\end{itemize}
%
%To summarize:
%\begin{enumerate}
%	\item The principal components $Z_1, \ldots, Z_p$ are orthogonal (i.e. $Z^{-1}=Z^T$), and the variances of $Z_1,\ldots, Z_p$ are the eigenvalues of the covariance matrix of $X$ in descending order.
%	\item With the representation of the covariance of the data using SVD, we can apply PCA to reduce the dimensionality of the data.
%	
%	Once we have the principal components and the loading matrix, the representation of $X$ becomes
%	\begin{equation}
%		X = ZV^{−1} = ZV^T = \sum_{j=1}^{p}Z_j v_j^T
%	\end{equation}
%	
%	Thus an approximating sequence of feature variables $X$ is obtained. In section (2.5) we have shown that $\hat{X}(k)$ including the first $k$ principal components of $X$:
%	\begin{equation}
%		\hat{X}^{(k)} = \sum_{j=1}^{k}Z_j v_j^T
%	\end{equation}
%	covers such proportion
%	\begin{equation}
%		\cfrac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}
%\end{equation}
%of the variance of $X$. From the equation above, it is obvious that if we take $k = p$, we recover all the original data of $X$.
%
%Assuming, therefore, that the information is incorporated in the covariance, given the data set $X$ and the information preservation criterion in the form of the percentage of variance explained, we can find an approximation of $X$ with reduced dimensionality.
%The residual from the approximation using $k$ PCs is 
%\begin{equation}
%R^{(k)} = X − \tilde{X}˜^{(k)} = \sum_{j=k+1}^{p}Z_j v^T_j
%\end{equation}


\section{Principal Components Analysis}
\label{sec:pca}

\emph{Principal Component Analysis} (PCA), is a dimensionality-reduction method that is often used to reduce the complexity of large datasets, by transforming the original set of variables into a smaller one that still contains most of the information of the large set.

Reducing the number of variables of a dataset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. 

So, to sum up, the idea of PCA is simple: reduce the number of variables of a dataset, while preserving as much information as possible.

\subsection{Principal Components}

Principal components are new variables that are constructed as linear combinations or mixtures of the original ones. These combinations are done in such a way that the new variables (i.e. principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. 

So, the idea is that a $n$-dimensional dataset gives you $n$ principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.

Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, by discarding the components with low information and considering the remaining components as your new variables.

\emph{An important thing to realize here is that the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.}

Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has.
%To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible.

\subsection{Step-by-Step Explanation of PCA}

Let's see how this kind of technique can be implemented starting from a multidimensional initial dataset 
\begin{equation}
	X=\begin{bmatrix}
		x^{(1)}_1 &x^{(2)}_1&\cdots &x^{(m)}_1 \\
		x^{(1)}_2 &x^{(2)}_2&\cdots &x^{(m)}_2 \\
		\vdots &\vdots &\vdots &\vdots \\
		x^{(1)}_n &x^{(2)}_n&\cdots &x^{(m)}_n 
	\end{bmatrix}
\end{equation}

For the sake of simplicity let's imagine that our sample is the two-dimensional one represented in the left graph of Fig.~\ref{fig:pca_dataset}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/pca_example_raw_normalized_data.png}
	\caption{The dataset used in the PCA example (left), and the same dataset after \emph{normalization} (right).}
	\label{fig:pca_dataset}
\end{figure}

\subsubsection*{Step 1: Standardization}

The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.

More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (for example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.

Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable
\begin{equation*}
x' = \cfrac{x-\bar{x}}{\sigma_x}
\end{equation*}

Once the standardization is done, all the variables will be transformed to the same scale. Right plot of Fig.~\ref{fig:pca_dataset} shows the standardized dataset used in the following.

\subsubsection*{Step 2: Covariance Matrix Computation}

The aim of this step is to understand how the variables of the input dataset are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. 

The covariance matrix is a symmetric $n\times n$ (where $n$ is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. %Since the covariance of a variable with itself is its variance ($\textrm{cov}(a, a)= \textrm{var}(a)$), in the main diagonal (top-left to bottom-right) we actually have the variances of each initial variable. And since the covariance is commutative ($\textrm{cov}(a, b) = \textrm{cov}(b, a)$), the entries of the covariance matrix are symmetric with respect to diagonal.
Once we have centred our data around 0, $[X^T][X]$ is the covariance matrix.

\begin{equation}
	[\Sigma]=[X^T][X] =
	\begin{bmatrix}
		\textrm{var}(x_1) & \textrm{cov}(x_2, x_1) & \cdots & \textrm{cov}(x_n, x_1) \\
		\textrm{cov}(x_1, x_2) & \textrm{var}(x_2) & \cdots & \textrm{cov}(x_n, x_2) \\
		\vdots & \vdots & \vdots & \vdots \\
		\textrm{cov}(x_1, x_n) & \textrm{cov}(x_2, x_n) & \cdots & \textrm{var}(x_n)
	\end{bmatrix}
\end{equation}

What do the covariances that we have as entries of the matrix tell us about the correlations between the variables ?
It’s actually the sign of the covariance that matters:
\begin{itemize}
	\item if positive then: the two variables increase or decrease together (correlated);
	\item if negative then: one increases when the other decreases (Inversely correlated).
\end{itemize}

\subsubsection*{Step 3: Compute the Eigenvectors and Eigenvalues of the Covariance Matrix to Identify the Principal Components}

It is possible to demonstrate that the eigenvectors of the covariance matrix are actually the directions of the axis where there is the most variance (i.e. most information) and that we call \emph{principal components}. Eigenvalues are simply the coefficients, attached to eigenvectors, which give the amount of variance carried in each principal component (see Sec.~\ref{eigenvectors-and-eigenvalues} for a description of eigenvectors-eigenvalues).

For our example the eigenvectors are defined as
\begin{ioutput}
        PC1       PC2
x  0.707107  0.707107
y  0.707107 -0.707107
\end{ioutput}
with the following eigenvalues (normalized to 1)
\begin{ioutput}
array([0.99662716, 0.00337284])
\end{ioutput}

According to these results PC1 and PC2 carry respectively 99.7\% and 0.3\% of the variance of the data.

\subsubsection*{Step 4: Create a Feature Vector}

In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call \emph{feature vector}.

So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only $p$ eigenvectors (components) out of $n$, the final data set will have only $p$ dimensions.

Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors $v1$ and $v2$, or discard the eigenvector $v_2$, which is the one of lesser significance, and form a feature vector with $v_1$ only.

Discarding the eigenvector $v_1$ will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set. But given that $v_1$ was carrying only 0.3\% of the information, the loss will be therefore not important and we will still have 99.7\% of the information that is carried by $v_2$.

Figure~\ref{fig:pca_result} shows the two principal components.

\begin{figure}[hbt]
\centering
\includegraphics[width=0.5\linewidth]{figures/pca_components}
\caption{The directions corresponding to the first two principal components of our example dataset.}
\label{fig:pca_result}
\end{figure}

\subsubsection*{Step 5: Recast Data Along Principal Components}

At this point we can use the feature vector to re-orient data from the original axis to the ones represented by the principal components. 
This can be done by multiplying the transpose of the original dataset by the transpose of the feature vector

\begin{equation}
[Y] = [X^T][F^T]
\end{equation}

The rotation transformed our dataset that have now the more variance on one of the Cartesian axis. 
You could keep only this dimension and have a fairly good representation of the data. Figure~\ref{fig:pca_rotated} shows the effect of such a rotation to our sample.
%\clearpage
\begin{figure}[hbtp]
\centering
\includegraphics[width=0.5\linewidth]{figures/pca_projected}
\caption{The dataset rotated so that the usual Cartesian coordinates correspond to the first two principal components.}
\label{fig:pca_rotated}
\end{figure}

\subsection{Practical problems}
PCA is not invariant to scaling. Before applying PCA, we should consider whether or not to standardize the data $X$, besides removing of the mean from the data before applying PCA. In practice, it depends on the context of the data and the objective, but there are two general tips regarding this problem:
\begin{enumerate}
	\item If features are measured in different units that are not comparable, we should first standardize them before applying PCA.
	\item If features are measured in comparable units, then we can keep them unscaled to preserve the original variability information.
\end{enumerate}

Furthermore PCA can not be used if the data is not Gaussian. PCA is based on the assumption that the data is normally distributed, that is why we can represent all the information from the covariance. In the case of non-Gaussian data, independent component analysis (ICA) is often used.

\begin{finmarkets}
In the previous example and in the following the \texttt{PCAWrapper} class of the \texttt{finmarkets} module has been used. 
This is a simple wrapper around the \texttt{sklearn.decomposition.PCA} functions with the addition of few simple utilities to extract components, eigenvalues and project the data along the new axis.
\end{finmarkets}

\begin{ipython}
import pandas as pd, numpy as np

from sklearn.decomposition import PCA

class PCAWrapper:
    def __init__(self, X, normalize=False):
        self.X = X
        self.n_features = X.shape[1]
        self.index = X.index
        self.Xc = (X - np.mean(X, axis=0))
        if normalize:
            self.Xc /= np.std(self.Xc, axis=0)
        self.pc_names = lambda n: ['PC' + str(i) for i in np.arange(1, n + 1)]

    def to_df_pc(self, data, is_loading=False):
        cols = self.pc_names(self.n_features)
        idx = self.X.columns if is_loading else self.index
        return pd.DataFrame(data, columns=cols, index=idx)

    def pca(self, n_pc=None):
        if n_pc:
            model = PCA(n_components=n_pc).fit(self.Xc)
        else:
            model = PCA().fit(self.Xc)
        return model

    def cps(self):
        cps = self.pca().components_.T
        cps = self.to_df_pc(cps, is_loading=True)
        return cps

    def scores(self): # project on the new axis
        scores = self.pca().transform(self.Xc)
        scores = self.to_df_pc(scores)
        return scores

    def cumsum_expvar_ratio(self):
        var_exp = self.pca().explained_variance_ratio_
        var_exp_cumsum = np.cumsum(var_exp)
        return var_exp, var_exp_cumsum

    def x_projected(self, p, centered=False):
        xp = self.scores().iloc[:, 0:p].dot(self.cps().T.iloc[0:p, :])
        if not centered:
            xp = xp + self.X.mean()
        return xp

    def residuals(self, p):
        residuals = self.X - self.x_projected(p, centered=False)
        return residuals
\end{ipython}

\subsection{Term Structure Analysis Using PCA}

A bond’s yield is the interest income received by an investor from investing in debt securities. Bonds with a fixed coupon have an interest rate that accrues on the face value.

Current yield is the ratio of annual coupon payments to the current market value of the bond:
$$
\text{Current Yield} = \cfrac{\text{Annual Coupon Payment}}{\text{Bond Price}}
$$

The yield curve represents the time structure of interest rates and shows the relationship between the yields of financial instruments and their maturity.

The Treasuries bond yield curve is not only used as a benchmark to assess the value of other debt instruments but is also considered one of the most important economic indicators to watch as a precursor to a future cyclical downturn in the US economy.

Usually, yields of longer-dated securities are higher than those of shorter-dated securities (this is a compensation for risk).
Such a yield curve is considered \emph{normal} because the market usually requires more compensation for more risk under normal conditions. Long-term bonds are subject to greater risks, such as changes in interest rates and increased potential default risk.
However, the short-term yield sometimes exceeds the long-term yield, and the spread turns negative. The result is a \emph{concave} curve.

The \emph{inversion} (concavity) of the Treasuries curve in the US is considered one of the proxies for an imminent recession or downturn in the economy.

In the following example, we analyse the data of Treasury bonds between $1^{st}$ January 2017 and $31^{st}$ December 2018 taken using the Federal Reserve Bank of St. Louis API.
The US economy has been growing steadily in the period under review since after a rebound in 2016, it grew by 3.1\% in 2018 after climbing by 2.5\% in 2017.

The dateset is available \href{https://github.com/matteosan1/finance_course/raw/develop/input_files/DGS_2017_2018.csv"}{here}.

\subsection{Factor Model}
Let $X$ be the yield curve consisting of bonds of $p$ maturities with $N$ observations. We will model $X$ as a $k$ factor model.

The $k$ factor model for the yield curve $X$ can then be presented in the following way

$$
X = \mu_ X + Zf + e
$$
where $\mu_X = (\mu_1,\ldots, \mu_p)$ is the mean vector of $X$, $e = (e_1,\ldots, e_N )$ is the residuals and

$$
Z =
\begin{bmatrix}
	z_{1,1} & z_{1,2} & \cdots & z_{1,k} \\
	z_{2,1} & z_{2,2} & \cdots & z_{2,k} \\
	\vdots & \vdots & \ddots & \vdots \\
	z_{N,1} & z_{N,2} & \cdots & z_{N,k} \\
\end{bmatrix}
$$
consist of $k$ factors,
$$
f =
\begin{bmatrix}
	f_{1,1} & f_{1,2} & \cdots & f_{1,p} \\
	f_{2,1} & f_{2,2} & \cdots & f_{2,p} \\
	\vdots & \vdots & \ddots & \vdots \\
	f_{k,1} & f_{k,2} & \cdots & f_{k,p} \\
\end{bmatrix}
$$
the factor loading matrix; i.e., the observation $i$ of feature $j$ is

$$
x_{i,j} = \mu_j + \sum_{l=1}^k z_{i,l}f_{l,j} + e_{i,j}
$$

The difference with the previous studies is that instead of identifying external factors, we will focus on self-contained independent drivers, i.e., factors related to the curve itself, such as slope and curvature.

Through this approach, we will recognize the key structure of the yield curve dynamics and will be able to answer how 2Y, 10Y, and 30Y bond yields move together. Moreover, PCA will help us to naturally reduce the dimensionality of the factors based on the coefficient of explained variance.

\subsection{PCA on Term Structure}

\begin{ipython}
import pandas as pd

df = pd.read_csv("DGS_2017_2018.csv", index_col="date")

print (df.head())
\end{ipython}
\begin{ioutput}
DGS1    DGS2    DGS5    DGS7   DGS10   DGS20   DGS30
date                                                              
2017-01-03  0.0089  0.0122  0.0194  0.0226  0.0245  0.0278  0.0304
2017-01-04  0.0087  0.0124  0.0194  0.0226  0.0246  0.0278  0.0305
2017-01-05  0.0083  0.0117  0.0186  0.0218  0.0237  0.0269  0.0296
2017-01-06  0.0085  0.0122  0.0192  0.0223  0.0242  0.0273  0.0300
2017-01-09  0.0082  0.0121  0.0189  0.0218  0.0238  0.0269  0.029
\end{ioutput}

The yield/rate time series of different maturities are shown in the top plot of Fig.~\ref{fig:yield_rate_ts}. It is clear that all curves are moving in the same direction, which indicates the idea that some common factors drive the yield dynamic.

In bottom Fig.~\ref{fig:yield_rate_ts} we can see the yield curve term structure on different dates. In contrast to the previous figure, here one can distinctly see the inconsistencies in the dynamics and it is obvious that in some periods the movement is quite the opposite.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/yield_rate_ts}
	\caption{Yield rate time series for different maturity Treasury bonds (top). Yield rate at different dates (bottom).}
	\label{fig:yield_rate_ts}
\end{figure}

Let us consider in more detail the different types of changes between the dates:
\begin{enumerate}
\item Level change: The level shock modifies the interest rates of all maturities in nearly equal amounts, inducing a parallel shift that changes the level of the entire yield curve. An example of level change can be seen by observing the green (2018-02-05) and orange (2018-05-17) curves. Both are moving in the same direction with almost identical slopes, and the only difference is the level: the green curve is shifted compared to the orange one.
\item Slope change: The shock to the slope factor increases short-term interest rates by much more than long-term interest rates so that the yield curve becomes less abrupt and its slope decreases.
For example, from 2018-05-17 (orange) to 2018-12-10 (pink), the slope of the curve changed as the short-term yield increased while the long-term yield decreased (DGS2 to DGS5).
\item Curvature change: The change in curvature reflects how the difference between the medium-long-term premium (i.e., the long-term minus the medium-term rate level) and the short-medium-term premium (i.e., the medium-term minus the short-term rate level) changes from each day. In other words, the ”belly” of the curve shifts relatively to variations in short-term and long-term levels daily. Moreover, we can say that the orange (2018-05-17) and green (2018-02-05) curves show us a classic example of the so-called \emph{normal yield curve}, a concave curve with a positive slope.
\end{enumerate}

Let's know apply PCA on the matrix $X$ representing the de-meaned daily yield change of $p$ maturities on $N$ days.

\begin{ipython}
from finmarkets.machine_learning import PCAWrapper

pca = PCAWrapper(df)
pca.cps()
\end{ipython}

The evolution of the first three components is shown in Fig.~\ref{fig:pca_evolution_components}. It is clear that the variances of these components are in descending order: the PC1 (green) varies a lot, while PC3 (blue) fluctuates around zero.

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/pca_evolution_components}
	\caption{Evolution of in time of the first three principal components.}
	\label{fig:pca_evolution_components}
\end{figure}

Figure~\ref{fig:pca_explained_variance} indicates that almost 95\% of the variation in the data is described by the first principal component, and combined with the second and third components, it is almost 100\%, which means that all of the dynamics of the yield curve can be described by the first three components.

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/pca_explained_variance}
	\caption{(left) Variance explained by each principal component. (right) Cumulative explained variance.}
	\label{fig:pca_explained_variance}
\end{figure}

The matrix made of the components values is called the \emph{loading matrix} and represents $X$ in the new coordinates.
Let's check the loading values for the first three components as shown in Fig.~~\ref{fig:yield_pca_components}.
 
\begin{itemize}
\item PC1 (green): all loads are positive, implying that the PC1 factor causes the movement of the yield curve in the same direction. Therefore, this factor is responsible for the level change. This reflects that short-term yields tend to move more than long-term yields since the loads on the short end are larger. Moreover, this figure demonstrates that most of the variance is accumulated in PC1.
\item PC2 (orange): the loading increases from a negative value at the short end to a positive value at the long end. Furthermore, it crosses zero one time between two (2Y) and five (5Y) years bonds. This indicates that short-term yields and long-term yields tend to move in different directions, so it describes the change in slope of the curve. Note that a zero crossing point of the curve is an anchor point, and as we can see from the figure, it is around five years bonds.
\item PC3 (blue): the loadings from short to long-term cross zero twice, which means that the very short-term and long-term yields move in the same direction while the ”belly” part of the curve has the tendency to move in the opposite direction. This factor represents the curvature of the curve in a natural way.
\end{itemize}
	
\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/yield_pca_components}
	\caption{Components values after having performed PCA.}
	\label{fig:yield_pca_components}
\end{figure}

When the underlying factors have been found from PCA, we can finally use the first three principal components to approximate the original data $X$. As we observed above, almost 100\% of the variance can be explained by the first three components therefore, we will use those components to construct $\tilde{X}^{(3)}$.

Upper plots of Figures~\ref{fig:pca_dsg5},~\ref{pca_dsg10} and~\ref{pca_dsg30} shows the approximation of bond yield 5Y, 10Y and 30Y by all PCs in aggregate.
The corresponding residual $R^{(3)}$ are reported in the lower plots.

For 5Y rate it can be seen that even just PC1 is quite adequate to obtain a very close result with an error of the order of $10^{-3}$.

In the 10Y bond approximation: PC1 has the dominant explanatory power, its residuals are about $10^{-3}$. Adding PC2 improves the reconstruction so that the residuals are roughly $10^{-4}$, and including PC3, in addition, makes no appreciable difference. This
means that PC1 and PC2 are sufficient to provide an explanation of the dynamics.

Our last approximation is for 30Y bonds. In comparison to the 10Y curve, PC2 is more significant in explaining the 30Y trends. Moreover, in this case, the contribution of PC3 is much more substantial.

\begin{figure}[hp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/pca_dsg5}
	\caption{DGS5 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg5}
\end{figure}

\begin{figure}[hp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/pca_dsg10}
	\caption{DGS10 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg10}
\end{figure}

\begin{figure}[hp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/pca_dsg30}
	\caption{DGS30 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg30}
\end{figure}

%\subsection{Impact of Covid19}
%Using PCA analysis it can be assessed the impact of the Covid19 pandemic on different stocks, clustering together assets that were most/least affected without any prior knowledge of their fundamental.
%
%In this analysis we are going to use the equities forming the \emph{Dow Jones 30} index, so the first step is to download their returns with \texttt{yfinance} package (the data availability is from March 2019 to today).
%
%\begin{ipython}
%import yfinance as yf
%
%tickers = ["BA","CAT","CVX","CSCO","KO",
%	       "DOW","XOM","GS","HD","INTC","IBM",
%	       "JNJ","JPM","MCD","MRK","MSFT","NKE",
%	       "PFE","PG","RTX","TRV","UNH","VZ",
%	       "V","WBA", "WMT", "^DJI"]
%	
%proxy = yf.Tickers(tickers)           
%df = proxy.history(start='2020-01-02',
%                   end='2020-08-16')['Close'].pct_change().dropna()
%\end{ipython}
%
%After downloading data the PCA analysis is performed
%\begin{ipython}
%import numpy as np
%	
%equities = df.iloc[:, :-1]
%cov = equities.cov()
%eigVals, eigVecs = np.linalg.eig(cov)
%\end{ipython}
%
%Let's look at the first principal component and select the corresponding stocks that have the best and the worst PCA weights, as shown in Fig.~\ref{fig:pca_covid_neg}.
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=.6\textwidth]{figures/pc1_weights_covid}
%\caption{Ten most negative (top) and least negative (bottom) weights of the PC1 portfolio.}
%\label{fig:pca_covid_neg}
%\end{figure}
%
%Notice from Figure~\ref{fig:pca_covid_neg} how the most negative stocks are Boing and Chevron, connected to travel and energy sector. This makes sense since Covid19 heavily impacted the traveling business, as well as the energy companies that provide fuels for those business. On the other hand, the least impacted companies are Verizon, Wallmart, Pfizer and Johnson \& Johnson, which belong to sectors boosted during the quarantine measures and the pandemic in general.
%
%In addition we can check the winning portfolio that is long the top 3 companies according to the PCA weights.
%
%\begin{ipython}
%# normalize weights
%norm_eigVecs = abs(eigVecs.T[0])/sum(abs(eigVecs.T[0]))
%	
%# market portfolio
%pc_daily_ret = equities.dot(norm_eigVecs)
%pc_cum_ret = pc_daily_ret.cumsum()
%	
%# worst portfolio
%worst_idx = np.argsort(eigVecs.T[0])[:3]
%eq_worst = equities.iloc[:, worst_idx]
%worst_eigv = norm_eigVecs[worst_idx]
%worst_ret = eq_worst.dot(worst_eigv).cumsum()
%print ("Worst :", df.columns[worst_idx])
%	
%# best portfolio         
%best_idx = np.argsort(eigVecs.T[0])[-3:]
%eq_best = equities.iloc[:, best_idx]
%best_eigv = norm_eigVecs[best_idx]
%best_ret = eq_best.dot(best_eigv).cumsum()
%print ("Best :", df.columns[best_idx])
%\end{ipython}
%\begin{ioutput}
%Worst : Index(['BA', 'CVX', 'DOW'], dtype='object')
%Best : Index(['JNJ', 'VZ', 'WMT'], dtype='object')
%\end{ioutput}
%
%As shown in Figure~\ref{fig:pca_covid_return}, the resulting portfolio would have performed significantly better than the market, since it invested in companies that actually didn't suffer to much from the pandemic. 
%
%\begin{figure}[bhtp]
%\centering
%\includegraphics[width=.7\textwidth]{figures/pc1_weights_covid_returns}
%\caption{Three most negative and least negative weights portfolios compared to the market portfolio.}
%\label{fig:pca_covid_return}
%\end{figure}
%
%Note that this portfolio is formed with look-ahead bias, indeed portfolio weights are computed using future dates that were not available at the time of market downturn. The PCA used in this way is therefore a backward looking analytics tool.

%\section{Portfolio Optimization with PCA}
%\label{portfolio-optimization-and-pca}
%
%In Section~\ref{sec:pca} the Principal Component Analysis (PCA) has been introduced and here we are going to apply PCA on equity return covariance matrix to construct principal component portfolios because they have some interesting characteristics. This means that the portfolio weights will be based on the eigenvectors of the covariance matrix.
%
%From the theory we know that the first principal component (PC1) is a factor that captures the maximal amount of variance (i.e. the linear combination of assets that has highest possible variance). The second principal component factor (PC2) is the second most variable portfolio that is orthogonal to the first and so on.
%
%From the discussion of the CAPM model in Section~\ref{sec:capm} we learn that the market factor ($\beta$) is the primary driver of the stock market returns, as it tends to explain most of the returns of any given stock in any given day.
%So it is expected that applying PCA to daily stock returns, the first principal component approximates the market risk premium from the CAPM model.
%
%In this analysis we are going to use the equities forming the \emph{Dow Jones 30} index, so the first step is to download their returns with \texttt{yfinance} package (the data availability is from March 2019 to today).
%
%\begin{ipython}
%import yfinance as yf
%tickers = ["BA","CAT","CVX","CSCO","KO",
%	       "DOW","XOM","GS","HD","INTC","IBM",
%	       "JNJ","JPM","MCD","MRK","MSFT","NKE",
%	       "PFE","PG","RTX","TRV","UNH","VZ",
%	       "V","WBA", "WMT", "^DJI"]
%	
%proxy = yf.Tickers(tickers)           
%df = proxy.history(start='2014-03-27',
%	               end='2021-03-31')['Close'].pct_change().dropna()
%\end{ipython}
%
%Since the package returns already daily returns we can directly compute the covariance matrix from them. Also we determine eigenvalues and eigenvectors of this matrix.
%
%\begin{ipython}
%import numpy as np
%	
%equities = df.iloc[:, :-1]
%cov = equities.cov()
%eigVals, eigVecs = np.linalg.eig(cov)
%indices = [i for i in reversed(np.argsort(eigVals))]
%l = [eigVals[i]/np.*@sum@*(eigVals)*100 for i in    reversed(np.argsort(eigVals))]
%\end{ipython}
%
%Looking at the explained variance plot (see Fig.~\ref{fig:explained_variance}) we can notice how the first principal component explains almost 60\% then the value falls quickly down.
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=.7\textwidth]{figures/portfolio_pca_expl_var}
%\caption{Explained variance plot. Notice how the explained variance decreases steeply after the first component, and also how the returned components are not ordered by eigenvalue.}
%\label{fig:explained_variance}
%\end{figure}
%
%Next we can re-scale the PC's eigenvectors to sum up to 1 so they can be used as portfolio weights.
%
%\begin{ipython}
%norm_eigVecs = [v/np.linalg.norm(v) for v in eigVecs.T]
%\end{ipython}
%
%Finally we can construct a portfolio using the weights derived from PCs. In other words each asset takes a weight equal to its corresponding component in the PC eigenvector. Figure~\ref{fig:pca_weights} reports the weights of each asset in PC1 (the first principal component).
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=.7\textwidth]{figures/portfolio_pca_pc1_weights}
%\caption{Weights of the PC1 portfolio. Each asset is weighed according to the corresponding component in the first principal component eigenvector.}
%\label{fig:pca_weights}
%\end{figure}
%
%Then we calculate the cumulative return of the portfolio and compare it to the market return. This process is repeated for the first three components.
%
%\begin{ipython}
%for i, index in enumerate(indices[0:3]):
%    pc_daily_ret = equities.dot(norm_eigVecs[index])
%    pc_cum_ret = pc_daily_ret.cumsum()
%    market_cum_ret = df['^DJI'].cumsum()
%\end{ipython}
%
%From Fig.~\ref{fig:ret_pc1} to~\ref{fig:ret_pc3} it is clear how the PC1 portfolio return looks identical to the market. The second and third PC portfolio returns look quite different instead. It is not easy to interpret them though. 
%
%\begin{figure}[htbp]
%	\centering
%	\subfloat[Cumulative return of PC1 (left) compared to the market return (right).\label{fig:ret_pc1}]{%
%		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc1_vs_market}
%	}\\
%	\subfloat[Cumulative return of PC2 (left) compared to the market return (right).
%	\label{fig:ret_pc2}]{%
%		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc2_vs_market}
%	}\\
%	\subfloat[Cumulative return of PC3 (left) compared to the market return (right). 
%	\label{fig:ret_pc3}]{%
%		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc3_vs_market}
%	}
%	\caption{Comparison of cumulative return of various principle components portfolios to the market return.}
%	\label{fig:dummy2}
%\end{figure}
%
%It can be verified that PC1 portfolio still looks like the market also for other time horizon, repeating the same steps using weekly or monthly data.

%Regression Analysis represents a set of statistical methods and techniques, which we use to evaluate the relationship between variables. These are one dependent variable (our target) and one or more independent variables (predictors).
%
%We have three primary variants of regression — simple linear, multiple linear, and non-linear. However, most of the time, we use linear regression models. Non-linear models are helpful when working with more complex data, where variables impact each other in a non-linear way.
%
%Regression Analysis has many applications, and one of the most common is in financial analysis and modeling.
%
%In financial modeling, we can employ regression analysis to estimate the strength of the relationship between variables and subsequently forecast this relationship’s future behavior. It fits in any setting where we hypothesize there is (or not) a correlation between two or more variables.
\newpage
\section*{Exercises}
\input{linear_models_ex_text}

\begin{thebibliography}{9}
\bibitem{bib:bayesian_regression}W. Kohersen, \href{https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7}{\emph{Introduction to Bayesian Linear Regression}}, Toward Data Science [Online]
\bibitem{bib:fred}\href{https://fred.stlouisfed.org/}{\emph{FRED Economic data}}, 1991 [Online]
\bibitem{bib:capm_lintner} Lintner, J. \emph{“The Valuation of Risky Assets and the Selection of Risky Investments in Stock Portfolio and Capital Budgets}, 1965, Review of Economics and
Statistics, 47: 13-37.
\bibitem{bib:capm_sharpe} Sharpe, W. \emph{Capital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk}, 1964, Journal of Finance, 19: 425-442.
\bibitem{bib:fama_french} E. F. Fama, K. R. French \emph{Common risk factors in the returns on stocks and bonds}, Journal of Financial Economics, 1993, doi:10.1016/0304-405X(93)
\end{thebibliography}



