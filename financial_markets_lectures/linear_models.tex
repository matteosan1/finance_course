\chapter{Linear Models}
\label{linear-models}

\section{Simple Linear Regression Analysis}
\label{sec:linear-regression}
Regression Analysis is a form of predictive analysis. 
In particular the single (or simple) \emph{linear regression} model expresses the (linear) relationship between the dependent variable and one independent variable. The regression analysis attempts to find the strength of that relationship. For example this can be used to find the relation of a company’s performance to the industry performance or competitor business.

Regression models are expressed by an equation representing the dependent variable as a function of the independent variable. A simple linear model can be described by the following equation:
\begin{equation}
y = \beta x + \alpha + \epsilon
\label{eq:linear_regression}
\end{equation}
where $y$ is the dependent variable (target), $\alpha$ the intercept, $\beta$ the slope, $x$ the independent variable (predictor), and $\epsilon$ the residual (error).

The purpose is to estimate the underlying relationship so that we can predict the target variable based on the other (predictor).

\subsection{Ordinary Least Squares}
We need to solve a problem when running the regression model, and this is to fit a straight line to a set of pairs of observations of the dependent and independent variables. The line of best fit is where the sum of the squares of the vertical deviations (distances) between observation points and the line is at its minimum. This is the method of ordinary least squares (OLS) and the one we most commonly apply to a linear regression model.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/linear_regression}
	\caption{Example of linear regression.}
	\label{fig:linear_regression_example}
\end{figure}

The regression equation gives no exact prediction of the target value for any predictor variable. The regression coefficients we calculate from our sample data observations are only the best estimate of the real population variables.
This is why we introduce $\epsilon$ (residual/error) to the model, it covers the element of chance that an independent variable can experience variations.

\subsubsection{Covariance}
One of the measures we get from a regression analysis is the covariance

\begin{equation}
\text{Cov}(x, y) = \mathbb{E}[(x - \mathbb{E}[x])(y - \mathbb{E}[y])]
\end{equation}
where: $x$ and $y$ are the values of the independent and dependent variables at each observation.

The formula measures the direction of the relationship: if one variable is going up when the other is going down, then the covariance will be negative, and vice versa.

\subsubsection{Correlation}
That’s where correlation, another measure of regression analysis, comes in. It helps us to standardize the covariance to be able to better understand it.

Correlation takes the covariance and divides it over the product of the standard deviations of the variables. This makes sure we get a correlation coefficient between -1 and +1.
\begin{equation}
\corr(x, y) = \frac{\cov(x, y)}{\std(x)\std(y)}
\end{equation}

A correlation of +1 suggests the two variables are perfectly positively correlated, and a value of -1 suggests an entirely negative correlation.

From the definition of the $\beta$ coefficient (i.e. the slope of the linear regression) through least squares we have
\begin{equation}
\hat{\beta} = \corr(x, y)\cdot \frac{\std(y)}{\std(x)}
\end{equation}
therefore the two only coincide when $\std(y) = \std(x)$ That is, they only coincide when the two variables are, in some sense, on the same scale. 

Correlation and slope at some extent give the same information, i.e. they each tell the linear relationship strength between $x$ and $y$. But, they also do each give distinct information:
\begin{itemize}
\item the correlation gives you a bounded measurement that can be interpreted independently of the two variables scale. The closer the estimated correlation is to $\pm1$, the closer the two are to a perfect linear relationship. The regression slope, in isolation, does not tell you that piece of information.
\item the regression slope gives a useful quantity interpreted as the estimated change in the expected value of $y$ for a given value of $x$. Specifically, $\hat{\beta}$ tells you the change in the expected value of $y$ corresponding to a 1-unit increase in $x$. This information can not be deduced from the correlation coefficient alone.
\end{itemize}

\subsection{Multiple Linear Regression}
Simple regression is usually not enough in a real-life scenario, as targets (dependent variables) are rarely impacted only by a single predictor. The natural extention is given by the \emph{multiple linear regression} model which can have two or more independent variables.

The function to represent the regression equation now becomes:
\begin{equation}
y = \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \alpha + \epsilon
\label{eq:multiple_regression}
\end{equation}

It is crucial to keep in mind that the multiple regression model requires \emph{non-collinearity}. This means the independent variables should have a minimal correlation between them. Otherwise, it is difficult to assess the real relationship between the dependent (target) and the independent (predictors) variables.

%\subsection{Linear Regression Example}
%
%The Chinese Yuan (CNY) was pegged to the US Dollar (USD) prior to July 2005. Then, China announced that the exchange rate would be set with reference to a basket of other currencies, allowing for a movement of up to 0.3\% within any given day. The actual currencies and their basket weights are unannounced by China. Figure~\ref{fig:yuan_rate} reports the Yuan exchange rate to USD between 1999 and 2013.
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=0.7\textwidth]{figures/yuan_exchange_rate}
%\caption{Yuan exchange rate to USD from January 1999 to December 2013.}
%\label{fig:yuan_rate}
%\end{figure}
%
%From an empirical point of view, there are several important questions:
%\begin{itemize}
%\item for any given period, what is the implicit reference basket for the Chinese currency ?
%\item has the reference basket changed over time ?
%\item has the Chinese currency depreciated with respect to the dollar? If so, how much and when?
%\end{itemize}
%
%A possible approach for evaluating the implicit exchange rate regime of the Yuan involves the regression of the changes in the target currency on changes in the values of possible currencies in the reference basket.
%
%Consider the dataset \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/exchange_rates.csv}{exchange\_rates.csv} containing daily currencies exchange rates in the 1999 to 2013 period (data has been fetched from the Federal Reserve Archive~\cite{bib:fred}).
%
%To apply this methodology the original dollar-based exchange rates have been converted using the Swiss Franc. This allows currency moves of the dollar to be be used to explain moves in the Yuan. The choice of Swiss Franc is consistent with evaluations with respect to a stable currency. The dataframe already has columns with daily variations.
%
%\begin{ipython}
%import pandas as pd
%
%data = pd.read_csv("exchange_rates.csv", index_col="Date")
%print (data.head())
%\end{ipython}
%\begin{ioutput}
%            DEXCHUS  DEXJPUS  DEXKOUS  DEXMAUS  DEXUSEU  DEXUSUK  DEXTHUS  \
%Date                                                                        
%1999-01-05   8.2795   111.15   1166.0      3.8   1.1760   1.6566    36.18   
%1999-01-06   8.2795   112.78   1160.0      3.8   1.1636   1.6547    36.50   
%1999-01-07   8.2798   111.69   1151.0      3.8   1.1672   1.6495    36.30   
%1999-01-08   8.2796   111.52   1174.0      3.8   1.1554   1.6405    36.45   
%...
%                ret_THB_SFR      ret_USD_SFR  
%Date                                          
%1999-01-05        -0.002599        -0.002047  
%1999-01-06        -0.002666        -0.011472  
%1999-01-07        -0.006288        -0.000794  
%1999-01-08        -0.003565        -0.007689  
%
%[5 rows x 24 columns]
%\end{ioutput}
%
%%Figure~\ref{fig:rate_variation} shows CNY and USD daily variation with respect to Swiss Franc (SFR).
%%\begin{figure}[htbp]
%%\centering
%%\includegraphics[width=0.7\textwidth]{figures/log_variation_exch}
%%\caption{Daily variation of CNY and USD exchange rates from January 1999 to December 2013 with respect to Swiss Franc.}
%%\label{fig:rate_variation}
%%\end{figure}
%
%To implement the linear regression model it can be used the \texttt{statsmodel} package. It is necessary to define the $X$ vector (i.e. the independent exchange rate variations) and $y$ (i.e. the target Yuan rate to SFR). Then add a constant to the model which represents the rate variation not correlated to other currency variations (i.e. $\alpha$).
%
%First, we fit the regression model for the period prior to July 2005 when the Chinese currency was pegged to the US dollar. 
%
%\begin{ipython}
%import statsmodels.api as sm
%
%X = df.loc[df.index < '2005-06-30' ,
%           ['ret_YEN_SFR', 'ret_EUR_SFR', 
%            'ret_GBP_SFR', 'ret_USD_SFR']]
%y = df.loc[df.index < '2005-06-30' ,'ret_CNY_SFR']
%X = sm.add_constant(X)
%
%est = sm.OLS(y, X).fit()
%print(est.summary())
%\end{ipython} 
%\begin{ioutput}
%                            OLS Regression Results                            
%==============================================================================
%Dep. Variable:            ret_CNY_SFR   R-squared:                       1.000
%Model:                            OLS   Adj. R-squared:                  1.000
%Method:                 Least Squares   F-statistic:                 3.647e+06
%Date:                Tue, 08 Nov 2022   Prob (F-statistic):               0.00
%Time:                        13:46:53   Log-Likelihood:                 13749.
%No. Observations:                1692   AIC:                        -2.749e+04
%Df Residuals:                    1687   BIC:                        -2.746e+04
%Df Model:                           4                                         
%Covariance Type:            nonrobust                                         
%===============================================================================
%                  coef    std err          t      P>|t|      [0.025      0.975]
%-------------------------------------------------------------------------------
%const       -1.921e-07   1.74e-06     -0.110      0.912   -3.61e-06    3.23e-06
%ret_YEN_SFR    -0.0001      0.000     -0.477      0.633      -0.001       0.000
%ret_EUR_SFR    -0.0003      0.001     -0.379      0.705      -0.002       0.001
%ret_GBP_SFR    -0.0001      0.000     -0.224      0.823      -0.001       0.001
%ret_USD_SFR     1.0002      0.000   2545.320      0.000       0.999       1.001
%==============================================================================
%Omnibus:                      758.491   Durbin-Watson:                   2.824
%Prob(Omnibus):                  0.000   Jarque-Bera (JB):           889538.472
%Skew:                           0.456   Prob(JB):                         0.00
%Kurtosis:                     115.324   Cond. No.                         486.
%==============================================================================
%\end{ioutput} 
%
%The most interesting (for us) part of the summary is: 
%\begin{itemize}
%\item R-squared: the closer to 1 the higher is the linear correlation between $y$ and $X$;
%\item coef: the estimate of this model parameter (the weight assigned to this feature);
%\item std err: the standard error of our estimate. From these two values we can compute the t-score of our estimate:
%\begin{equation*}
%t = \frac{\textrm{coeff}}{\textrm{std err}}
%\end{equation*}
%which we can find on the third column ($t$); 
%\item t: this value essentially provides us with a metric of how small the error is with respect to the estimated value: the larger the t-score, the smaller the error and the more confident we can be in our estimate.
%\item P>|t|: to better quantify our confidence, it’s usual to compute the p-value associated to the t-score. Under relatively common assumptions, we expect the t-score to follow a standard distribution and the probability of obtaining results at least as extreme as the value observed is simply the area under the curve to the right of the t-score value. This area is known as the p-value.
%There are some nuances to interpreting p-values but briefly, the smaller the p-value, the stronger the evidence that the value we’re estimating is different than zero (if the coefficient of a given feature is indistinguishable from zero then that feature is not relevant for our model).
%
%Typical thresholds for the p-value are:
%\begin{itemize}
%	\item $p<0.05$: moderate evidence;
%	\item $p<0.01$: strong evidence;
%	\item $p<0.001$: very strong evidence.
%\end{itemize}
%\end{itemize}
%
%In our example R-squared is 1, and the only largely significant predictor is USD (p-value 0) confirming that indeed CNY was anchored to US dollar prior July 2005.
%
%Second, we fit the regression model for the first six months following the announcement of the change in currency policy.
%
%\begin{ipython}
%X = df.loc[(df.index > '2005-07-01') & (df.index < '2005-12-31'),
%           ['ret_YEN_SFR', 'ret_EUR_SFR', 
%            'ret_GBP_SFR', 'ret_USD_SFR',
%            'ret_WON_SFR', 'ret_MYR_SFR', 
%            'ret_THB_SFR']]
%y = df.loc[(df.index > '2005-07-01') & (df.index < '2005-12-31'),
%           'ret_CNY_SFR']
%
%X = sm.add_constant(X)
%est = sm.OLS(y, X).fit()
%print(est.summary())
%\end{ipython}
%\begin{ioutput}
%                            OLS Regression Results                            
%==============================================================================
%Dep. Variable:            ret_CNY_SFR   R-squared:                       0.949
%Model:                            OLS   Adj. R-squared:                  0.946
%Method:                 Least Squares   F-statistic:                     326.3
%Date:                Tue, 08 Nov 2022   Prob (F-statistic):           8.41e-76
%Time:                        13:53:28   Log-Likelihood:                 667.58
%No. Observations:                 130   AIC:                            -1319.
%Df Residuals:                     122   BIC:                            -1296.
%Df Model:                           7                                         
%Covariance Type:            nonrobust                                         
%===============================================================================
%                  coef    std err          t      P>|t|      [0.025      0.975]
%-------------------------------------------------------------------------------
%const          -0.0001      0.000     -0.920      0.359      -0.000       0.000
%ret_YEN_SFR    -0.0097      0.037     -0.262      0.794      -0.083       0.063
%ret_EUR_SFR     0.0583      0.092      0.636      0.526      -0.123       0.240
%ret_GBP_SFR    -0.0306      0.044     -0.688      0.493      -0.119       0.057
%ret_USD_SFR     0.2122      0.148      1.433      0.154      -0.081       0.505
%ret_WON_SFR     0.1790      0.036      5.041      0.000       0.109       0.249
%ret_MYR_SFR     0.7373      0.142      5.178      0.000       0.455       1.019
%ret_THB_SFR    -0.0655      0.059     -1.110      0.269      -0.182       0.051
%==============================================================================
%Omnibus:                      195.464   Durbin-Watson:                   2.331
%Prob(Omnibus):                  0.000   Jarque-Bera (JB):            15756.757
%Skew:                          -5.874   Prob(JB):                         0.00
%Kurtosis:                      55.639   Cond. No.                     1.57e+03
%==============================================================================
%\end{ioutput}
%
%R-squared is quite close to 1 so there is some correlation between $y$ and $X$.
%During this six-month period, there is evidence of the Yuan departing from a US Dollar peg. The exchange rates with the statistically significant regression parameters (lowest p-values) are for the Korean Won (WON) and the Malaysian Ringgit (MYR).
%
%Similar regressions can be performed to examine for further changes in the implicit reference basket from 2006 through 2013.
%
%Finally it is possible to measure the annualized trend in the Yuan exchange rate relative to the other currencies in the studied period. The annualization is performed on the $\alpha$ coefficient which is the only one not related to other currency variations (i.e. the idiosyncratic part of the rate variation).
%
%\begin{ipython}
%print (f"{(1+est.params[0])**126-1:.4f}")
%\end{ipython}
%\begin{ioutput}
%-0.0150
%\end{ioutput}
%\noindent
%So the Yuan depreciated in the studied period.

\section{Capital Asset Pricing Model}
\label{sec:capm}

The Capital Asset Pricing Model (CAPM) describes the relationship between asset expected returns and \emph{systematic risk} of the market. No measure of unsystematic risk appears in the risk premium for in the world of CAPM diversification has already eliminated it.

Sharpe~\cite{bib:capm_sharpe} and Lintner~\cite{bib:capm_lintner} developed the Capital Asset Pricing Model whose central insight is that the riskiness of an asset is not measured by the standard deviation of its return but by its beta. In particular, there is a linear relationship between the expected return of any security (or portfolio) and the expected return of the market portfolio. It is given by

\begin{equation}
r_i = r_f + \beta_i(r_m-r_f)
\label{eq:capm}
\end{equation}
where:
\begin{itemize}
\item $r_i$ is the expected return of the $i^{th}$ security;
\item $r_f$ is the risk-free rate with zero standard deviation (e.g. risk-free asset includes Treasury Bills as they are backed by the U.S. government);
\item $r_m - r_f$ is the risk premium, $r_m$ denotes the market return including all securities in the market, whose proxy can be an index like SP500;
\item $\beta_i$ is a measure of $i^{th}$ asset volatility in relation to the overall market. $\beta$ is used in the CAPM to describe the relationship between market risk, and expected return.
\end{itemize}

The relationship between risk ($\beta$) and expected return is called \emph{Security Market Line} (SML). An example of this line is shown in Fig.~\ref{fig:sml}.

In the freely competitive financial markets described by CAPM, no security can sell for long at prices low enough to yield more than its appropriate return on the SML (\emph{undervalued} asset). The security would then be very attractive compared with other securities of similar risk, and investors would bid its price up until its expected return fell to the appropriate position on the SML. Conversely, investors would sell off any stock selling at a price high enough to put its expected return below its appropriate position (\emph{overvalued} asset). The resulting reduction in price would continue until the stock’s expected return rose to the level justified by its systematic risk.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/sml}
	\caption{Security market line.}
	\label{fig:sml}
\end{figure}

The key point in CAPM is the determination of $\beta$. This can be achieved with the measurement of the \emph{regression line} slope, in the market vs individual stock return plot.

\subsection{Regression in CAPM}

The regressed coefficient estimates can be expressed as 

\begin{equation}
\beta \approx \cfrac{\text{Cov}(X,y)}{\text{Var}(X)}
\end{equation}

In the case of CAPM the line estimates the stock returns $y$ given the global market returns $X$ and so provides insights about how \emph{volatile}, or how risky, a stock is relative to the rest of the market.

In CAPM $\beta$ calculation is used to help investors understand whether a stock moves in the same direction as the rest of the market but for it to provide any useful clue, the market proxy should be related to the stock.

If $\beta$ of an individual stock = 1.0, means its price is perfectly correlated with the market, if $\beta < 1.0$, which is referred to as "defensive", indicates the security is theoretically less volatile than the market (provides lower returns, so it is less risky), while if $\beta > 1.0$, or "aggressive", indicates the assets price is more volatile than the market.

Those who use CAPM pick individual stocks or portfolios, and compare them to different indexes. The point is to find stocks that have high $\beta$, and portfolios that have high $\alpha$. High $\beta$ means the stock fares better than index with positive market and performs worse for negative market (contrary low $\beta$ gives lower performance for positive market and "better" returns in negative market), so those stocks have a chance at beating the market. $\alpha$ values above zero mean that a portfolio outperforms the market whatever it does.

\subsection{CAPM Example}
\label{sec:linear_regression}

Let's apply CAPM model to a couple of securities, the market proxy is the SP500 index while the risk free rate is approximated by the 3 months Treasury rate.
Input data can be downloaded with \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/capm.csv}{capm.csv}.

Consider the General Electrics (GE) stock and apply CAPM to the series of its returns. Figure~\ref{fig:ge_returns} reports the historical series of closing price for GE and SP500.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_ge}
\caption{Historical series of closing price for General Electrics stock (left) and SP500 (right).}
\label{fig:ge_returns}
\end{figure}

To implement the linear regression model it can be used the \texttt{statsmodel} package. It is necessary to define the $X$ vector (i.e. the independent exchange rate variations) and $y$ (i.e. the target Yuan rate to SFR). Then add a constant to the model which represents the rate variation not correlated to other currency variations (i.e. $\alpha$).

So to determine $\beta$ and $\alpha$ for GE stock the model is made of its excess of returns ($y$) and market excess of returns ($X)$ 

\pythoncodenon{code/linear_models_capm.py}

\begin{ioutput}
OLS Regression Results                                
===========================================================================
Dep. Variable:                 ret_GE   R-squared :                   0.572
Model:                            OLS   Adj. R-squared :              0.572
Method:                 Least Squares   F-statistic:                  4469.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):            0.00
Time:                        08:51:58   Log-Likelihood:              9655.8
No. Observations:                3340   AIC:                     -1.931e+04
Df Residuals:                    3339   BIC:                     -1.930e+04
Df Model:                           1                                                  
Covariance Type:            nonrobust                                                  
===========================================================================
coef          std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------
ret_SP500      1.1867      0.018     66.854      0.000       1.152    1.222
===========================================================================
Omnibus:                      900.145   Durbin-Watson:                1.973
Prob(Omnibus):                  0.000   Jarque-Bera (JB):         66833.895
Skew:                           0.275   Prob(JB):                      0.00
Kurtosis:                      24.908   Cond. No.                      1.00
===========================================================================
\end{ioutput}

The most interesting (for us) part of the summary is: 
\begin{itemize}
\item R-squared: the closer to 1 the higher is the linear correlation between $y$ and $X$;
\item coef: the estimate of this model parameter (the weight assigned to this feature);
\item std err: the standard error of our estimate. From these two values we can compute the t-score of our estimate:
\begin{equation*}
t = \frac{\text{coeff}}{\text{std err}}
\end{equation*}
which we can find on the third column ($t$); 
\item t: this value essentially provides us with a metric of how small the error is with respect to the estimated value: the larger the t-score, the smaller the error and the more confident we can be in our estimate.
\item P>|t|: to better quantify our confidence, it’s usual to compute the p-value associated to the t-score. Under relatively common assumptions, we expect the t-score to follow a standard distribution and the probability of obtaining results at least as extreme as the value observed is simply the area under the curve to the right of the t-score value. This area is known as the p-value.
There are some nuances to interpreting p-values but briefly, the smaller the p-value, the stronger the evidence that the value we’re estimating is different than zero (if the coefficient of a given feature is indistinguishable from zero then that feature is not relevant for our model).
	
Typical thresholds for the p-value are:
\begin{itemize}
	\item $p<0.05$: moderate evidence;
	\item $p<0.01$: strong evidence;
	\item $p<0.001$: very strong evidence.
\end{itemize}
\end{itemize}

From the summary results that $\beta$ is statistically significant and equal to 1.187. Fig.~\ref{fig:capm_fit} reports the dataset and the linear regression fit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_fit}
\caption{Linear regression of market return vs stock return for GE security.}
\label{fig:capm_fit}
\end{figure}

If you imagine to have a portfolio, and that we have the $\beta$s of each individual stock to apply CAPM it is enough to perform a weighted sum of the expected return according to the model of each stock.

\subsection{Criticism to CAPM}
As we have seen the whole model is about plotting a line in a scatter plot, it’s not a very complex model. Assumptions under the model are even more simplistic. For example:
\begin{itemize}
\tightlist
\item expect that all investors are rational and they avoid risk;
\item everyone have full information about the market;
\item everyone have similar investment horizons and expectations about future movements;
\item stocks are all correctly priced.
\end{itemize}

Moreover, this is a model from the 1960s. Market dynamics were different back then. And of course, this is a retrospective model. We cannot know how future stock prices move and how the market behaves.
Interesting extension of CAPM involves \emph{Bayesian regression}\cite{bib:bayesian_regression} but it will not be discussed here.

\section{Multifactor Models}

In its original formulation the Capital Asset Pricing Model (CAPM) treats the market return as the only factor. Nevertheless a stock’s return can depend also on other macro-economic factors, such commodity prices, interest rates, economic growth (GDP). In this case we talk about \emph{multifactor models} which can be used for: return forecasting, risk modeling, transaction cost analysis, and performance attribution. 
By decomposing asset returns into different components (\emph{factors}), allows a better understanding of the sources of portfolio return and risk. 

Factor models are based on the idea that security returns consist of two components:
\begin{itemize}
\tightlist
\item the part that is driven by a set of common factors;
\item and the part unexplained by these factors hence idiosyncratic in nature. 
\end{itemize}
\begin{equation}
r_{t} = \sum_{k=1}^{K} X_{kt} f_{kt} + \epsilon_{t} = \mathbf{Xf} + \boldsymbol{\epsilon}
\label{eq:multifactor}
\end{equation}
where: $r_{t}$ is the security return over period $t$, $X_{kt}$ its \emph{exposure} to k$^{th}$ factor at the beginning of period $t$, $f_{kt}$ is the \emph{factor return} to k$^{th}$ factor over period $t$ and $\epsilon_{t}$ is the residual return over period $t$ (i.e. there are $K$ factors, and $T$ time periods).

\subsection{Types of Factor Models}

In practice, there are three common classes of factor models depending on the statistical techniques used to estimate factor returns and/or factor exposures.

\begin{center}
\scriptsize
\begin{tabularx}{\linewidth}{>{\hsize=.3\hsize}X>{\hsize=.4\hsize}X>{\hsize=.4\hsize}X>{\hsize=.5\hsize}X>{\arraybackslash}X}
Type & Inputs & Outputs & Estimation Method & \textcolor{ansi-green}{Adv.}/\textcolor{ansi-red}{Disadv.} \\
\toprule
Explicit & security returns, factor returns & factor exposures &\begin{tabular}[t]{@{}l@{}}time-series\\ regression\end{tabular}
& \textcolor{ansi-green}{Accommodates any time series as factors, linked to macroeconomic fundamentals.}
\textcolor{ansi-red}{Possible non-intuitive factor exposures.} \\
\midrule
Implicit & security returns, factor exposures & factor returns &\begin{tabular}[t]{@{}l@{}}cross-sectional\\ regression\end{tabular} & \textcolor{ansi-green}{Intuitive exposures linked to asset characteristics, incorporates security fundamental data, responsive to asset characteristics changes.} \textcolor{ansi-red}{Data intensive.} \\
\midrule
Statistical & security returns, factor exposures & factor returns &\begin{tabular}[t]{@{}l@{}}principal component\\analysis\end{tabular} & \textcolor{ansi-green}{Requires only asset-level price data, might capture new risk sources, relatively easy to build.} \textcolor{ansi-red}{Lack of interpretability.} \\
\end{tabularx}
\end{center}
\normalsize

\subsection{Risk Decomposition and Estimation with Factor Models}

Once factor exposures are calculated, one can carry out risk decomposition and forecast at both the individual security and the aggregate portfolio levels.

Let $R_P$ denote the return of some portfolio $P$ over period $t$
\begin{equation}
R_P = \mathbf{w}_P^T \mathbf{r}
\label{eq:portfolio_return}
\end{equation}
where $\mathbf{w}_P$ are the portfolio weights and $\mathbf{r}$ denotes the vector of security returns.

Substituting~\ref{eq:multifactor} into Eq.~\ref{eq:portfolio_return} gives
\begin{equation}
R_P = \mathbf{w}^T_P \mathbf{Xf} + \mathbf{w}^T_P\boldsymbol{\epsilon}
\end{equation}
where $\mathbf{X}$ is a $N\times K$ matrix specifying each security’s exposures to each model factors ($\mathbf{f}$). 

The risk $\sigma_P$ of the portfolio can be decomposed into the factor and the idiosyncratic components: 
\begin{equation}
\sigma^2_P = \mathbf{w}_P^T\mathbf{\Omega}\mathbf{w}_P = \mathbf{w}_P^T \mathbf{XFX}^T \mathbf{w}_P + \mathbf{w}_P^T \mathbf{\Delta} \mathbf{w}_P
\end{equation}

$\mathbf{\Omega}$ represents the $N \times N$ asset covariance matrix that without a factor model the calculation of $\mathbf{\Omega}$ would involve computing the pairwise sample covariance for each element in the matrix (e.g. $N$ is usually very large, hundreds of assets, relative to the number of time periods $T$ of the return time series, which makes the estimated covariance matrix unstable and ill-conditioned).

The factor model framework allows to reduce the dimensionality of the problem significantly. In this approach, we can express the asset covariance matrix in terms of factor model ingredients:
\begin{equation}
\mathbf{\Omega} = \mathbf{XFX}^T + \mathbf{\Delta}
\end{equation}
where $\mathbf{X}$ is the factor exposure matrix, $\mathbf{F}$ is the $K\times K$ factor covariance matrix, and $\mathbf{\Delta}$ is the $N\times N$ idiosyncratic covariance matrix (which is usually assumed to be diagonal). 

\subsubsection{Example}

Consider the example of previous Section and improve the model by adding the crude oil price as a second factor. Perform again the linear regression

\pythoncodenon{code/linear_models_multifactor.py}

\begin{ioutput}
OLS Regression Results                                
==============================================================================
Dep. Variable:                 ret_GE   R-squared (uncentered):          0.574
Model:                            OLS   Adj. R-squared (uncentered):     0.574
Method:                 Least Squares   F-statistic:                     2251.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):               0.00
Time:                        09:05:11   Log-Likelihood:                 9662.9
No. Observations:                3340   AIC:                        -1.932e+04
Df Residuals:                    3338   BIC:                        -1.931e+04
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
coef          std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ret_SP500      1.2004      0.018     66.376      0.000       1.165       1.236
ret_Brent     -0.0371      0.010     -3.768      0.000      -0.056      -0.018
==============================================================================
Omnibus:                      878.756   Durbin-Watson:                   1.974
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63119.110
Skew:                           0.231   Prob(JB):                         0.00
Kurtosis:                      24.292   Cond. No.                         1.90
==============================================================================
\end{ioutput}

The regression coefficient for the oil factor (ret\_Brent) is statistically significant and negative. Over the analysis period, price changes in GE stock are negatively related to the price changes in oil.
Let's apply the same model now to Exxon (XOM) stock.

\begin{ioutput}
OLS Regression Results                                
==============================================================================
Dep. Variable:                ret_XOM   R-squared (uncentered):          0.549
Model:                            OLS   Adj. R-squared (uncentered):     0.549
Method:                 Least Squares   F-statistic:                     2031.
Date:                Thu, 17 Nov 2022   Prob (F-statistic):               0.00
Time:                        09:07:25   Log-Likelihood:                 10402.
No. Observations:                3340   AIC:                        -2.080e+04
Df Residuals:                    3338   BIC:                        -2.079e+04
Df Model:                           2                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
coef          std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
ret_SP500      0.8129      0.014     56.082      0.000       0.785       0.841
ret_Brent      0.1453      0.008     18.391      0.000       0.130       0.161
==============================================================================
Omnibus:                      477.069   Durbin-Watson:                   2.075
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6852.309
Skew:                           0.036   Prob(JB):                         0.00
Kurtosis:                      10.017   Cond. No.                         1.90
==============================================================================
\end{ioutput}

The R-squared for XOM is slightly lower than for GE. Its relationship to the market index is less strong (lower t value).
The regression coefficient for the oil factor (ret\_Brent) is statistically significant and, unlike GE, positive.

\subsection{Fama-French Three-Factor Model}
This model was proposed in 1993 by Eugene Fama and Kenneth French to describe stock returns~\cite{bib:fama_french}. The three-factor model is
\begin{equation}
r=\alpha + \beta_m \textrm{MKT} +\beta_s \textrm{SMB}+ \beta_h \textrm{HML}
\end{equation}
where
\begin{itemize}
\tightlist
\item MKT: is the excess return of the market (e.g. the value-weighted return of all firms listed on the NYSE, AMEX, or NASDAQ minus the 1-month Treasury Bill rate);
\item SMB (Small Minus Big): measures the excess return of stocks with small market cap over those with larger market cap;
\item HML (High Minus Low): measures the excess return of value stocks over growth stocks. Value stocks have high book to price ratio (B/P) than growth stocks.
\end{itemize}

This setting represents the original Fama-French model. This very same approach has been also extended with the Fama-French Five-Factor model which comprises two more factors:
\begin{itemize}
\tightlist
\item RMW (Robust Minus Weak): measures the excess returns of firms with high operating profit margins over those with lower profits;
\item CMA (Conservative Minus Aggressive): measures the excess returns of firms investing less over those investing more.
\end{itemize}
Finally, momentum is another commonly used factor. It captures excess returns of stocks with highest returns over those with lowest returns.

The application of such models goes along the lines of the CAPM with the only difference being the calculation of a multidimensional regression.

%* The Fama and French model has three factors: 
%* the size of firms (**SMB**, small minus big), 
%* book-to-market values (**HML**, high minus low), 
%* **excess return on the market** (portfolio's return less the risk-free rate of return). 
%
%* SMB accounts for publicly traded companies with small market caps that generate higher returns, while HML accounts for value stocks with high book-to-market ratios that generate higher returns in comparison to the market.
%
%* Represent each characteristic of interest ($f_k(t)$) by a portfolio: 
%1. sort all assets by that characteristic;
%2. build the portfolio by going long the top quantile of assets and short the bottom quantile. 
%
%* **The factor corresponding to this characteristic is the return on this portfolio**. 
%* The $X_{nk}(t)$ are estimated for each asset $n$ by regressing over the historical values of $r_n(t)$ and of the factors.

\section{Principal Components Analysis}
\label{sec:pca}

\emph{Principal Component Analysis} (PCA), is a dimensionality-reduction method that is often used to reduce the complexity of large datasets, by transforming the original set of variables into a smaller one that still contains most of the information of the large set.

Reducing the number of variables of a dataset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. 

So, to sum up, the idea of PCA is simple: reduce the number of variables of a dataset, while preserving as much information as possible.

\subsection{Principal Components}

Principal components are new variables that are constructed as linear combinations or mixtures of the original ones. These combinations are done in such a way that the new variables (i.e. principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. 

So, the idea is that a $n$-dimensional dataset gives you $n$ principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.

Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, by discarding the components with low information and considering the remaining components as your new variables.

\emph{An important thing to realize here is that the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables.}

Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data. The relationship between variance and information here, is that, the larger the variance carried by a line, the larger the dispersion of the data points along it, and the larger the dispersion along a line, the more the information it has.
%To put all this simply, just think of principal components as new axes that provide the best angle to see and evaluate the data, so that the differences between the observations are better visible.

\subsection{Step-by-Step Explanation}

Let's see how this kind of technique can be implemented starting from a multidimensional initial dataset 
\begin{equation}
	X=\begin{bmatrix}
		x^{(1)}_1 &x^{(2)}_1&\cdots &x^{(m)}_1 \\
		x^{(1)}_2 &x^{(2)}_2&\cdots &x^{(m)}_2 \\
		\vdots &\vdots &\vdots &\vdots \\
		x^{(1)}_n &x^{(2)}_n&\cdots &x^{(m)}_n 
	\end{bmatrix}
\end{equation}

For the sake of simplicity let's imagine that our sample is the two-dimensional one represented in the left graph of Fig.~\ref{fig:pca_dataset}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/pca_raw_std_data.png}
	\caption{The dataset used in the PCA example (left), and the same dataset after \emph{normalization} (right).}
	\label{fig:pca_dataset}
\end{figure}

\subsubsection*{Step 1: Standardization}

The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.

More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (for example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.

Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable
\begin{equation*}
x' = \cfrac{x-\bar{x}}{\sigma_x}
\end{equation*}

Once the standardization is done, all the variables will be transformed to the same scale. Right plot of Fig.~\ref{fig:pca_dataset} shows the standardized dataset used in the following.

\subsubsection*{Step 2: Covariance Matrix Computation}

The aim of this step is to understand how the variables of the input dataset are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. 

The covariance matrix is a symmetric $n\times n$ (where $n$ is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables. %Since the covariance of a variable with itself is its variance ($\textrm{cov}(a, a)= \textrm{var}(a)$), in the main diagonal (top-left to bottom-right) we actually have the variances of each initial variable. And since the covariance is commutative ($\textrm{cov}(a, b) = \textrm{cov}(b, a)$), the entries of the covariance matrix are symmetric with respect to diagonal.
Once we have centred our data around 0, $[X^T][X]$ is the covariance matrix.

\begin{equation}
	[\Sigma]=[X^T][X] =
	\begin{bmatrix}
		\var(x_1) & \cov(x_2, x_1) & \cdots & \cov(x_n, x_1) \\
		\cov(x_1, x_2) & \var(x_2) & \cdots & \cov(x_n, x_2) \\
		\vdots & \vdots & \vdots & \vdots \\
		\cov(x_1, x_n) & \cov(x_2, x_n) & \cdots & \var(x_n)
	\end{bmatrix}
\end{equation}

What do the covariances that we have as entries of the matrix tell us about the correlations between the variables ?
It’s actually the sign of the covariance that matters:
\begin{itemize}
	\item if positive then: the two variables increase or decrease together (correlated);
	\item if negative then: one increases when the other decreases (Inversely correlated).
\end{itemize}

\subsubsection*{Step 3: Compute the Eigenvectors and Eigenvalues of the Covariance Matrix to Identify the Principal Components}

It is possible to demonstrate that the eigenvectors of the covariance matrix are actually the directions of the axis where there is the most variance (i.e. most information) and that we call \emph{principal components}. Eigenvalues are simply the coefficients, attached to eigenvectors, which give the amount of variance carried in each principal component (see Sec.~\ref{eigenvectors-and-eigenvalues} for a description of eigenvectors-eigenvalues).

For our example the eigenvectors are defined as
\begin{ioutput}
        PC1       PC2
x  0.707107  0.707107
y  0.707107 -0.707107
\end{ioutput}
with the following eigenvalues (normalized to 1)
\begin{ioutput}
array([0.99662716, 0.00337284])
\end{ioutput}

According to these results PC1 and PC2 carry respectively 99.7\% and 0.3\% of the variance of the data.

\subsubsection*{Step 4: Create a Feature Vector}

In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call \emph{feature vector}.

So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only $p$ eigenvectors (components) out of $n$, the final data set will have only $p$ dimensions.

Continuing with the example from the previous step, we can either form a feature vector with both of the eigenvectors $v1$ and $v2$, or discard the eigenvector $v_2$, which is the one of lesser significance, and form a feature vector with $v_1$ only.

Discarding the eigenvector $v_1$ will reduce dimensionality by 1, and will consequently cause a loss of information in the final data set. But given that $v_1$ was carrying only 0.3\% of the information, the loss will be therefore not important and we will still have 99.7\% of the information that is carried by $v_2$.

Figure~\ref{fig:pca_result} shows the two principal components.

\begin{figure}[hbt]
\centering
\includegraphics[width=0.5\linewidth]{figures/pca_pcs}
\caption{The directions corresponding to the first two principal components of our example dataset.}
\label{fig:pca_result}
\end{figure}

\subsubsection*{Step 5: Recast Data Along Principal Components}

At this point we can use the feature vector to re-orient data from the original axis to the ones represented by the principal components. 
This can be done by multiplying the transpose of the original dataset by the transpose of the feature vector

\begin{equation}
[Y] = [X^T][F^T]
\end{equation}

The rotation transformed our dataset that have now the more variance on one of the Cartesian axis. 
You could keep only this dimension and have a fairly good representation of the data. Figure~\ref{fig:pca_rotated} shows the effect of such a rotation to our sample.
%\clearpage
\begin{figure}[hbtp]
\centering
\includegraphics[width=0.5\linewidth]{figures/pca_projected_data}
\caption{The dataset rotated so that the usual Cartesian coordinates correspond to the first two principal components.}
\label{fig:pca_rotated}
\end{figure}

\subsection{Practical problems}
PCA is not invariant to scaling. Before applying PCA, we should consider whether or not to standardize the data $X$, besides removing of the mean from the data before applying PCA. In practice, it depends on the context of the data and the objective, but there are two general tips regarding this problem:
\begin{enumerate}
	\item If features are measured in different units that are not comparable, we should first standardize them before applying PCA.
	\item If features are measured in comparable units, then we can keep them unscaled to preserve the original variability information.
\end{enumerate}

Furthermore PCA can not be used if the data is not Gaussian. PCA is based on the assumption that the data is normally distributed, that is why we can represent all the information from the covariance. In the case of non-Gaussian data, independent component analysis (ICA) is often used.

\begin{finmarkets}
In the previous example and in the following the \texttt{PCAWrapper} class of the \texttt{finmarkets} module has been used. 
This is a simple wrapper which implements the PCA algorithm and has few utility methods to get relevant information, e.g. extract components and project the data along the new axis.
\end{finmarkets}

\pythoncodenon{code/linear_models_pca.py}

\subsection{Term Structure Analysis Using PCA}

A bond’s yield is the interest income received by an investor from investing in debt securities. Bonds with a fixed coupon have an interest rate that accrues on the face value.
Current yield is the ratio of annual coupon payments to the current market value of the bond:
\begin{equation}
\text{Current Yield} = \cfrac{\text{Annual Coupon Payment}}{\text{Bond Price}}
\end{equation}

The yield curve represents the time structure of interest rates and shows the relationship between the yields of financial instruments and their maturity.

The Treasuries bond yield curve is not only used as a benchmark to assess the value of other debt instruments but is also considered one of the most important economic indicators to watch as a precursor to a future cyclical downturn in the US economy.

Usually, yields of longer-dated securities are higher than those of shorter-dated securities. Such a yield curve is considered \emph{normal} because the market usually requires more compensation for more risk under normal conditions. Long-term bonds are subject to greater risks, such as changes in interest rates and increased potential default risk.
However, the short-term yield sometimes exceeds the long-term yield, and the spread turns negative. The result is a \emph{concave} curve.

The \emph{inversion} (concavity) of the Treasuries curve in the US is considered one of the proxies for an imminent recession or downturn in the economy.

In the following example, we analyse the data of Treasury bonds between $1^{st}$ January 2017 and $31^{st}$ December 2018 taken using the Federal Reserve Bank of St. Louis API.
The US economy has been growing steadily in the period under review since after a rebound in 2016, it grew by 3.1\% in 2018 after climbing by 2.5\% in 2017.

The dateset is available \href{https://github.com/matteosan1/finance_course/raw/develop/input_files/DGS_2017_2018.csv"}{here}.

\subsection{Factor Model}
Let $X$ be the yield curve consisting of bonds of $p$ maturities with $N$ observations. We will model $X$ as a $k$ factor model.

The $k$ factor model for the yield curve $X$ can then be presented in the following way

\begin{equation}
X = \mu_ X + Zf + e
\end{equation}
\noindent
where $\mu_X = (\mu_1,\ldots, \mu_p)$ is the mean vector of $X$, $e = (e_1,\ldots, e_N )$ is the residuals and
\begin{equation}
Z =
\begin{bmatrix}
	z_{1,1} & z_{1,2} & \cdots & z_{1,k} \\
	z_{2,1} & z_{2,2} & \cdots & z_{2,k} \\
	\vdots & \vdots & \ddots & \vdots \\
	z_{N,1} & z_{N,2} & \cdots & z_{N,k} \\
\end{bmatrix}
\end{equation}
consist of $k$ factors,
\begin{equation}
f =
\begin{bmatrix}
	f_{1,1} & f_{1,2} & \cdots & f_{1,p} \\
	f_{2,1} & f_{2,2} & \cdots & f_{2,p} \\
	\vdots & \vdots & \ddots & \vdots \\
	f_{k,1} & f_{k,2} & \cdots & f_{k,p} \\
\end{bmatrix}
\end{equation}
the factor loading matrix; i.e., the observation $i$ of feature $j$ is

\begin{equation}
x_{i,j} = \mu_j + \sum_{l=1}^k z_{i,l}f_{l,j} + e_{i,j}
\end{equation}

The difference with the previous studies is that instead of identifying external factors, we will focus on self-contained independent drivers, i.e., factors related to the curve itself, such as slope and curvature.

Through this approach, we will recognize the key structure of the yield curve dynamics and will be able to answer how 2Y, 10Y, and 30Y bond yields move together. Moreover, PCA will help us to naturally reduce the dimensionality of the factors based on the coefficient of explained variance.

\subsection{PCA on Term Structure}

The yield/rate time series of different maturities are shown in the top plot of Fig.~\ref{fig:yield_rate_ts}. It is clear that all curves are moving in the same direction, which indicates the idea that some common factors drive the yield dynamic.

In bottom Fig.~\ref{fig:yield_rate_ts} we can see the yield curve term structure on different dates. In contrast to the previous figure, here one can distinctly see the inconsistencies in the dynamics and it is obvious that in some periods the movement is quite the opposite.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/treasury_rate}
	\caption{Yield rate time series for different maturity Treasury bonds (top). Yield rate at different dates (bottom).}
	\label{fig:yield_rate_ts}
\end{figure}

Let us consider in more detail the different types of changes between the dates:
\begin{enumerate}
\item Level change: modifies the interest rates of all maturities in nearly equal amounts, inducing a parallel shift that changes the level of the entire yield curve. An example of level change can be seen by observing the green (2018-02-05) and orange (2018-05-17) curves. Both are moving in the same direction with almost identical slopes, and the only difference is the level: the green curve is shifted compared to the orange one.
\item Slope change: increases short-term interest rates by much more than long-term interest rates so that the yield curve becomes less abrupt and its slope decreases.
For example, from 2018-05-17 (orange) to 2018-12-10 (pink), the slope of the curve changed as the short-term yield increased while the long-term yield decreased (DGS2 to DGS5).
\item Curvature change: reflects how the difference between the medium-long-term premium (i.e., the long-term minus the medium-term rate level) and the short-medium-term premium (i.e., the medium-term minus the short-term rate level) changes from each day. In other words, the ”belly” of the curve shifts relatively to variations in short-term and long-term levels daily. Moreover, we can say that the orange (2018-05-17) and green (2018-02-05) curves show us a classic example of the so-called \emph{normal yield curve}, a concave curve with a positive slope.
\end{enumerate}

Let's know apply PCA on the matrix $X$ representing the de-meaned daily yield change of $p$ maturities on $N$ days.

\pythoncodenon{code/linear_models_pca_cps.py}

Figure~\ref{fig:pca_explained_variance} indicates that almost 96\% of the variation in the data is described by the first principal component, and combined with the second and third components, it is almost 100\%, which means that all of the dynamics of the yield curve can be described by the first three components.

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=\linewidth]{figures/pca_explained_variance}
	\caption{(left) Variance explained by each principal component. (right) Cumulative explained variance.}
	\label{fig:pca_explained_variance}
\end{figure}

The matrix made of the components values is called the \emph{loading matrix} and represents $X$ in the new coordinates.

\begin{ioutput}
          PC1     PC2     PC3     PC4     PC5     PC6     PC7
DGS1   0.5531  0.5219  0.4068 -0.5032  0.0142 -0.0498 -0.0073
DGS2   0.5484  0.2293 -0.0587  0.7846 -0.1420  0.0332  0.0785
DGS5   0.4014 -0.1873 -0.4582 -0.1051  0.6647  0.0586 -0.3706
DGS7   0.3307 -0.3223 -0.3483 -0.2636 -0.1468 -0.0413  0.7566
DGS10  0.2792 -0.3737 -0.1011 -0.1424 -0.6269 -0.3044 -0.5157
DGS20  0.1804 -0.4297  0.3907 -0.0105 -0.0624  0.7893 -0.0546
DGS30  0.1103 -0.4600  0.5802  0.1733  0.3451 -0.5249  0.1215
\end{ioutput}

Let's check the loading values for the first three components as shown in Fig.~~\ref{fig:yield_pca_components}.
 
\begin{itemize}
\item PC1 (green): all loads are positive, implying that the PC1 factor causes the movement of the yield curve in the same direction. Therefore, this factor is responsible for the level change. This reflects that short-term yields tend to move more than long-term yields since the loads on the short end are larger. Moreover, this figure demonstrates that most of the variance is accumulated in PC1.
\item PC2 (orange): the loading decreases from a positive value at the short end to a negative value at the long end. Furthermore, it crosses zero one time between two (2Y) and five (5Y) years bonds (this crossing point is said \emph{anchor point}). This indicates that short-term yields and long-term yields tend to move in different directions, so it describes the change in slope of the curve.
\item PC3 (blue): the loadings from short to long-term cross zero twice, which means that the very short-term and long-term yields move in the same direction while the ”belly” part of the curve has the tendency to move in the opposite direction. This factor represents the curvature of the curve in a natural way.
\end{itemize}
	
\begin{figure}[hbtp]
	\centering
	\includegraphics[width=0.45\linewidth]{figures/pca_loading_matrix}
	\caption{Components values after having performed PCA.}
	\label{fig:yield_pca_components}
\end{figure}

When the underlying factors have been found from PCA, we can finally use few principal components to approximate the original data $X$. As we observed above, almost 100\% of the variance can be explained by the first three components therefore, we will use those components to construct $\tilde{X}^{(3)}$.

Upper plots of Figures~\ref{fig:pca_dsg5},~\ref{fig:pca_dsg10} and~\ref{fig:pca_dsg30} shows the approximation of bond yield 5Y, 10Y and 30Y by all PCs in aggregate.
The corresponding residual $R^{(3)}$ are reported in the lower plots.

For 5Y rate it can be seen that even just PC1 is quite adequate to obtain a very close result with an error of the order of $10^{-3}$. In the 10Y bond approximation: PC1 has the dominant explanatory power, its residuals are about $10^{-3}$. Adding PC2 improves the reconstruction so that the residuals are roughly $10^{-4}$, and including PC3, in addition, makes no appreciable difference. This means that PC1 and PC2 are sufficient to provide an explanation of the dynamics. Our last approximation is for 30Y bonds. In comparison to the 10Y curve, PC2 is more significant in explaining the 30Y trends. Moreover, in this case, the contribution of PC3 is much more substantial.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/dgs5_reco}
	\caption{DGS5 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg5}
\end{figure}

\begin{figure}[hp]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/dgs10_reco}
	\caption{DGS10 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg10}
\end{figure}

\begin{figure}[hp]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/dgs30_reco}
	\caption{DGS30 reconstruction by PCs (top). Residuals with original data by components (down).}
	\label{fig:pca_dsg30}
\end{figure}

\newpage
\section*{Exercises}
\input{linear_models_ex_text}
