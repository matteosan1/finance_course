\chapter{Matrices}
\label{sec:matrices}

In mathematics, a matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. Matrices are commonly written in box brackets. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively. The size of a matrix is defined by the number of rows and columns that it contains. A matrix with $m$ rows and $n$ columns is called an $m\times n$ matrix or $m$-by-$n$ matrix, while $m$ and $n$ are called its dimensions. The dimensions of the following matrix are $2\times 3$ (read "two by three"), because there are two rows and three columns.

\begin{equation*}
\begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6
\end{bmatrix}
\end{equation*}

The individual items (numbers, symbols or expressions) in a matrix are called its elements or entries. 
In \texttt{python} matrices can be represented as \texttt{numpy.array}, so the example above will become:

\begin{ipythonnon}
import numpy as np

print (np.array([[1, 2, 3],[4, 5, 6]]))
\end{ipythonnon}
\begin{ioutput}
[[1 2 3]
 [4 5 6]]
\end{ioutput}

Essentially a \texttt{numpy.array} is a list of lists each one representing a matrix row. Arrays can have any dimension so they can be used to represent also vectors in \texttt{python}. There are two special types of arrays: \texttt{zeros} and \texttt{ones} whose names already clarify their nature.

\begin{ipythonnon}
A = np.zeros(shape=(3, 3))
B = np.ones(shape+(4, 4))

print (A)
print (B)
\end{ipythonnon}
\begin{ioutput}
[[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]

[[1. 1. 1. 1.]
 [1. 1. 1. 1.]
 [1. 1. 1. 1.]
 [1. 1. 1. 1.]]
\end{ioutput}

\section{Transpose of a Matrix}
The transpose of a matrix $[A]$, denoted by $[A^T]$, may be constructed by writing the rows of $[A]$ as the columns of $[A^T]$
and the columns of $[A]$ as the rows of $[A^T]$.
Formally, the $i$-th row, $j$-th column element of $[A^T]$ is the $j$-th row, $i$-th column element of $[A]$:

\begin{equation}[A^T]_{ij} = [A]_{ji}\end{equation}

If $[A]$ is an $m\times n$ matrix, then $[A^T]$ is an $n\times m$ matrix. 
As an example the transpose of:

\begin{equation*}
\begin{bmatrix}
1 & 5 & 3 \\
2 & -3 & 8
\end{bmatrix}
\quad \mathrm{is} \quad
\begin{bmatrix}
1 & 2 \\
5 & -3 \\
3  & 8
\end{bmatrix}
\end{equation*}

\section{Operation with Matrices}
\subsection{Adding and Subtracting Matrices}\label{adding-and-subtracting-matrices}

Usually matrices are used to list data or to represent system of equations. Since the entries are numbers, numerical operations can be performed on matrices. Indeed matrices can be added or subtracted by adding or subtracting corresponding entries.

In order to do this, the entries must correspond, therefore, addition and subtraction of matrices is only possible when the matrices have the
same dimensions.

Adding two matrices is very simple. Just add each element in the first matrix to the corresponding element in the second matrix.

\begin{equation*}
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
+
\begin{bmatrix}
10 & 20 & 30\\
40 & 50 & 60
\end{bmatrix}
=
\begin{bmatrix}
11 & 22 & 33\\
44 & 55 & 66
\end{bmatrix}
\end{equation*}
As you might guess, subtracting works the same way except that you subtract instead of adding.

\begin{equation*}
\begin{bmatrix}
10 & 20 & 30\\
40 & 50 & 60
\end{bmatrix}
-
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
=
\begin{bmatrix}
9 & 18 & 27\\
36 & 45 & 54
\end{bmatrix}
\end{equation*}
Adding and subtracting \texttt{numpy.array} is as easy as that:

\begin{ipythonnon}
A = np.array([[1, 2, 3],[4, 5, 6]]
B = np.array([[10, 20, 30],[40, 50, 60]]

print (A + B)
print (A - B)
\end{ipythonnon}
\begin{ioutput}
[[11 22 33]
 [44 55 66]]
 
[[ 9 18 27]
 [36 45 54]]
\end{ioutput}

\subsection{Scalar Multiplication}\label{scalar-multiplication}

Multiplying a matrix by a scalar $c$ means you add the matrix to itself $c$ times, or simply multiply each element by that constant.

\begin{equation*}
3 \cdot
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
=
\begin{bmatrix}
3 & 6 & 9 \\
12 & 15 & 18
\end{bmatrix}
\end{equation*}
Scalar multiplication of \texttt{numpy.array} is:

\begin{ipythonnon}
c = 3
A = np.array([[1, 2, 3],[4, 5, 6]]

print (c * A)
\end{ipythonnon}
\begin{ioutput}
[[ 3  6  9]
 [12 15 18]]
\end{ioutput}

\subsection{Matrix Multiplication}\label{matrix-multiplication}

Matrix multiplication is multiplying every element of each row of the first matrix times every element of each column in the second matrix. Each entry of the resultant matrix is computed one at a time.

Let's see with an example: 
\begin{equation*} 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
= ?
\end{equation*}
First ask: do the number of columns in the first matrix equal the number of rows in the second ? If so the product exists. Then start with producing the product for the first row, first column element. Take the first row of the first matrix and multiply by the first column of the second like this:

\begin{equation*} 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
(1\cdot 5) + (2\cdot 7) & X \\
X & X
\end{bmatrix}
\end{equation*}
Continue the pattern with the first row of the first matrix with the second column of the second matrix:

\begin{equation*} 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
(1\cdot 5) + (2\cdot 7) & (1\cdot 6) + (2\cdot 8)  \\
X & X
\end{bmatrix}
\end{equation*}
Then do the same with the second row of the first matrix and you are done:

\begin{equation*} 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
(1\cdot 5) + (2\cdot 7) & (1\cdot 6) + (2\cdot 8)  \\
(3\cdot 5) + (4\cdot 7) & (3\cdot 6) + (4\cdot 8) 
\end{bmatrix}
=
\begin{bmatrix}
19 & 22 \\
43 & 50 
\end{bmatrix}
\end{equation*}
In \texttt{numpy} array multiplication can be done like this:

\begin{ipythonnon}
A = np.array([[1, 2],[3, 4]]
B = np.array([[5, 6],[7, 8]]
	
print (np.dot(A, B))
# or equivalently A.dot(B)
\end{ipythonnon}
\begin{ioutput}
[[19 22]
 [43 50]]
\end{ioutput}

\section{The identity matrix}\label{the-identity-matrix}

$[I]$ is defined so that $[A][I]=[A]$, i.e.~it is the matrix version of multiplying a number by one. What matrix has this property? A first guess might be a matrix full of 1s, but that does not work:

\begin{equation*}
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
3 & 3 \\
7 & 7
\end{bmatrix}
\end{equation*}
The matrix that does work is a diagonal stretch of 1s, with all other elements being 0:

\begin{equation*}
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\end{equation*}
So $[I] = 
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$ is the identity matrix for $2\times 2$ matrices. The \texttt{numpy} equivalent of the identity matrix is given by
\texttt{numpy.identity(n)} with \texttt{n} the dimension of the matrix. So for example:

\begin{ipythonnon}
A = np.array([[1, 2],[3, 4]]
I = np.identity(2)	

print (A)
print (np.dot(A, I))
\end{ipythonnon}
\begin{ioutput}
[[1 2]
 [3 4]]

[[1. 2.]
 [3. 4.]]
\end{ioutput}
    
\section{Matrix Decomposition}
\label{eigendecomposition}

Decomposing a matrix means to find a product of matrices that is equal to the initial matrix. In this Section the case of \emph{eigen-decomposition} is described, where the initial matrix is decomposed into the product of its \emph{eigenvectors} and \emph{eigenvalues}.
%Before all, let's see the link between matrices and linear
%transformation. Then, you'll learn what are eigenvectors and
%eigenvalues.

\subsection{Eigenvectors and eigenvalues}
\label{eigenvectors-and-eigenvalues}
%\subsection{Matrices as linear transformations}
%\label{matrices-as-linear-transformations}

You can think of matrices as linear transformations so some matrices will rotate your space, others will re-scale it. When a matrix is applied to a vector (apply here means calculate the dot product of the matrix with the vector), we end up with a transformed version of the vector.

Let's apply the matrix $[A]$ to a vector $\boldsymbol{v}$

\begin{ipythonnon}
import numpy as np

A = np.array([[1, 3], [2, 2]])	
v = np.array([2, 1])
Av = A.dot(v)
\end{ipythonnon}
\begin{ioutput}
(-1.0, 4.0)
\end{ioutput}

Figure~\ref{fig:matrix_as_transform} shows the effect of the application of the matrix to the vector, the new vector (orange) has a different direction than the original one (light blue).

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/matrix_transformation}
	\caption{Application of a matrix to a vector, the new vector (orange) is different than the original one (blue).}
	\label{fig:matrix_as_transform}
\end{figure}


%We have seen an example of a vector transformed by a matrix. Now
%imagine that the transformation of the initial \%vector gives us a new
%vector that has the exact same direction. The scale can be different but
%the direction is the \%same. Applying the matrix doesn't change the
%direction of the vector. This special vector is called an eigenvector of
%\%the matrix.

Now imagine that the transformation of the initial vector by the matrix gives a new vector with the exact same direction. This vector is called an \emph{eigenvector} of $[A]$.

This means that \textbf{v} is a eigenvector of $[A]$ if \textbf{v} and $[A]\boldsymbol{v}$ (the transformed vector) have the same direction. The output vector is just a scaled version of the input one with the scaling factor $\lambda$ which is called an \emph{eigenvalue} of \([A]\). Mathematically, the following equation holds:

\begin{equation}
[A]\boldsymbol{v}=\lambda \boldsymbol{v}
\end{equation}

\subsection{Finding Eigenvalues and Eigenvectors in \texttt{python}}
\label{find-eigenvalues-and-eigenvectors-in-python}

\texttt{numpy} provides a function returning eigenvectors and eigenvalues: 

\begin{ipythonnon}
A = np.array([[5, 1], [3, 3]])
np.linalg.eig(A)
\end{ipythonnon}
\begin{ioutput}
(array([6., 2.]),

array([[ 0.70710678, -0.31622777],
       [ 0.70710678,  0.9486833 ]]))
\end{ioutput}
The eigenvectors correspond to the columns of the second array. This means that the eigenvector corresponding to \(\lambda=6\) is:

\begin{equation*}
\begin{bmatrix}
0.70710678 \\
0.70710678
\end{bmatrix}
\end{equation*}
while the eigenvector corresponding to $\lambda=2$ is:

\begin{equation*}
\begin{bmatrix}
-0.31622777 \\
0.9486833
\end{bmatrix}
\end{equation*}

It is important to note that there exists an infinite number of eigenvectors corresponding to a given eigenvalue, they are equivalent because they have the same direction but are just scaled differently. For example the first eigenvector found above is a scaled version of $\begin{bmatrix}1\\ 1\end{bmatrix}$.

\begin{ipythonnon}
v = np.array([1, 2])
Av = A.dot(v)
v_np = [0.31622777, 0.9486833]	
\end{ipythonnon}
\begin{ioutput}
(-1.0, 3.0)
\end{ioutput}
Figure~\ref{fig:eigenvectors} shows three equivalent eigenvectors found
with the code above. We can see that the vector found with \texttt{numpy} (in dark blue) is a scaled version of $\begin{bmatrix}1\\ -3\end{bmatrix}$.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/eigenvectors}
	\caption{Graphical representation of three equivalent eigenvectors.}
	\label{fig:eigenvectors}
\end{figure}

%Let's \([A]\) be the following matrix:
%
%\[A=\begin{bmatrix}
%5 &1 \\
%3 &3
%\end{bmatrix}\]
%
%We know that one eigenvector of A is:
%
%\[v=\begin{bmatrix}
%1 \\
%1\end{bmatrix}\] We can check that \([A]v=\lambda v\)
%
%\[\begin{bmatrix}
%5 & 1 \\
%3& 3\end{bmatrix}
%\begin{bmatrix}
%1 \\
%1\end{bmatrix}
%=\begin{bmatrix}
%6 \\
%6\end{bmatrix}\]
%
%We can represent \(v\) and \([A]v\) to check if their directions are the
%same:
%
%\begin{codebox}
%	\begin{Verbatim}[commandchars=\\\{\}]
%	\PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{)}
%	\PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
%	
%	\PY{n}{Av} \PY{o}{=} \PY{n}{A}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{v}\PY{p}{)}
%	
%	\PY{n}{orange} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}FF9A13}\PY{l+s+s1}{\PYZsq{}}
%	\PY{n}{blue} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}1190FF}\PY{l+s+s1}{\PYZsq{}}
%	
%	\PY{n}{plotVectors}\PY{p}{(}\PY{p}{[}\PY{n}{Av}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{v}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{cols}\PY{o}{=}\PY{p}{[}\PY{n}{blue}\PY{p}{,} \PY{n}{orange}\PY{p}{]}\PY{p}{)}
%	\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}
%	\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}
%
%(-1.0, 7.0)
%\end{Verbatim}
%\end{codebox}
%
%\begin{center}
%	\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{eigenvectors_files/eigenvectors_3_1.png}
%\end{center}
%
%We can see that their directions are the same!
%
%This example shows that the eigenvectors \(v\) are vectors that change
%only in scale when we apply the matrix \([A]\) to them.

\section{The Inverse of a Matrix}\label{the-inverse-of-a-matrix}

The inverse of matrix $[A]$ is $[A^{-1}]$, and is defined by the property:

\begin{equation} 
[A][A^{-1}]=[I] 
\end{equation}

Hence the matrix $[B]$ is the inverse of the matrix $[A]$ if when multiplied together, $[A][B]$ gives the identity matrix. 
Using the definition let's try to find the inverse of:

\begin{equation*}
\begin{bmatrix}
3 & 4\\
5 & 6
\end{bmatrix}
\end{equation*}
First, let the following be true:

\begin{equation*}
\begin{bmatrix}
3 & 4\\
5 & 6
\end{bmatrix}
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}
\end{equation*}
When multiplying this mystery matrix by our original matrix, the result is

\begin{equation*}
\begin{bmatrix}
3a+4c & 3b+4d\\
5a+6c & 5b+6d
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}
\end{equation*}
For two matrices to be equal, every element on the left must equal its corresponding element on the right. So, for these two matrices to equal each other:

\begin{equation*}
\begin{cases}
3a+4c=1\\
3b+4d=0\\
5a+6c=0\\
5b+6d=1
\end{cases}
\end{equation*}
Solving this simple system we get the following result:

\begin{equation*}
\begin{cases}
a=-3\\
b=2\\
c=2.5\\
d=-1.5
\end{cases}
\end{equation*}
Having solved for the four variables, the result is the inverse

\begin{equation*}
\begin{bmatrix}
-3 & 2\\
2.5 & -1.5
\end{bmatrix}
\end{equation*}
The quick check to be sure the result is correct is done in \texttt{python}. The \texttt{linalg.inv()} function can be used to find the inverse of a matrix:

\begin{ipythonnon}
from numpy.linalg import inv

A = np.array([[3, 4],[5, 6]])	
print (inv(A))
\end{ipythonnon}
\begin{ioutput}
[[-3.   2. ]
 [ 2.5 -1.5]]
\end{ioutput}

\subsection{The Moore-Penrose Pseudo-inverse}
\label{sec:the-moore-penrose-pseudoinverse}

%The Moore-Penrose pseudo-inverse is a direct application of the SVD. But
%before all, we have to remind that systems of equations can be expressed
%under the matrix form.
The Moore-Penrose pseudo-inverse is $[A^+]$ such as:

\begin{equation*}
[A][A^+]\approx[I]
\end{equation*}

Think of it as a generalization of the inverse. It is defined for all matrices, but has fewer guaranteed properties as a result. For example, it will be a matrix such that $[A][A^+][A]=[A]$, but not necessarily the stronger usual inverse property is true $[A^{-1}][A]=[A][A^{-1}]=[I]$ (the second one implies the first). In the special case where a matrix has an inverse, it will be the same as the pseudo-inverse.

Let's create a non square matrix $[A]$, calculate its singular value decomposition and its pseudo-inverse with \texttt{linalg.pinv()}.

\begin{equation*}
A=\begin{bmatrix}
7&2\\
3&4\\
5&3
\end{bmatrix}
\end{equation*}

\begin{ipythonnon}
A = np.array([[7, 2, 3],[4, 5, 3]])
A = np.linalg.pinv(A)
A_plus = np.linalg.pinv(A)
print (A_plus)
\end{ipythonnon}
\begin{ioutput}
[[ 0.16666667 -0.10606061  0.03030303]
 [-0.16666667  0.28787879  0.06060606]]
\end{ioutput}
It can be checked that it is really the near inverse of $[A]$ since we know that

\begin{equation*}
A^{+}][A]\approx[A^{-1}][A]=[I]
\end{equation*}

\begin{ipythonnon}
A_plus.dot(A)
\end{ipythonnon}
\begin{ioutput}
array([[1.00000000e+00, 2.35922393e-16],
       [3.33066907e-16, 1.00000000e+00]])
\end{ioutput}
This is almost the identity matrix~!~Another difference with respect to the real inverse is that $[A^+][A]\approx[I]$ but $[A][A^+]\neq [I]$.

\subsection{Eigen-decomposition}
\label{concatenating-eigenvalues-and-eigenvectors}

Now that we have an idea of what eigenvectors and eigenvalues are, we can see how it can be used to decompose a matrix. 
The \emph{eigen-decomposition} is given by

\begin{equation}
[A]=[V]\cdot \textrm{diag}(\boldsymbol{\lambda})\cdot [V^{-1}]
\end{equation}
where the matrix $[V]$ is made by concatenating all eigenvectors of $[A]$ in each column (like in the second array returned by \texttt{np.linalg.eig(A))}, $\textrm{diag}(\boldsymbol{\lambda})$ is a diagonal matrix containing all the eigenvalues. Continuing with our example above we have

\begin{equation*}
V=\begin{bmatrix}
1 & 1 \\
1 &-3\end{bmatrix}\qquad 
\textrm{diag}(\boldsymbol{\lambda})=\begin{bmatrix}
6&0\\
0&2
\end{bmatrix}
\end{equation*} 
and the inverse matrix of $[V]$ can be calculated with \texttt{numpy}

\begin{ipythonnon}
V = np.array([[1, 1], [1, 3]])
V_inv = np.linalg.inv(V)
print (V_inv)
\end{ipythonnon}
\begin{ioutput}
array([[ 0.75,  0.25],
       [ 0.25, -0.25]])
\end{ioutput}

So let's plug it into our equation:

\begin{equation*} 
[V]\cdot\textrm{diag}(\boldsymbol{\lambda})\cdot [V^{-1}]=
\begin{bmatrix}
1 & 1 \\
1 &-3\end{bmatrix}
\begin{bmatrix}
6&0\\
0&2\end{bmatrix}
\begin{bmatrix}
0.75 & 0.25\\
0.25&-0.25 \end{bmatrix}
=\begin{bmatrix}
5 & 1\\
3& 3\end{bmatrix}
\end{equation*}

Let's check our result with \texttt{python}

\begin{ipythonnon}
lambdas = np.diag([6, 2])	
V.dot(lambdas).dot(V_inv)
\end{ipythonnon}
\begin{ioutput}
array([[5., 1.],
       [3., 3.]])
\end{ioutput}

\subsubsection{Real symmetric matrix}
\label{real-symmetric-matrix}

In the case of real symmetric matrices ($[A]=[A^T]$) the eigen-decomposition can be expressed as

\begin{equation}
[A]=[Q][\Lambda][Q^T]
\end{equation} 
where $[Q]$ is the matrix with eigenvectors as columns and $\Lambda$ is $\textrm{diag}(\boldsymbol{\lambda})$.

For that reason, it can be useful to use symmetric matrices! Let's do it now with \texttt{linalg} from \texttt{numpy}

\begin{ipythonnon}
A = np.array([[6, 2], [2, 3]])

eigVals, eigVecs = np.linalg.eig(A)
eigVals = np.diag(eigVals)

print (eigVecs.dot(eigVals).dot(eigVecs.T))
\end{ipythonnon}
\begin{ioutput}
array([[6., 2.],
       [2., 3.]])
\end{ioutput}

%\subsubsection{Quadratic form to matrix form}
%\label{quadratic-form-to-matrix-form}
%
%Let's have the following quadratic equation:
%
%\[f(x)=ax^2_1+(b+c)x_1x_2+dx^2_2\] These quadratic forms can be
%generated by matrices:
%
%\[f(x)=
%\begin{bmatrix}
%x_1 & x_2
%\end{bmatrix}
%\begin{bmatrix}
%a&b\\
%c&d
%\end{bmatrix}
%\begin{bmatrix}
%x_1\\
%x_2\end{bmatrix}
%=x^T[A]x\]
%
%\subsection{With the Principal Axes Theorem}
%\label{with-the-principal-axes-theorem}
%
%Actually there is a simpler way to do the change of variable. We can
%stay in the matrix form. Recall that we start with the form:
%
%\[f(x)=x^T[A]x\] The linear substitution can be wrote in these terms. We
%want replace the variables \(x\) by \(y\) that relates by:
%
%\[x=[P]y\] We want to find \([P]\) such as our new equation doesn't
%contain the cross terms. The first step is to replace that in the first
%equation:
%
%\[x^T[A]x=([P]y)^T[A]([P]y)=y^T([P]^T[A][P])y\] Can you see the how to
%transform the left hand side (\(x\)) into the right hand side (\(y\))?
%The substitution is done by replacing \([A]\) with \([P]^T[A][P]\). We
%also know that \([A]\) is symmetric and thus that there is a diagonal
%matrix \([D]\) containing the eigenvectors of \([A]\) and such as
%\([D]=[P]^T[A][P]\). We thus end up with:
%
%\[x^T[A]x=y^T[D]y\] All of this implies that we can use \([D]\) to
%simplify our quadratic equation and remove the cross terms. If you
%remember from example 2 we know that the eigenvalues of \([A]\) are:
%
%\[[D]=\begin{bmatrix}
%7 &0\\
%0 &2\end{bmatrix}\]
%
%\[x^T[A]x=y^T[D]y=y^T
%\begin{bmatrix}
%7 &0\\
%0 &2\end{bmatrix}y=
%\begin{bmatrix}y_1 & y_2\end{bmatrix}
%\begin{bmatrix}
%7 &0\\
%0 &2\end{bmatrix}
%\begin{bmatrix}y_1\\y_2\end{bmatrix}
%=\begin{bmatrix}7y_1+0y_2 & 0y_1+2y_2\end{bmatrix}
%\begin{bmatrix}y_1\\y_2\end{bmatrix}=7y^2_1+2y^2_2
%\]
%
%This form (without cross-term) is called the principal axes form.
%
%\subsection{Quadratic form optimization}
%\label{quadratic-form-optimization}
%
%Depending to the context, optimizing a function means finding its
%maximum or its minimum. It is for instance widely used to minimize the
%error of cost functions in machine learning.
%
%Here we will see how eigendecomposition can be used to optimize
%quadratic functions and why this can be done easily without cross terms.
%The difficulty is that we want a constrained optimization, that is to
%find the minimum or the maximum of the function for \(f(x)\) being a
%unit vector.
%
%\paragraph{Example 8.}\label{example-8.}
%
%We want to optimize:
%
%\(f(x)=x^T[A]x\) subject to \(||x||_2=1\).In our last example we ended
%up with:
%
%\(f(x)=7y^2_1+2y^2_2\) And the constraint of \(x\) being a unit vector
%imply:
%
%\[||x||_2=1 \implies x^2_1+x^2_2=1\] We can also show that \(y\) has to
%be a unit vector if it is the case for \(x\). Recall first that
%\(x=[P]y\):
%
%%\[||x||_2=$x^Tx=([P]y)^T([P]y)=y^T[P]^T[P]y=y^Ty=||y||_2\] So
%\(||x||_2=||y||_2=1\) and thus \(y^2_1+y^2_2=1\) Since \(y^2_1\) and
%\(y^2_2\) cannot be negative because they are squared values, we can be
%sure that \(2y^2_2\leq 7y^2_2\). Hence:
%
%\[f(x)=7y^2_1+2y^2_2\leq 7y^2_1+7y^2_2 \leq 7(y^2_1+y^2_2)\leq7\] This
%means that the maximum value of \(f(x)\) is 7.
%
%The same way can lead to find the minimum of \(f(x)\).
%\(7y^2_1\geq 2y^2_1\) and:
%
%\[f(x)=7y^2_1+2y^2_2\geq2y^2_1+2y^2_2\geq2(y^2_1+y^2_2)\geq2\] And the
%minimum of \(f(x)\) is 2.

\section{Matrix Equations}
\label{matrix-equations}
As we have learned in previous Sections, matrices can be manipulated in any way that a normal equation can be. Here it will be shown how matrices can be used to compactly write and work with systems of multiple linear equations. 

\subsection{Writing a System of Equations with Matrices}
\label{writing-a-system-of-equations-with-matrices}

It is possible to solve a system of equations using the elimination or substitution method, but it is also possible to do it with a matrix
operation. Before we start setting up the matrices, it is important to do the following:

\begin{itemize}
\tightlist
\item make sure that all of the equations are written in a similar manner, meaning the variables need to all be in the same order;
\item make sure that one side of the equation is only variables and their coefficients, and the other side is just constants;
\end{itemize}

Using matrix multiplication, we may define a system of equations with the same number of equations as variables as:

\begin{equation} 
[A]\boldsymbol{x} = [B]
\end{equation}
where $[A]$ is the coefficient matrix, $\boldsymbol{x}$ is the variable matrix, and $[B]$ is the constant matrix. Given the system:

\begin{equation*}
\begin{cases}
x + 8y = 7 \\
2x -8y = -3
\end{cases}
\end{equation*}
the corresponding matrices are

\begin{equation*}
[A]=
\begin{bmatrix}
1 & 8\\
2 & -8
\end{bmatrix}
;\quad
\boldsymbol{x}=
\begin{bmatrix}
x\\
y
\end{bmatrix}
;\quad
[B]=
\begin{bmatrix}
7\\
-3
\end{bmatrix}
\end{equation*}

\subsection{Solving Systems of Equations Using Matrix Inverses}
\label{solving-systems-of-equations-using-matrix-inverses}

A system of equations can be readily solved using the concepts of the inverse matrix and matrix multiplication. Indeed to solve a system $[A]\boldsymbol{x}=[B]$, for $\boldsymbol{x}$, multiply both sides by the inverse of $[A]$

\begin{equation}
[A^{-1}][A]\boldsymbol{x}=[A^{-1}][B] \implies \boldsymbol{x} = [A^{-1}][B]
\label{eq:matrix_solution} 
\end{equation}

Provided the inverse $[A^{-1}]$ exists, Eq.~\ref{eq:matrix_solution} will solve the system. If the coefficient matrix is not invertible, the system could be inconsistent and have no solution, or be dependent and have infinitely many solutions.

Solve the following system of linear equations:

\begin{equation*}
\begin{cases}
x+2y-z=11\\
2x-y+3z=7\\
7x-3y-2z=2
\end{cases}
\end{equation*}
Set up the three necessary matrices:

\begin{equation*}
[A]=
\begin{bmatrix}
1 & 2 & -1 \\ 
2 & -1 & 3 \\
7 & -3 & -2
\end{bmatrix}
;\quad
[B]=
\begin{bmatrix}
11\\
7\\
2
\end{bmatrix}
;\quad
\boldsymbol{x}=
\begin{bmatrix}
x\\
y \\ 
z
\end{bmatrix}
\end{equation*}
Since to solve this system we have to find the inverse matrix of $[A]$ and multiply it to $[B]$ we have all the ingredients to do it in \texttt{python}:

\begin{ipythonnon}
A = np.array([[1, 2, 1], [2, 1, 3],[7, 3, 2]])
B = np.array([11, 7, 2])

A_inv = inv(A)
sol = np.dot(A_inv, B)

print (sol)
\end{ipythonnon}
\begin{ioutput}
[3. 5. 2.]
\end{ioutput}

So the solution of the system is: 
\begin{equation*}
\begin{cases}
x=3\\
y=5\\
z=2
\end{cases}
\end{equation*}

\subsection{The Cholesky decomposition}
\label{sec:cholesky}

The \emph{Cholesky decomposition} is another way of solving systems of linear equations. It can be significantly faster than other similar methods (e.g. LU decomposition) by exploiting the property of symmetric matrices. However, it is required that the matrix being decomposed be \emph{Hermitian} (i.e real-valued symmetric and thus square) and positive definite. This means that when a matrix $[A]$ is decomposed as $[A] = [L][L]$, $[L]$ is a lower triangular matrix with real and positive numbers on the diagonals, and $[L^T]$ is the conjugate transpose of $[L]$.

Let's consider an example of a system of linear equations where matrix $[A]$ is both Hermitian and positive definite. The equation is in the form of $[A]\boldsymbol{x} = [B]$, where $[A]$ and $[B]$ take the following values:

\begin{equation*}
[A] = \begin{bmatrix}
10.& -1.& 2.& 0.\\ 
-1.& 11.& -1.& 3.\\ 
2.& -1.& 10.& -1.\\
0.& 3.& -1.& 8.
\end{bmatrix};\quad
[B] = \begin{bmatrix}
	6.& 25.& -11.& 15.
\end{bmatrix}
\end{equation*}

Let's represent these matrices as \texttt{numpy} arrays:

\begin{ipythonnon}
import numpy as np

A = np.array([[10., -1., 2., 0.], 
              [-1., 11., -1., 3.], 
              [2., -1., 10., -1.],
              [0., 3., -1., 8.]]) 

B = np.array([6., 25., -11., 15.])
L = np.linalg.cholesky(A)
\end{ipythonnon}

The \texttt{cholesky} function of \texttt{numpy.linalg} would compute the lower triangular factor of matrix $[A]$. Let's view the lower triangular matrix:

\begin{ipythonnon}
print (L}
\end{ipythonnon}
\begin{ioutput}
[[ 3.16227766  0.         0.         0.        ]
 [-0.31622777  3.3015148  0.         0.        ] 
 [ 0.63245553 -0.24231301 3.08889696 0.        ] 
 [ 0.          0.9086738 -0.25245792 2.6665665 ]]
\end{ioutput}

To verify that the Cholesky decomposition results are correct, we can use the definition of the Cholesky factorization by multiplying $[L]$ with its conjugate transpose that should lead us back to the values of matrix $[A]$:

\begin{ipythonnon}
print (np.dot(L, L.T.conj())) # A=L.L* 
\end{ipythonnon}
\begin{ioutput}
[[ 10. -1.  2   0.]
 [ -1. 11. -1.  3.] 
 [  2. -1. 10. -1.] 
 [  0.  3. -1.  8.]]
\end{ioutput}

Before solving for $\boldsymbol{x}$, we need to solve for $[L^T]\boldsymbol{x}$ as $\boldsymbol{y}$. We will use the solve function of numpy.linalg:

\begin{ipythonnon}
y = np.linalg.solve(L, B) # L.L*.x=B; when L*.x=y, then L.y=B
\end{ipythonnon}

To solve for $\boldsymbol{x}$, all we need to do is to solve again using the conjugate transpose of $[L]$ and $\boldsymbol{y}$:

\begin{ipythonnon}
x = np.linalg.solve(L.T.conj(), y) # x=L*'.y

print(x)
\end{ipythonnon}
\begin{ioutput}
array([ 1.,  2., -1.,  1.])
\end{ioutput}
The output gives us our values of $\boldsymbol{x}$. To show that the Cholesky factorization gives us the correct values, we can verify the answer by multiplying the matrix $[A]$ by the transpose of $\boldsymbol{x}$ to return the values of $[B]$:
\begin{ipythonnon}
print (A.dot(x.T)) # B=Ax 
\end{ipythonnon}
\begin{ioutput}
[  6.  25. -11.  15.]
\end{ioutput}

This shows that the values of $\boldsymbol{x}$ by the Cholesky decomposition would lead to the same values given by $[B]$.

\subsection{Solving Over-determined Systems}
\label{using-the-pseudoinverse-to-solve-a-overdetermined-system-of-linear-equations}

As we have seen the inverse of a matrix $[A]$ can be used to solve the equation $[A]\boldsymbol{x}=[B]$. But in the case where the set of equations have 0 or many solutions the inverse cannot be found and the equation cannot be solved. 

The pseudo-inverse helps to solve the system returning the solution that minimize the error (i.e. the solution which gives the closest result to 0 for each equation in the system). 

For this example we will consider this set of three equations with two unknowns:

\begin{equation*}
\begin{cases}
-2x_1-x_2=-2 \\
4x_1-x_2=-8 \\
-x_1-x_2=-2 
\end{cases}
\end{equation*}

%\begin{codebox}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{)}
%\PY{n}{x2\PYZus{}1} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{x1} \PY{o}{+} \PY{l+m+mi}{2}
%\PY{n}{x2\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{n}{x1} \PY{o}{+} \PY{l+m+mi}{8}
%\PY{n}{x2\PYZus{}3} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{*}\PY{n}{x1} \PY{o}{+} \PY{l+m+mi}{2}
%\end{Verbatim}
%\end{codebox}

Figure~\ref{fig:overdet_system} shows a graphical representation of each equation. Being an over-determined system the three lines don't intersect in a single point. Putting this into the matrix form we have:

\begin{equation*}
[A]=\begin{bmatrix}
-2&-1\\
4&-1\\
-1&-1\end{bmatrix}\quad 
\boldsymbol{x}=\begin{bmatrix}
x_1\\
x_2\end{bmatrix}\quad
[B]=\begin{bmatrix}
-2\\
-8\\
-2\end{bmatrix}
\end{equation*}

\begin{equation*}
[A]\boldsymbol{x}=[B]\implies 
\begin{bmatrix}
-2&-1\\
4&-1\\
-1&-1\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\end{bmatrix}=
\begin{bmatrix}
-2\\
-8\\
-2\end{bmatrix}
\end{equation*}

We will now calculate the pseudo-inverse of $[A]$:

\begin{ipythonnon}
A = np.array([[2, 1], [4, 1], [1, 1]])
A_plus = np.linalg.pinv(A)

print (A_plus)
\end{ipythonnon}
\begin{ioutput}
array([[-0.11290323,  0.17741935, -0.06451613],
       [-0.37096774, -0.27419355, -0.35483871]])
\end{ioutput}

Now that we have calculated the pseudo-inverse of $[A]$ we can use it to find $\boldsymbol{x}$ knowing that: $\boldsymbol{x}=[A^+] [B]$

\begin{ipythonnon}
b = np.array([[2], [8], [2]])
res = A_plus.dot(b)
print (res)
\end{ipythonnon}
\begin{ioutput}
array([[-1.06451613],
       [ 3.64516129]])
\end{ioutput}

In our two dimensions, these are the coordinates of $\boldsymbol{x}$. In Fig.~\ref{fig:overdet_system} this point, representing our "best" solution is plotted.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\linewidth]{figures/overdet_system}
	\caption{The three lines represent our system of equation, the point 
		the solution found with the pseudo-inverse technique.}
	\label{fig:overdet_system}
\end{figure}

Maybe you would have expected the point being at the center of gravity of the triangle, well this is not the case because the equations are not scaled the same way.


%To me the important property of the pseudo-inverse arises in solving a
%simple linear system of equations. It has 0, 1, or infinitely many
%solutions. \(ùë•=[ùê¥]^+ùëè\) is the closest solution when none exists in the
%sense above. It gives the single answer when 1 exists. And when many
%exists, it is the smallest solution in the sense that \(||ùë•||_2\) is
%smallest.

\section*{Exercises}
\input{matrices_ex_text}
