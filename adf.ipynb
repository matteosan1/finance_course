{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "progressive-exemption",
   "metadata": {},
   "source": [
    "Augmented Dickey Fuller test (ADF Test) is a common statistical test used to test whether a given Time series is stationary or not. It is one of the most commonly used statistical test when it comes to analyzing the stationary of a series.\n",
    "\n",
    "## Introduction\n",
    "In ARIMA time series forecasting, the first step is to determine the number of differencing required to make the series stationary.\n",
    "\n",
    "Since testing the stationarity of a time series is a frequently performed activity in autoregressive models, the ADF test along with KPSS test is something that you need to be fluent in when performing time series analysis.\n",
    "\n",
    "Another point to remember is the ADF test is fundamentally a statistical significance test. That means, there is a hypothesis testing involved with a null and alternate hypothesis and as a result a test statistic is computed and p-values get reported.\n",
    "\n",
    "It is from the test statistic and the p-value, you can make an inference as to whether a given series is stationary or not.\n",
    "\n",
    "So, how exactly does the ADF test work? let’s see the mathematical intuition behind the test with clear examples.\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "A statistical hypothesis is a hypothesis that is testable on the basis of observed data modelled as the realised values taken by a collection of random variables. A set of data is modelled as being realised values of a collection of random variables having a joint probability distribution in some set of possible joint distributions. The hypothesis being tested is exactly that set of possible probability distributions. A statistical hypothesis test is a method of statistical inference. An alternative hypothesis is proposed for the probability distribution of the data, either explicitly or only informally. The comparison of the two models is deemed statistically significant if, according to a threshold probability—the significance level—the data would be unlikely to occur if the null hypothesis were true. A hypothesis test specifies which outcomes of a study may lead to a rejection of the null hypothesis at a pre-specified level of significance, while using a pre-chosen measure of deviation from that hypothesis (the test statistic, or goodness-of-fit measure). The pre-chosen level of significance is the maximal allowed \"false positive rate\". One wants to control the risk of incorrectly rejecting a true null hypothesis.\n",
    "\n",
    "The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by considering two conceptual types of errors. The first type of error occurs when the null hypothesis is wrongly rejected. The second type of error occurs when the null hypothesis is wrongly not rejected. (The two types are known as type 1 and type 2 errors.)\n",
    "\n",
    "Hypothesis tests based on statistical significance are another way of expressing confidence intervals (more precisely, confidence sets). In other words, every hypothesis test based on significance can be obtained via a confidence interval, and every confidence interval can be obtained via a hypothesis test based on significance.\n",
    "\n",
    "Significance-based hypothesis testing is the most common framework for statistical hypothesis testing. An alternative framework for statistical hypothesis testing is to specify a set of statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model.[3] The most common selection techniques are based on either Akaike information criterion or Bayes factor. However, this is not really an \"alternative framework\", though one can call it a more complex framework. It is a situation in which one likes to distinguish between many possible hypotheses, not just two. Alternatively, one can see it as a hybrid between testing and estimation, where one of the parameters is discrete, and specifies which of a hierarchy of more and more complex models is correct.\n",
    "\n",
    "Null hypothesis significance testing* is the name for a version of hypothesis testing with no explicit mention of possible alternatives, and not much consideration of error rates. It was championed by Ronald Fisher in a context in which he downplayed any explicit choice of alternative hypothesis and consequently paid no attention to the power of a test. One simply set up a null hypothesis as a kind of straw man, or more kindly, as a formalisation of a standard, establishment, default idea of how things were. One tried to overthrow this conventional view by showing that it led to the conclusion that something extremely unlikely had happened, thereby discrediting the theory.\n",
    "\n",
    "### The testing process\n",
    "The usual line of reasoning is as follows:\n",
    "\n",
    "* There is an initial research hypothesis of which the truth is unknown.\n",
    "* The first step is to state the relevant null and alternative hypotheses. This is important, as mis-stating the hypotheses will muddy the rest of the process.\n",
    "* The second step is to consider the statistical assumptions being made about the sample in doing the test; for example, assumptions about the statistical independence or about the form of the distributions of the observations. This is equally important as invalid assumptions will mean that the results of the test are invalid.\n",
    "* Decide which test is appropriate, and state the relevant test statistic T.\n",
    "* Derive the distribution of the test statistic under the null hypothesis from the assumptions. In standard cases this will be a well-known result. For example, the test statistic might follow a Student's t distribution with known degrees of freedom, or a normal distribution with known mean and variance. \n",
    "* Select a significance level $(\\alpha)$, a probability threshold below which the null hypothesis will be rejected. Common values are 5% and 1%.\n",
    "* The distribution of the test statistic under the null hypothesis partitions the possible values of T into those for which the null hypothesis is rejected—the so-called critical region—and those for which it is not. The probability of the critical region is α. In the case of a composite null hypothesis, the maximal probability of the critical region is α.\n",
    "* Compute from the observations the observed value tobs of the test statistic T.\n",
    "* Decide to either reject the null hypothesis in favor of the alternative or not reject it. The decision rule is to reject the null hypothesis H0 if the observed value tobs is in the critical region, and to accept or \"fail to reject\" the hypothesis otherwise.\n",
    "\n",
    "A common alternative formulation of this process goes as follows:\n",
    "\n",
    "* Compute from the observations the observed value tobs of the test statistic T.\n",
    "* Calculate the p-value. This is the probability, under the null hypothesis, of sampling a test statistic at least as extreme as that which was observed (the maximal probability of that event, if the hypothesis is composite).\n",
    "* Reject the null hypothesis, in favor of the alternative hypothesis, if and only if the p-value is less than (or equal to) the significance level (the selected probability) threshold ({\\displaystyle \\alpha }\\alpha ).\n",
    "\n",
    "The difference in the two processes applied to the Radioactive suitcase example (below):\n",
    "\n",
    "\"The Geiger-counter reading is 10. The limit is 9. Check the suitcase.\"\n",
    "\"The Geiger-counter reading is high; 97% of safe suitcases have lower readings. The limit is 95%. Check the suitcase.\"\n",
    "The former report is adequate, the latter gives a more detailed explanation of the data and the reason why the suitcase is being checked.\n",
    "\n",
    "The difference between accepting the null hypothesis and simply failing to reject it is important. The \"fail to reject\" terminology highlights the fact that the a non-significant result provides no way to determine which of the two hypotheses is true, so all that can be concluded is that the null hypothesis has not been rejected. The phrase \"accept the null hypothesis\" may suggest it has been proved simply because it has not been disproved, a logical fallacy known as the argument from ignorance. Unless a test with particularly high power is used, the idea of \"accepting\" the null hypothesis is likely to be incorrect. Nonetheless the terminology is prevalent throughout statistics, where the meaning actually intended is well understood.\n",
    "\n",
    "\n",
    "Interpretation\n",
    "The p-value is the probability that a given result (or a more significant result) would occur under the null hypothesis (or in the case of a composite null, it is the largest such probability; see Chapter 10 of \"All of Statistics: A Concise Course in Statistical Inference\", Springer; 1st Corrected ed. 20 edition, September 17, 2004; Larry Wasserman). For example, say that a fair coin is tested for fairness (the null hypothesis). At a significance level of 0.05, the fair coin would be expected to (incorrectly) reject the null hypothesis in about 1 out of every 20 tests. The p-value does not provide the probability that either hypothesis is correct (a common source of confusion).[9]\n",
    "\n",
    "If the p-value is less than the chosen significance threshold (equivalently, if the observed test statistic is in the critical region), then we say the null hypothesis is rejected at the chosen level of significance. Rejection of the null hypothesis is a conclusion. This is like a \"guilty\" verdict in a criminal trial: the evidence is sufficient to reject innocence, thus proving guilt. We might accept the alternative hypothesis (and the research hypothesis).\n",
    "\n",
    "If the p-value is not less than the chosen significance threshold (equivalently, if the observed test statistic is outside the critical region), then the evidence is insufficient to support a conclusion. (This is similar to a \"not guilty\" verdict.) The researcher typically gives extra consideration to those cases where the p-value is close to the significance level.\n",
    "\n",
    "Some people find it helpful to think of the hypothesis testing framework as analogous to a mathematical proof by contradiction.\n",
    "\n",
    "Real world applications of hypothesis testing include:\n",
    "\n",
    "Testing whether more men than women suffer from nightmares\n",
    "Establishing authorship of documents\n",
    "Evaluating the effect of the full moon on behavior\n",
    "Determining the range at which a bat can detect an insect by echo\n",
    "Deciding whether hospital carpeting results in more infections\n",
    "Selecting the best means to stop smoking\n",
    "Checking whether bumper stickers reflect car owner behavior\n",
    "Testing the claims of handwriting analysts\n",
    "\n",
    "\n",
    "The book How to Lie with Statistics[15][16] is the most popular book on statistics ever published.[17] It does not much consider hypothesis testing, but its cautions are applicable, including: Many claims are made on the basis of samples too small to convince. If a report does not mention sample size, be doubtful.\n",
    "\n",
    "Hypothesis testing acts as a filter of statistical conclusions; only those results meeting a probability threshold are publishable. Economics also acts as a publication filter; only those results favorable to the author and funding source may be submitted for publication. The impact of filtering on publication is termed publication bias. A related problem is that of multiple testing (sometimes linked to data mining), in which a variety of tests for a variety of possible effects are applied to a single data set and only those yielding a significant result are reported. These are often dealt with by using multiplicity correction procedures that control the family wise error rate (FWER) or the false discovery rate (FDR).\n",
    "\n",
    "Those making critical decisions based on the results of a hypothesis test are prudent to look at the details rather than the conclusion alone. In the physical sciences most results are fully accepted only when independently confirmed. The general advice concerning statistics is, \"Figures never lie, but liars figure\" (anonymous).\n",
    "\n",
    "## What is a Unit Root Test?\n",
    "The ADF test belongs to a category of tests called ‘Unit Root Test’, which is the proper method for testing the stationarity of a time series.\n",
    "\n",
    "So what does a ‘Unit Root’ mean?\n",
    "Unit root is a characteristic of a time series that makes it non-stationary. Technically speaking, a unit root is said to exist in a time series of the value of alpha = 1 in the below equation.\n",
    "\n",
    "$$Y_t = \\alpha Y_{t-1} + \\beta X_e + \\epsilon$$\n",
    "\n",
    "where, Yt is the value of the time series at time ‘t’ and Xe is an exogenous variable (a separate explanatory variable, which is also a time series).\n",
    "\n",
    "What does this mean to us?\n",
    "\n",
    "The presence of a unit root means the time series is non-stationary. Besides, the number of unit roots contained in the series corresponds to the number of differencing operations required to make the series stationary.\n",
    "\n",
    "Alright, let’s come back to topic.\n",
    "\n",
    "3. Dickey-Fuller Test\n",
    "Before going into ADF test, let’s first understand what is the Dickey-Fuller test.\n",
    "\n",
    "A Dickey-Fuller test is a unit root test that tests the mull hypothesis that α=1 in the following model equation. alpha is the coefficient of the first lag on Y.\n",
    "\n",
    "Null Hypothesis (H0): alpha=1\n",
    "\n",
    "$$y_t = c + \\beta t + \\alpha y_{t-1} + \\phi\\Delta Y_{t-1} + \\epsilon_t$$\n",
    "\n",
    "where, $y_{t-1}$ = lag1 of time series, $\\Delta Y_{t-1}$ = first difference of the series at time $(t-1)$.\n",
    "Fundamentally, it has a similar null hypothesis as the unit root test. That is, the coefficient of Y(t-1) is 1, implying the presence of a unit root. If not rejected, the series is taken to be non-stationary.\n",
    "\n",
    "The Augmented Dickey-Fuller test evolved based on the above equation and is one of the most common form of Unit Root test.\n",
    "\n",
    "4. How does Augmented Dickey Fuller (ADF) Test work?\n",
    "As the name suggest, the ADF test is an ‘augmented’ version of the Dickey Fuller test.\n",
    "\n",
    "The ADF test expands the Dickey-Fuller test equation to include high order regressive process in the model.\n",
    "\n",
    "$$y_t = c + \\beta t + \\alpha y_{t-1} + \\phi_1 \\Delta Y_{t-1} + \\phi_2 \\Delta Y_{t-2} + + \\phi_p \\Delta Y_{t-p} + \\epsilon_t$$\n",
    "\n",
    "If you notice, we have only added more differencing terms, while the rest of the equation remains the same. This adds more thoroughness to the test.\n",
    "\n",
    "The null hypothesis however is still the same as the Dickey Fuller test.\n",
    "\n",
    "A key point to remember here is: Since the null hypothesis assumes the presence of unit root, that is α=1, the p-value obtained should be less than the significance level (say 0.05) in order to reject the null hypothesis. Thereby, inferring that the series is stationary.\n",
    "\n",
    "However, this is a very common mistake analysts commit with this test. That is, if the p-value is less than significance level, people mistakenly take the series to be non-stationary.\n",
    "\n",
    "5. ADF Test in Python\n",
    "So, how to perform a Augmented Dickey-Fuller test in Python?\n",
    "\n",
    "The statsmodel package provides a reliable implementation of the ADF test via the adfuller() function in statsmodels.tsa.stattools. It returns the following outputs:\n",
    "\n",
    "The p-value\n",
    "The value of the test statistic\n",
    "Number of lags considered for the test\n",
    "The critical value cutoffs.\n",
    "When the test statistic is lower than the critical value shown, you reject the null hypothesis and infer that the time series is stationary.\n",
    "\n",
    "Alright, let’s run the ADF test on the a10 dataset from the fpp package from R. This dataset counts the total monthly scripts for pharmaceutical products falling under ATC code A10. The original source of this dataset is the Australian Health Insurance Commission.\n",
    "\n",
    "As see earlier, the null hypothesis of the test is the presence of unit root, that is, the series is non-stationary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Import data\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/selva86/datasets/master/a10.csv'\n",
    "df = pd.read_csv(url, parse_dates=['date'], index_col='date')\n",
    "series = df.loc[:, 'value'].values\n",
    "df.plot(figsize=(14,8), legend=None, title='a10 - Drug Sales Series');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-details",
   "metadata": {},
   "source": [
    "The packages and the data is loaded, we have everything needed to perform the test using adfuller().\n",
    "\n",
    "An optional argument the adfuller() accepts is the number of lags you want to consider while performing the OLS regression.\n",
    "\n",
    "By default, this value is 12*(nobs/100)^{1/4}, where nobs is the number of observations in the series. But, optionally you can specify either the maximum number of lags with maxlags parameter or let the algorithm compute the optimal number iteratively.\n",
    "\n",
    "This can be done by setting the autolag='AIC'. By doing so, the adfuller will choose a the number of lags that yields the lowest AIC. This is usually a good option to follow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test\n",
    "result = adfuller(series, autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'n_lags: {result[1]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-relaxation",
   "metadata": {},
   "source": [
    "The p-value is obtained is greater than significance level of 0.05 and the ADF statistic is higher than any of the critical values.\n",
    "\n",
    "Clearly, there is no reason to reject the null hypothesis. So, the time series is in fact non-stationary.\n",
    "\n",
    "6. ADF Test on stationary series\n",
    "Now, let’s see another example of performing the test on a series of random numbers which is usually considered as stationary.\n",
    "\n",
    "Let’s use np.random.randn() to generate a randomized series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test on random numbers\n",
    "series = np.random.randn(100)\n",
    "result = adfuller(series, autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-brooklyn",
   "metadata": {},
   "source": [
    "The p-value is very less than the significance level of 0.05 and hence we can reject the null hypothesis and take that the series is stationary.\n",
    "\n",
    "Let’s visualise the series as well to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-environment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
