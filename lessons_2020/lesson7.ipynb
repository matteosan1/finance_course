{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Correlation between Risks\n",
    "In credit derivative valuation and credit risk management, one of the most important issue is the estimate of default probabilities and their correlations. \n",
    "\n",
    "Default correlation measures the tendency of two companies to default at about the same time. For this, generally speaking, there are two ways: using historical default data or using mathematical models, like copulas. \n",
    "\n",
    "Historical default data has played an important role in the estimation of default probabilities. However, because default events are rare, there is very limited default data available. Moreover, historical data reflects the historical default pattern only and it may not be a proper indicator of the future. This makes the estimation of default probabilities from historical data difficult and inexact. To use this same data to estimate default correlations is even more difficult and more inexact. \n",
    "\n",
    "On the other hand mathematical models don't rely on historical default data. We have already seen how it is possible to derive default probabilities from market data. Before going into the details of the application of the copula to them let's introduce two more kind of credit derivatives.\n",
    "\n",
    "## Basket Default Swaps\n",
    "A basket default swap is a credit derivative on a portfolio of reference entities. The simplest basket default swaps are the nth-to-default swaps.\n",
    "\n",
    "With respect to a basket of reference entities, a first-to-default swap provides insurance for only the first default, a second-to-default swap provides insurance for only the second default, an nth-to-default swap provides insurance for only the nth default. \n",
    "\n",
    "For example, in an nth-to-default swap, the protection seller does not make a payment to the protection buyer for the first n−1 defaulted reference entities, and makes the payment only for the nth defaulted reference entity. Once there is a payment upopn the default of the a defaulted reference entity, the swap terminates. It behaves like a standard CDS but refers to then nth entity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collateralized Debt Obligation\n",
    "A collateralized debt obligation (CDO) is a security backed by a diversified pool of\n",
    "one or more kinds of debt obligations such as bonds, loans, credit default swaps or\n",
    "structured products (mortgage-backed securities, asset-backed securities, and even\n",
    "other CDOs). A CDO can be initiated by one or more of the following: banks,\n",
    "nonbank financial institutions, and asset management companies, is referred to as\n",
    "the sponsor. The sponsor of a CDO creates a company so-called the special purpose\n",
    "vehicle (SPV). The SPV works as an independent entity. In this way, CDO investors\n",
    "are isolated from the credit risk of the sponsor. Moreover, the SPV is responsible\n",
    "for the administration. The SPV obtains the credit risk exposure by purchasing\n",
    "debt obligations (bonds or residential and commercial loans) or selling CDSs; it\n",
    "transfers the credit risk by issuing debt obligations (tranches/credit-linked notes).\n",
    "The investors in the tranches of a CDO have the ultimate credit risk exposure to\n",
    "the underlying reference entities.\n",
    "The SPV issues four kinds of tranches. Each tranche has\n",
    "an attachment percentage and a detachment percentage. When the cumulative\n",
    "percentage loss of the portfolio reaches the attachment percentage, investors\n",
    "in the tranche start to lose their principal, and when the cumulative percentage loss\n",
    "of principal reaches the detachment percentage, the investors in the tranche lose all\n",
    "their principal and no further loss can occur to them.\n",
    "\n",
    "In the literature, tranches of a CDO are classified as subordinate/equity tranche,\n",
    "mezzanine tranches, and senior tranches according to their subordinate levels. \n",
    "Because the equity tranche is extremely\n",
    "risky, the sponsor of a CDO holds the equity tranche and the SPV sells other tranches\n",
    "to investors.\n",
    "\n",
    "### Role of Correlation in Basket CDS and CDO\n",
    "The cost of protection in a nth-to-default CDS or a tranche of a CDO is critically dependent on default correlation. \n",
    "\n",
    "As we have seen in the example, correlation plays a very important role. Consequently it is clear that it will affects the price of first-to-default CDS with respect to a 10th-to-default CDS for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating nth-to-default Probabilities\n",
    "\n",
    "The valuation of a basket default swap comes down to the calculation of relevant default probabilities. So let's see how we can compute them.\n",
    "\n",
    "#### Independent Defaults\n",
    "If the default times of the names of a basket are independent nth-to-default probabilities can be calculated through multiplication and integration of the default probability curves of each basket components. \n",
    "\n",
    "As an example, we consider the second-to-default probability of a 3-name basket. Let $\\tau_i$ be the default time of name $i$ and $F_i(t)$ its distribution. Then the probability that name 1 defaults second in the basket before time $t$: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&P((\\tau_2\\lt\\tau_1)\\cap (\\tau_1\\lt t)\\cap (\\tau_1\\lt\\tau_3)) +\n",
    "P((\\tau_3\\lt\\tau_1)\\cap (\\tau_1\\lt t)\\cap (\\tau_1\\lt\\tau_2)) = \\\\\n",
    "&\\int_0^t{F_2 (s)\\cdot (1-F_3 (s))~dF_1(s)} +  \\int_0^t{F_3 (s)\\cdot (1-F_2 (s))~dF_1(s)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The formula for nth-to-default probability in a general basket can be derived similarly, however, complexity increases as the number of names increases.\n",
    "\n",
    "Suppose the default probabilities of three companies, A, B and C are given as in the following table:\n",
    "\n",
    "|time in years | A | B | C |\n",
    "| :-:|:-:|:-:|:-:|\n",
    "|0 | 0 | 0 | 0 |\n",
    "|1 | 0.022032 | 0.0317 | 0.035 |\n",
    "|2 | 0.046242 | 0.0655 | 0.075 |\n",
    "|3 | 0.07266 | 0.1022 | 0.121 |\n",
    "|4 | 0.101233 | 0.142 | 0.153 |\n",
    "|5 | 0.131885 | 0.1752 | 0.205 |\n",
    "\n",
    "and suppose that the default events of the three companies are independent. \n",
    "\n",
    "The default probabilities are linear in each time interval so the integral above can be solved by substitution:\n",
    "\n",
    "$$ \\int_{x_0}^{x_1}{(1-F_B(x))(1-F_C(x))dF_A(x)}$$\n",
    "\n",
    "Setting $t=m_A x + q_A$ it becomes with $m_A, q_A$ are the parameters of the line joining the default probabilities of company A:\n",
    "\n",
    "$$ \\int_{m_A x_0 + q_A}^{m_A x_1 + q_A}{(1-F_B(x(t)))(1-F_C(x(t)))dt}\\qquad\\textrm{, with}~x(t) = \\cfrac{t -q_A}{m_A} $$\n",
    "and similarly for company B and C.\n",
    "\n",
    "To convert it into python we can use $\\tt{scipy.integrate.quad}$ to perform the integral and $\\tt{numpy.interp}$ to determine the intermediate default probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First to default prob at time (1) for company A: 0.02131\n",
      "First to default prob at time (2) for company A: 0.04301\n",
      "First to default prob at time (3) for company A: 0.06460\n",
      "First to default prob at time (4) for company A: 0.08573\n",
      "First to default prob at time (5) for company A: 0.10606\n",
      "First to default prob at time (1) for company B: 0.03080\n",
      "First to default prob at time (2) for company B: 0.06160\n",
      "First to default prob at time (3) for company B: 0.09245\n",
      "First to default prob at time (4) for company B: 0.12315\n",
      "First to default prob at time (5) for company B: 0.15018\n",
      "First to default prob at time (1) for company C: 0.03407\n",
      "First to default prob at time (2) for company C: 0.07071\n",
      "First to default prob at time (3) for company C: 0.10986\n",
      "First to default prob at time (4) for company C: 0.13879\n",
      "First to default prob at time (5) for company C: 0.17011\n"
     ]
    }
   ],
   "source": [
    "from scipy.integrate import quad\n",
    "from numpy import interp\n",
    "\n",
    "default_rates = {\"A\":(0, 0.022032, 0.046242, 0.07266, 0.101233, 0.131885), # company A\n",
    "                 \"B\":(0, 0.0317, 0.0655, 0.1022, 0.142, 0.1752), # company B\n",
    "                 \"C\":(0, 0.035, 0.075, 0.121, 0.153, 0.205)} # company C\n",
    "\n",
    "def func(x, default, companies, t):\n",
    "    m = default[companies[0]][t] - default[companies[0]][t-1]\n",
    "    q = default[companies[0]][t-1] - m * (t-1)\n",
    "    t = (x-q)/m\n",
    "    F2 = 1 - interp(t, range(len(default[companies[1]])), default[companies[1]])\n",
    "    F3 = 1 - interp(t, range(len(default[companies[2]])), default[companies[2]])\n",
    "    return F2*F3\n",
    "\n",
    "def integral(default, companies, t):\n",
    "    return quad(func, 0, default[companies[0]][t], args=(default, companies, t))[0]\n",
    "                 \n",
    "for companies in [(\"A\", \"B\", \"C\"), (\"B\", \"A\", \"C\"), (\"C\", \"A\", \"B\")]:\n",
    "    prob = 0\n",
    "    for t in range(1, 6):\n",
    "        prob = integral(default_rates, companies, t)\n",
    "        print (\"First to default prob at time ({}) for company {}: {:.5f}\".format(t, companies[0], prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlated Defaults\n",
    "When the default probabilities of the companies are correlated the copula approach can be used like in the example shown above.\n",
    "\n",
    "Suppose we would like to simulate the defaults for the next 5 years for 6 companies. The copula default correlation between each company is 0.2 and the cumulative probability of default during the next 1,2,3,4 5 years is 1%, 3%, 6%, 10%, 13% respectively for each company.\n",
    "\n",
    "When a Gaussian copula is used in order to simulate the defaults we need to sample from a multivariate normal distribution a vector $\\mathbf{x}$, transform then each $x_i$ into the corresponding default probability $p_i$.\n",
    "\n",
    "Let's check the 3th-to-default probabilities for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd-to-default probabilies\n",
      "0: 0.0000\n",
      "1: 0.0003\n",
      "2: 0.0033\n",
      "3: 0.0109\n",
      "4: 0.0250\n",
      "5: 0.0267\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "p_default = [0, 0.01, 0.03, 0.06, 0.10, 0.13]\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0]*6,\n",
    "                             cov = [[1, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                                    [0.2, 1, 0.2, 0.2, 0.2, 0.2],\n",
    "                                    [0.2, 0.2, 1, 0.2, 0.2, 0.2],\n",
    "                                    [0.2, 0.2, 0.2, 1, 0.2, 0.2],\n",
    "                                    [0.2, 0.2, 0.2, 0.2, 1, 0.2],\n",
    "                                    [0.2, 0.2, 0.2, 0.2, 0.2, 1]])\n",
    "\n",
    "trials = 100000\n",
    "result = [0., 0., 0., 0., 0., 0.]\n",
    "x = mvnorm.rvs(size=trials)\n",
    "\n",
    "for n in range(len(x)):\n",
    "    p = sorted(norm.cdf(x[n]))\n",
    "    for i in range(1, len(p_default)):\n",
    "        if p_default[i-1] <= p[2] <= p_default[i]:\n",
    "            result[i] += 1\n",
    "\n",
    "print (\"3rd-to-default probabilies\")\n",
    "for i in range(len(p_default)):\n",
    "    print (\"{}: {:.4f}\".format(i, result[i]/trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Market Model\n",
    "While there are several types of copula function models, the first introduced was the one-factor Gaussian copula model. This model has, above all, the advantage that can be solved semi-analytically.\n",
    "\n",
    "Consider a portfolio of $N$ companies and assume that the marginal probabilities of default are known for each company. Define:\n",
    "\n",
    "* $t_i$, the time of default of the $i$th company:\n",
    "* $Q_i(t)$, the cumulative probability that company $i$ will default before time $t$; that is, the probability that $t_i \\le t$;\n",
    "* $S_i(t) = 1 – Q_i(t)$, the probability that company $i$ will survive beyond time $t$; that is, the probability that $t_i > t$. \n",
    "\n",
    "To generate a one-factor model for the $t_i$ we define random variables $X_i$ $(1\\le i \\le N)$\n",
    "\n",
    "$$X_i = a_i M + \\sqrt{1-a_i^2 Z_i},\\qquad i = 1, 2,\\ldots, n$$\n",
    "\n",
    "where $M$ and the $Z_i$ are independent zero-mean unit-variance distributions and $–1 \\le a_i \\lt 1$. \n",
    "\n",
    "The previous equation defines a correlation structure between the $X_i$ dependent on a single common factor $M$. The correlation between $X_i$ and $X_j$ is $a_i a_j$.\n",
    "\n",
    "Let $F_i$ be the cumulative distribution of $X_i$. Under the copula model the $X_i$ are mapped to the $t_i$ using a *percentile-to-percentile* transformation. The five-percentile point in the probability distribution for $X_i$ is transformed to the five-percentile point in the probability distribution of $t_i$ and so on.\n",
    "\n",
    "In general the point $X_i = x$ is transformed to $t_i = t$ where $t = Q_i^{–1}[F_i(x)]$.\n",
    "\n",
    "Let $H$ be the cumulative distribution of the $Z_i$.\n",
    "It follows from the previous equation that \n",
    "\n",
    "$$\\mathbb{P}(X_i < x|M) = H\\left(\\cfrac{x-a_i M}{\\sqrt{1-a_i^2}}\\right)$$\n",
    "\n",
    "When $x = F_i^{–1}[Q_i(t)]$, $\\mathbb{P}(t_i < t) = \\mathbb{P}(x_i < x)$. Hence\n",
    "\n",
    "$$\\mathbb{P}(t_i < t|M) = H\\left\\{\\cfrac{F_i^{–1}[Q_i(t)]-a_i M}{\\sqrt{1-a_i^2}}\\right\\}$$\n",
    "\n",
    "The conditional probability that the $i$th bond will **survive** beyond time $T$ is therefore\n",
    "\n",
    "$$S_i(T|M) = 1 - H\\left\\{\\cfrac{F_i^{–1}[Q_i(t)]-a_i M}{\\sqrt{1-a_i^2}}\\right\\}$$\n",
    "\n",
    "Although in principle any distribution could be used for $M$’s and the $Z$’s (provided they have zero mean and unit variance),\n",
    "one common choice is to let them be standard normal distributions (resulting in a Gaussian copula). \n",
    "\n",
    "Different choices of distributions result in different copula models, and in different natures of the default dependence. For example, copulas where the $M$’s have heavy tails generate models where there is a greater likelihood of a clustering of early defaults for several companies. \n",
    "<!----\n",
    "Later on we\n",
    "will explore the effect of using normal and t-distributions for the $M$'s and $Z$'s. \n",
    "Suppose that a CDO includes $n$ assets $i = 1, 2,\\ldots, n$ and the default times $\\tau_i$ of the $i$th asset has a default intensity $\\lambda_i$. Then the probability of a default occurring before the time $t$ is\n",
    "$$\\mathbb{P}(\\tau_i \\lt t) = 1 - \\mathrm{exp}(-\\lambda_i t)$$\n",
    "see Chapter~\\ref{hazard}.----->\n",
    "\n",
    "For simplicity, the following two assumptions are made:\n",
    "\n",
    "* all the companies have the same default intensity, i.e, $\\lambda_i = \\lambda$;\n",
    "* the pairwise default correlations are the same, i.e $a_i = a$.\n",
    "\n",
    "The second assumption means that the contribution of the market component is\n",
    "the same for all the companies and the correlation between any two companies is\n",
    "constant, $\\rho = a^2$.\n",
    "\n",
    "Under these assumptions, given the market situation $M = m$, all the companies\n",
    "have the same cumulative default probability $DP_{t|M}=\\mathbb{P}(t_i < t|M)$. Moreover, for a\n",
    "given value of the market component $M$, the defaults are mutually independent for\n",
    "all the underlying companies. Letting $N_{t|m}$ be the total defaults that have occurred\n",
    "by time $t$ conditional on the market condition $M = m$, then $N_{t|m}$ follows a binomial\n",
    "distribution, and\n",
    "\n",
    "$$DP(N_{t|m} = j) = \\cfrac{n!}{j!(n-j)!}DP^j_{t|m}(1-DP_{t|m})^{n-j},\\qquad  j=0, 1, 2,\\ldots,n$$\n",
    "The probability that there will be exactly $j$ defaults by time $t$ is\n",
    "\n",
    "$$DP(N_{t} = j) = \\int_{-\\infty}^{\\infty}{DP(N_{t|m} = j)f_M(m)dm}$$\n",
    "\n",
    "where $f_M(m)$ is the probability density function (PDF) of the random variable $M$.\n",
    "\n",
    "<!------- In a one-factor Gaussian\n",
    "copula model, the distributions of the common market component $M$ and the individual component $Z_i$ are standard normal Gaussian distributions.\n",
    "Because the sum of two independent Gaussian distributions is still a Gaussian distribution, the $X_i$ have a standard normal distribution.---->\n",
    "\n",
    "The one-factor copula Gaussian copula model under the assumptions outlined above is the *market standard model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Quadrature\n",
    "The integral of the last equation of the previous Section can be quite complicated depending on the distribution of $f_M$. \n",
    "\n",
    "The Gaussian Quadrature is a technique that allows to approximate that integral with a discrete weighted sum with weights determined by the function $f_M$. Assuming $f$ is Gaussian we can write\n",
    "\n",
    "$$\\int_{-\\infty}^{+\\infty}\\cfrac{1}{\\sqrt{2\\pi}}e^{-F^{2}/2}g(F)dF\\approx\\sum_{k=1}^{k=N}w_k g(F_k)$$\n",
    "\n",
    "As $N$ increases the accuracy of the approximation increases, usually $N=60$ is sufficient.\n",
    "\n",
    "At [this page]() a dedicated class, $\\tt{GaussianQuadrature}$ is available. Through the method $\\tt{M}$ returns the appropriate list of weights $w_k$ and values $F_k$ to compute the intergral (it is necessary to downaload also [this file]() with all the values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:  [14.36715008, 13.46459111, 12.71717121, 12.04990464, 11.43418345, 10.85527694, 10.30435118, 9.775583711, 9.264879565, 8.769218063, 8.286287662, 7.814266859, 7.351685308, 6.897331785, 6.450190946, 6.009398508, 5.574208647, 5.143969727, 4.718105868, 4.296102681, 3.877496046, 3.461863141, 3.048815156, 2.637991292, 2.229053735, 1.821683385, 1.415576157, 1.010439725, 0.605990602, 0.201951448, -0.201951448, -0.605990602, -1.010439725, -1.415576157, -1.821683385, -2.229053735, -2.637991292, -3.048815156, -3.461863141, -3.877496046, -4.296102681, -4.718105868, -5.143969727, -5.574208647, -6.009398508, -6.450190946, -6.897331785, -7.351685308, -7.814266859, -8.286287662, -8.769218063, -9.264879565, -9.775583711, -10.30435118, -10.85527694, -11.43418345, -12.04990464, -12.71717121, -13.46459111, -14.36715008]\n",
      "weights:  [6.26018e-46, 1.37648e-40, 2.12791e-36, 7.51816e-33, 9.67908e-30, 5.8078e-27, 1.88764e-24, 3.67432e-22, 4.6002e-20, 3.90602e-18, 2.34277e-16, 1.02492e-14, 3.35604e-13, 8.40054e-12, 1.63579e-10, 2.51449e-09, 3.08925e-08, 3.06553e-07, 2.47921e-06, 1.64672e-05, 9.04268e-05, 0.000412859, 0.001574836, 0.005039442, 0.013575119, 0.030871873, 0.059408466, 0.096914318, 0.134203158, 0.157890214, 0.157890214, 0.134203158, 0.096914318, 0.059408466, 0.030871873, 0.013575119, 0.005039442, 0.001574836, 0.000412859, 9.04268e-05, 1.64672e-05, 2.47921e-06, 3.06553e-07, 3.08925e-08, 2.51449e-09, 1.63579e-10, 8.40054e-12, 3.35604e-13, 1.02492e-14, 2.34277e-16, 3.90602e-18, 4.6002e-20, 3.67432e-22, 1.88764e-24, 5.8078e-27, 9.67908e-30, 7.51816e-33, 2.12791e-36, 1.37648e-40, 6.26018e-46]\n"
     ]
    }
   ],
   "source": [
    "from finmarkets import GaussianQuadrature\n",
    "\n",
    "gq = GaussianQuadrature()\n",
    "print (\"values: \", gq.M(60)[0])\n",
    "print (\"weights: \", gq.M(60)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basket CDS Valuation under Market Standard Model\n",
    "We now present some numerical results for an $n$th to default basket. We assume that the\n",
    "principals and expected recovery rates are the same for all underlying reference assets.\n",
    "The valuation procedure is similar to that for a regular CDS where there is only one\n",
    "reference entity.\n",
    "\n",
    "In a regular CDS indeed its valuation is based on the probability that a default\n",
    "occured between times $t1$ and $t2$. Here instead the valuation will be based on the probability that the\n",
    "$n$th default was between times $t1$ and $t2$.\n",
    "\n",
    "We assume the buyer of protection makes quarterly payments at a specified rate\n",
    "until the $n$th default occurs or the end of the life of the contract is reached. \n",
    "\n",
    "In the event of the $n$th default occurring, the seller pays $N\\cdot(1-R)$. \n",
    "The contract can be valued by calculating the expected present value\n",
    "of payments and the expected present value of payoffs in a risk-neutral world. \n",
    "\n",
    "Consider first a 5-year $n$th to default CDS on a basket of 10 reference entities in the\n",
    "situation where the copula correlation is 0.3 and the expected recovery rate, $R$, is $40\\%$. The term structure of interest rates\n",
    "is assumed to be flat at 5%. The default probabilities for the 10 entities are generated by\n",
    "Poisson processes with constant default intensities (hazard rates), $\\lambda_i$, $(1 \\le i \\le 10)$ so that \n",
    "\n",
    "$$ DP(t) = 1 - e^{-\\lambda t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017795634956118353\n"
     ]
    }
   ],
   "source": [
    "from finmarkets import DiscountCurve, CreditCurve, CreditDefaultSwap\n",
    "from finmarkets import GaussianQuadrature\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import norm, binom\n",
    "from math import sqrt, exp\n",
    "\n",
    "n_cds = 10\n",
    "rho = 0.3\n",
    "l = 0.01\n",
    "pillar_dates = []\n",
    "df = []\n",
    "observation_date = date.today()\n",
    "\n",
    "for i in range(6):\n",
    "    pillar_dates.append(observation_date + relativedelta(years=i))\n",
    "    df.append(1/(1+0.05*i))\n",
    "dc = DiscountCurve(observation_date, pillar_dates, df)\n",
    "\n",
    "gq = GaussianQuadrature()\n",
    "values, weights = gq.M(60)\n",
    "\n",
    "Q = [1-exp(-(l*t)) for t in range(6)]\n",
    "cds = CreditDefaultSwap(1, observation_date, 0.01, 5)\n",
    "\n",
    "ndefault = 3\n",
    "S = []\n",
    "    \n",
    "for j in range(len(values)):\n",
    "    temp = []\n",
    "    for i in range(6):\n",
    "        P = norm.cdf((norm.ppf(Q[i]) - sqrt(rho)*values[j])/\n",
    "                     (sqrt(1-rho)))\n",
    "        b = binom(n_cds, P)\n",
    "        temp.append(1 - (b.cdf(n_cds)-b.cdf(ndefault-1)))\n",
    "    S.append(temp)\n",
    "\n",
    "s = 0\n",
    "for j in range(len(values)):\n",
    "    s += weights[j] * cds.breakevenRate(dc, CreditCurve(pillar_dates, \n",
    "                                              S[j]))\n",
    "\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valuation of CDO\n",
    "Suppose that the payment date on a CDO tranche are at times $\\tau_i$. Define $\\mathbb{E}_j$ the expected \n",
    "tranche principal at time $\\tau$ and $D(\\tau)$ the discount factor at time $\\tau$. Suppose also that the spread\n",
    "on a particular tranche (i.e. the number of basis point paid for protection on the remaining tranche principal) is $s$. \n",
    "\n",
    "The present value of the expected regular spread payments on the CDO is given by\n",
    "\\begin{equation}\n",
    "s\\cdot A = s\\cdot \\sum_{j=1}^{m}(\\tau_j - \\tau_{j-1})\\mathbb{E}_{j}D(\\tau_j)\n",
    "\\label{eq:A}\n",
    "\\end{equation}\n",
    "The expected loss between times $\\tau_{j-1}$ and $\\tau_j$ is $\\mathbb{E}_{j-1}-\\mathbb{E}_j$. For simplicity assume\n",
    "the loss occurs only at the midpoint of the time interval, so the present value of the expected payoffs on the CDO tranche is\n",
    "\\begin{equation}\n",
    "C=\\sum_{j=1}^{m}(\\mathbb{E}_{j-1}-\\mathbb{E}_j)D\\left(\\frac{\\tau_{j-1}+\\tau_j}{2}\\right)\n",
    "\\label{eq:C}\n",
    "\\end{equation}\n",
    "The accrual payment due on the losses is finally given by\n",
    "\\begin{equation}\n",
    "s\\cdot B = s\\cdot\\sum_{j=1}^{m}\\frac{1}{2}(\\tau_j - \\tau_{j-1})(\\mathbb{E}_{j-1}-\\mathbb{E}_j)D(\\frac{\\tau_{j-1}+\\tau_j}{2})\n",
    "\\label{eq:B}\n",
    "\\end{equation}\n",
    "\n",
    "The value of the tranche, valued from the point of view of the protection buyer is $C-sA-sB$. The breakeven spread \n",
    "on the tranche occurs when the present value of the payments equals the present value of the payoffs so\n",
    "\n",
    "$$ s = \\cfrac{C}{A+B}$$\n",
    "\n",
    "Suppose that the tranche under consideration covers losses on the portfolio between $\\alpha_L$ and $\\alpha_H$ and define\n",
    "\n",
    "$$n_L = \\cfrac{\\alpha_L n}{1-R}\\qquad \\mathrm{and}\\qquad n_H = \\cfrac{\\alpha_H n}{1-R}$$\n",
    "where $R$ is the recovery rate. Finally define $m(x)$ as the smallest integer greater than $x$.\n",
    "By definition the tranche principal stays $N$ while the number of defaults $k$ is less than $m(n_L)$, it is zero when the number of default is greater or equal to $m(n_H)$, otherwise is\n",
    "\n",
    "$$\\cfrac{\\alpha_H -k(1-R)/n}{\\alpha_H - \\alpha_L}$$\n",
    "\n",
    "The expected tranche principal at time $\\tau_j$ conditional of the value of the factor $M$ is\n",
    "$$\\begin{equation}\n",
    "\\mathbb{E}_j(M) = \\sum_{k=0}^{m(n_L)-1}\\mathbb{P}(k, \\tau_j|M) + \\sum_{k=m(n_L)}^{m(n_H)-1}\\mathbb{P}(k, \\tau_j|M) \\cfrac{\\alpha_H -k(1-R)/n}{\\alpha_H - \\alpha_L}\n",
    "\\label{eq:E}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "To compute the breakeven spread it is necessary to substitute Eq.~\\ref{eq:E} into Eq.~\\ref{eq:A},~\\ref{eq:B} and~\\ref{eq:C}\n",
    "and we need to integrate the result over the variable $M$ (remember that has a standard normal distribution). \n",
    "The integration is quite complicated and is best accomplished with a technique called *Gaussian quadrature* which exploits the approximation\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty}{\\cfrac{1}{\\sqrt{2}}e^{-M^{2}/2}g(M)dM} \\approx \\sum_{k=1}^{k=L}w_k g(F_k)$$\n",
    "as $L$ increases, accuracy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Complex Correlation Structures and the Financial Crisis\n",
    "\n",
    "In the example above we have used the multivariate normal which gave rise to the Gaussian copula.However, we can use other and more complex copulas as well. For example we might want to assume the correlation is non-symmetruc which is useful in quant finance where correlations become very strong during market crashes and returns very negative.\n",
    "\n",
    "Infact, Gaussian copulas are said to have played a key role in the 2008 financial crisis as tail-correlations were severely underestimated. Consider a set of mortgages in CDOs (a particular kind of contract that we are going to see) they are clearly correlated, if one mortgage fails, the likelihood that another failing is increased. In the early 2000s, the banks only knew how to model the marginals of the default rates. An (in)famous paper by Li then suggested to use copulas to model the correlations between those marginals. Rating agencies relied on thid model so heaviy, severely underestimating risk and giving false ratings...\n",
    "\n",
    "If you are interested in the argument read \\href{http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf}{this paper} for an excellent description of Gaussian copulas and the Financial Crisis which argues that different copula choices would not have made a difference but instead the assumed correlation was way too low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
