{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collateralized Debt Obligation\n",
    "\n",
    "A Collateralized Debt Obligation (CDO) is a credit derivative where the issuer, typically investment banks, gather risky assets and repackage them into discrete classes (*tranches*) based on the level of credit risk assumed by the investor. These tranches of securities become the final investment product.\n",
    "\n",
    "Tranches are named to reflect their risk profile: senior, mezzanine and subordinated/equity and are delimited by the attachment ($L$) and detachment points ($U$), which represent the percentages of the total principal defining their boundaries. \n",
    "For example, a 5-10% tranche has an attachment point of 5% and a detachment point of 10%. When the accumulated loss of the reference pool is no more than 5% of the total initial notional of the pool, the tranche will not be affected. However, when the loss has exceeded 5%, any further loss will be deducted from the tranche's notional until the detachment point, 10%, is reached.\n",
    "\n",
    "Each of these tranches has a different level of seniority relative to the others in the sense that a senior tranche has coupon\n",
    "and principal payment priority over a mezzanine tranche, while a mezzanine tranche has\n",
    "coupon and principal payment priority over an equity tranche. \n",
    "Indeed they receive returns using a set of rules known as *waterfall*. Incomes of the portfolio are first used to provide returns to the most senior tranche, then to the next and so on.\n",
    "So the senior tranches are generally safest because they have the first claim on the collateral, although they'll offer lower coupon rates.\n",
    "\n",
    "<img src=\"cdo_structure.png\">\n",
    "\n",
    "It is important to note\n",
    "that a CDO only redistributes the total risk associated with the underlying pool of assets\n",
    "to the priority ordered tranches. It neither reduces nor increases the total risk associated\n",
    "with the pool.\n",
    "\n",
    "There are various kind of CDOs:\n",
    "\n",
    "* in a **Cash CDO** the reference portfolio consists of corporate bonds owned by the CDO issuer. Cash flows from collateral are used to pay principal and interest to investors. If such cash flows prove inadequate, principal and interest is paid to tranches according to seniority. The equity tranche is usually kept by the issuer being the riskier but also the more rewarded.\n",
    "* in a **Synthetic CDO** the underlying reference portfolio is no longer a physical portfolio of bonds or loans, instead it is a *fictitious* portfolio consisting of a number of names each with an associated notional amount. The value of a synthetic CDO usually comes from insurance premiums of credit default swaps paid for by investors. The seller assumes the underlying assets will perform. The investor, on the other hand, assumes the underlying assets will default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cash CDO Expected Losses\n",
    "\n",
    "Consider a Cash CDO with a maturity of 1 year, made of 125 bonds. Each bond pays a coupon of one unit after 1 year and it has not yet defaulted (the recovery rate $R$ is assumed 0). We are interested in the following three tranches: equity ([0, 3] defaults), mezzanine ([4, 6] defaults) and senior ([7, 9] defaults), (note that now tranches are identified through the number of defaults and not percentages of the principal). \n",
    "\n",
    "<img src=\"ex_cdo_1.png\">\n",
    "\n",
    "We also assume that the probability of default within 1 year are identical for each bond ($Q$) and that the correlation between each pair is also identical and equal to $\\rho$.\n",
    "\n",
    "Under these assumptions we are in the position to use the Gaussian Copula Model and the derivation of the expected losses results quite simple.\n",
    "\n",
    "The probability of having $l$ defaults, conditional to the market parameter $M$ will follow a binomial distribution given by\n",
    "\n",
    "$$p(l|M) = \\binom{N}{l}Q_M^l (1-Q_M)^{N-l}$$\n",
    "\n",
    "where $N$ is the number of bonds in the portfolio and \n",
    "\n",
    "$$Q_M = \\Phi\\left(\\cfrac{\\Phi^{-1}(Q)-\\sqrt{\\rho}M}{\\sqrt{1-\\rho}}\\right)$$\n",
    "where $\\Phi$ is the standard normal CDF and $Q$ the probability of default within 1 year of a single name.\n",
    "\n",
    "From the definition of each tranche with have that the expected losses are\n",
    "\n",
    "* $\\mathbb{E}(\\textrm{equity loss})=3\\cdot\\mathbb{P}(l\\ge 3) + \\sum_{k=1}^{2}{k\\cdot\\mathbb{P}(l=k)}$\n",
    "* $\\mathbb{E}(\\textrm{mezzanine loss})=3\\cdot\\mathbb{P}(l\\ge 6) + \\sum_{k=1}^{2}{k\\cdot\\mathbb{P}(l=k+3)}$\n",
    "* $\\mathbb{E}(\\textrm{senior loss})=3\\cdot\\mathbb{P}(l\\ge 9) + \\sum_{k=1}^{2}{k\\cdot\\mathbb{P}(l=k+6)}$\n",
    "\n",
    "Each probability $\\mathbb{P}$ can be calculated by integrating the above with respect to $M$.\n",
    "\n",
    "Let's see the corresponding $\\tt{python}$ implementation.\n",
    "First we import the necessary modules and define the needed constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom, norm \n",
    "from scipy.integrate import quad \n",
    "import numpy as np\n",
    "\n",
    "N = 125\n",
    "C=1\n",
    "R=0\n",
    "q = 0.02\n",
    "tranches = [[1,3],[4, 6],[7,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we define a function $\\tt{p}$ which implements the expected losses for each tranche.\n",
    "The function depends on the parameter $\\tt{M}$, and takes as inputs the correlation $\\tt{rho}$ and the tranche attach-detach limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(M, rho, lims):\n",
    "    qM = norm.cdf((norm.ppf(q)-np.sqrt(rho)*M)/(np.sqrt(1-rho))) \n",
    "    pN = binom(N, qM)\n",
    "    prob = 3*(pN.cdf(N) - pN.cdf(lims[1]-1))\n",
    "    for i in range(lims[0], lims[1]):\n",
    "        index = i-lims[0]+1\n",
    "        prob += index*pN.pmf(i) \n",
    "    return norm.pdf(M)*prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we loop over a range of possible values for the correlation on each tranche to draw the plot of the expected losses vs the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[],[],[]]\n",
    "for i in range(len(tranches)):\n",
    "    for rho in np.arange(0, 1.05, 0.05): \n",
    "        if rho == 1.0:\n",
    "            rho = 0.99\n",
    "        v = quad(p, -np.inf, np.inf, args=(rho, tranches[i])) \n",
    "    res[i].append(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations can be done from these results. First of all, as expected, the equity tranche is the riskier, producing the highest level of loss. The \n",
    "$$\n",
    "\\mathbb{E}(\\mathrm{equity})\\ge \\mathbb{E}(\\mathrm{mezzanine}) \\ge \\mathbb{E}(\\mathrm{senior})\n",
    "$$ \n",
    "relation holds only if each tranche has the same notional exposure (in our example 3).\n",
    "\n",
    "Then we can notice that in the equity tranche losses are decreasing in $\\rho$. When the correlation is low indeed the probability to have few defaults is higher than that of many. As the correlation increases, there will be more and more \"simultaneous\" defaults so also other tranches start to suffer losses. In the extreme case of correlation equal to 1 all the tranches are the same (indeed the expected losses curves join together). \n",
    "\n",
    "When considering all the tranches covering the entire number of names, the last tranche (the one with detachment point of 100\\%) is always increasing in $\\rho$. Again this can be explained with the correlated defaults. \n",
    "Also, the total expected losses on the three tranches is independent of $\\rho$. This is not an accident but it is due to the fact that every default scenario is now categorized in one of the plotted tranches while before this was not the case.\n",
    "\n",
    "<img src=\"losses_vs_rho.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic CDO Valuation\n",
    "\n",
    "Imagine a CDO made of $N$ names in the reference portfolio. Each name has a notional amount $F$.\n",
    "When the $i^{th}$ name defaults, then the portfolio incurs in a loss of $F(1-R)$ (the recovery rate is assumed to be fixed for all entities of the portfolio).\n",
    "\n",
    "The tranche loss function $TL^{L,U}(l)$ for a given time $t$ is a function of the number of defaults $l$ occurred up to that time and is given by\n",
    "\n",
    "$$TL_{t}^{L,U}=\\mathrm{max}(\\mathrm{min}(lF(1-R), U)-L, 0)$$\n",
    "where $lF(1-R)$ is the total portfolio loss, if it is greater than $U$ then the tranche loss is $U$. Conversely if it is lower than $L$ there is no loss.\n",
    "\n",
    "So for example suppose $L=3\\%$ and $U=7\\%$ and suppose also that the portfolio loss is $lF(1-R)=5\\%$. Then the tranche loss is 2\\% of the total portfolio notional (or 50\\% of the tranche notional $=7\\%-3\\%=4\\%$).\n",
    "\n",
    "When an investor *sells protection* on a tranche she is guaranteeing to reimburse any realized losses on the tranche to the *protection buyer*. To better understand this concept it is useful to think of the protection as an *insurance*. \n",
    "\n",
    "In return, the protection seller receives a premium at regular intervals (typically every three months) from the protection buyer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premium Leg\n",
    "As seen above the premium leg represents the payments that are done periodically by the protection buyer to the protection seller.\n",
    "\n",
    "These payments are made at the end of each time interval and are proportional to the **remaining notional** in the tranche (this is an important difference with respect to CDS, where the contract ends as soon as a default occurs).\n",
    "\n",
    "We can then write the NPV of the premium leg as\n",
    "\n",
    "$$\\mathrm{NPV}_{\\mathrm{premium}}^{L,U}=S\\sum^{n}_{i=1}D(d_i)\\cfrac{(d_i - d_{i-1})}{360}\\left((U-L)-\\mathbb{E}[TL_{d-1}^{L,U}]\\right)$$\n",
    "where $n$ is the number of payment dates, $D(d_i)$ is the discount factor, $S$ is the annualized premium. The expected value represents the expected notional remaining in the tranche at time \n",
    "$d_{i-1}$.\n",
    "Note that for simplicity we are ignoring that the default may take place at any time between each payment date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Leg\n",
    "The default leg represents the cash flows paid to the protection buyer upon losses occurring in the considered tranche. \n",
    "\n",
    "The NPV of the leg can be expressed as\n",
    "$$\\mathrm{NPV}_{\\mathrm{default}}^{L,U}=\\sum_{i=1}^{n}D(d_i)\\left(\\mathbb{E}[TL_{d_i}^{L,U}]-\\mathbb{E}[TL_{d_{i-1}}^{L,U}]\\right)$$\n",
    "where the argument in parenthesis is the expected losses between time $d_{i-1}$ up to $d_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the key ingredient for the valuation of a CDO is the calculation of $\\mathbb{E}[TL_{d_i}^{L,U}]$ which appears in both legs.\n",
    "Using the Gaussian copula it is relatively easy to compute it. \n",
    "Indeed we know that \n",
    "\n",
    "$$TL_{t}^{L,U}=\\mathrm{max}(\\mathrm{min}(lF(1-R), U)-L, 0)$$\n",
    "\n",
    "where the only random variable is the number of defaults $l$. We also know that \n",
    "\n",
    "$$\\mathbb{E}[TL_{t}^{L,U}] = \\sum_{l=0}^{N}TL_{t}^{L,U}\\cdot \\int_{-\\infty}^{\\infty} DP(l_{t|M}=j) \\phi(M)dM$$\n",
    "\n",
    "And has we have already seen this calculation can be carried on without too much effort.\n",
    "The large popularity of the Gaussian copula just resides in this, it allows to compute very quickly very complicated contracts like CDOs which usually involve a large number of correlated names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDO Fair Value\n",
    "The *fair value* of a CDO tranche is that value of the premium $S^*$ for which the expected value of the premium leg equals the expected value of the default leg and for what we have seen depends on the expected value of the tranche loss function.\n",
    "\n",
    "$$ S^* = \\cfrac{\\mathrm{NPV_{default}}^{L,U}}{\\sum^{n}_{i=1}D(d_i)\\cfrac{(d_i - d_{i-1})}{360}\\left((U-L)-\\mathbb{E}[TL_{d-1}^{L,U}]\\right)}$$\n",
    "\n",
    "This equation defines the CDO fair value, but can also be used to calibrate the implied correlation parameter from the market.\n",
    "This can be obtained by plugging into the equation the market premium value and solve for the correlation parameter $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finmarkets_tot import DiscountCurve, CreditCurve, generate_swap_dates\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm, binom\n",
    "import numpy as np\n",
    "from numpy import exp, sqrt\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "class CollDebtObligation:\n",
    "    def __init__(self, notional, names, tranches, rho, cc,\n",
    "                 start_date, spreads,\n",
    "                 maturity, tenor=3, recovery=0.4):\n",
    "        self.notional = notional\n",
    "        self.names = names\n",
    "        self.tranches = tranches\n",
    "        self.payment_dates = generate_swap_dates(start_date, maturity * 12, tenor)\n",
    "        self.spreads = spreads\n",
    "        self.rho = rho\n",
    "        self.recovery = recovery\n",
    "        self.cc = cc\n",
    "\n",
    "    def expected_tranche_loss(self, d, L, U):\n",
    "        def func(M, Q, l, L, U):\n",
    "            P = norm.cdf((norm.ppf(Q) - sqrt(self.rho) * M) / (sqrt(1 - self.rho)))\n",
    "            b = binom(self.names, P)\n",
    "            return b.pmf(l) * norm.pdf(M) * max(min(l/self.names * \n",
    "                                                    self.notional * \n",
    "                                                    (1 - self.recovery), U) - L, 0)\n",
    "        Q = 1 - self.cc.ndp(d)\n",
    "        v = 0\n",
    "        for l in range(self.names+1):\n",
    "            i = quad(func, -np.inf, np.inf, args=(Q, l, L, U))[0]\n",
    "            v += i\n",
    "        return v\n",
    "\n",
    "    def npv_premium(self, tranche, dc):\n",
    "        L = self.tranches[tranche][0] * self.notional\n",
    "        U = self.tranches[tranche][1] * self.notional\n",
    "        v = 0\n",
    "        for i in range(1, len(self.payment_dates)):\n",
    "            ds = self.payment_dates[i - 1]\n",
    "            de = self.payment_dates[i]\n",
    "            D = dc.df(de)\n",
    "            ETL = self.expected_tranche_loss(ds, L, U)\n",
    "            v += D * (de - ds).days / 360 * max((U - L) - ETL, 0)\n",
    "        return v * self.spreads[tranche]\n",
    "\n",
    "    def npv_default(self, tranche, dc):\n",
    "        U = self.tranches[tranche][1] * self.notional\n",
    "        L = self.tranches[tranche][0] * self.notional\n",
    "        v = 0\n",
    "        for i in range(1, len(self.payment_dates)):\n",
    "            ds = self.payment_dates[i - 1]\n",
    "            de = self.payment_dates[i]\n",
    "            ETL1 = self.expected_tranche_loss(ds, L, U)\n",
    "            ETL2 = self.expected_tranche_loss(de, L, U)\n",
    "            v += dc.df(de) * (ETL2 - ETL1)\n",
    "        return v\n",
    "\n",
    "    def npv(self, tranche, dc):\n",
    "        return self.npv_default(tranche, dc) - self.npv_premium(tranche, dc)\n",
    "\n",
    "    def fair_value(self, tranche, dc):\n",
    "        num = self.npv_default(tranche, dc)\n",
    "        den = self.npv_premium(tranche, dc) / self.spreads[tranche]\n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranche 0 ([0.0, 0.03]): 0.15942\n",
      "Tranche 1 ([0.03, 0.06]): 0.02505\n",
      "Tranche 2 ([0.06, 0.09]): 0.00773\n",
      "Tranche 3 ([0.09, 1.0]): 0.00017\n"
     ]
    }
   ],
   "source": [
    "pillar_dates = []\n",
    "df = []\n",
    "observation_date = date.today()\n",
    "\n",
    "for i in range(2):\n",
    "    pillar_dates.append(observation_date + relativedelta(years=i))\n",
    "    df.append(1 / (1 + 0.05) ** i)\n",
    "dc = DiscountCurve(observation_date, pillar_dates, df)\n",
    "\n",
    "cc = CreditCurve([observation_date + relativedelta(years=i) for i in range(5)],\n",
    "                 [1, 0.99, 0.97, 0.95, 0.93])\n",
    "\n",
    "tranches = [[0.0, 0.03], [0.03, 0.06], [0.06, 0.09], [0.09, 1.0]]\n",
    "spreads = [0.15, 0.07, 0.03, 0.01]\n",
    "\n",
    "cdo = CollDebtObligation(100e6, 125, tranches, 0.3, cc,\n",
    "                         observation_date, spreads, 1, 12)\n",
    "for i in range(len(tranches)):\n",
    "    print (\"Tranche {} ({}): {:.5f}\".format(i, tranches[i], cdo.fair_value(i, dc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaR and Credit Risk\n",
    "## Value at Risk \n",
    "\n",
    "The value at risk (VaR) of a portfolio is a function of two parameters (time horizon and confidence level) and it is usually involved when it is important to know to a certain precentage of confidence ($X$) how much will be the maximum loss\n",
    "in the next $N $days. It can be interpreted as the loss level over $N$ days that has a probability of only $(100 - X)\\%$ of being exceeded.\n",
    "\n",
    "Mathematically the VaR is the loss corresponding to the $(100-X)\\textrm{th}$ precentile of the distribution of the change in the value of the portfolio over the next $N$ days.\n",
    "For example, with $N=1$ and $X=95$, VaR is the fifth percentile of the distribution of changes in the value of the portfolio over the next day (e.g. in the next picture the graphical representation of the VaR assuming a normal distribution for the changes of value).\n",
    "\n",
    "<img src=\"normal_curve.png\">\n",
    "\n",
    "VaR is useful to summarize all the information about the risk of a portfolio in one single number, but this can be also considered its main limitation (too much simplification).\n",
    "\n",
    "Concerning the time horizon parameter it is usually set to $N=1$ since it is not easy to estimate market variables over periods longer than 1 day. To generalize the VaR estimate it is assumed:\n",
    "\n",
    "$$\\textrm{N-day VaR} = \\textrm{1-day VaR}\\times \\sqrt{N}$$\n",
    "\n",
    "This relation is true only if the daily change of the portfolio value over the considered period of time has normal distribution with mean 0 (otherwise it is just an approximation).\n",
    "\n",
    "## How to Estimate the VaR\n",
    "\n",
    "In the following examples we are going to use market data collected in [historical_data.csv](https://drive.google.com/file/d/1pxzLjR_dsFdF3vildeh6vbmZJQmS6Qx7/view?usp=sharing). As usual we can inspect and load this file with $\\tt{pandas}$. Also beside the three existing columns (date, ticker and closing price) we are going to add the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date ticker  adj_close      rets\n",
      "4201  2018-03-27   AAPL    168.340       NaN\n",
      "4202  2018-03-26   AAPL    172.770  0.026316\n",
      "4203  2018-03-23   AAPL    164.940 -0.045320\n",
      "4204  2018-03-22   AAPL    168.845  0.023675\n",
      "4205  2018-03-21   AAPL    171.270  0.014362\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "w = np.array([0.6, 0.4])\n",
    "df = pd.read_csv(\"historical_data.csv\")\n",
    "\n",
    "aapl = df[df['ticker']==\"AAPL\"].copy() \n",
    "nflx = df[df['ticker']==\"NFLX\"].copy()\n",
    "\n",
    "aapl['rets'] = aapl['adj_close']/aapl['adj_close'].shift(1) - 1 \n",
    "nflx['rets'] = nflx['adj_close']/nflx['adj_close'].shift(1) - 1\n",
    "\n",
    "print (aapl.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will **assume to have a portfolio made of 60% of AAPL and 40% NFLX stocks**.\n",
    "\n",
    "### Historical Simulation\n",
    "In order to estimate the VaR from an historical series, we need to collect the market variables affecting the portfolio over the last $N$ days (with $N$ quite large). \n",
    "\n",
    "The variation over each day in our time interval will provide different scenarios to be applied to today's market simulation so that for each of them we need to compute the variation in the portfolio value ($\\Delta P$). Our VaR estimate will be the (100 - X)% percentile of the resulting distribution.\n",
    "Given the 1-day VaR it is then possible to determine the N-day VaR using the above formula.\n",
    "\n",
    "Of course such historical simulation relies on the assumption that past behaviors are indicative of what might happen in the future.\n",
    "\n",
    "#### Example\n",
    "Imagine a portfolio $P$ whose value depends only on two market variables ($x_1(t) , x_2(t)$). From the historical series of the market variables we can determine various *simulated* portfolio values:\n",
    "\n",
    "$$P_i(t_n+1) = P\\Big(x_1(t_n)\\frac{x_1(t_i)}{x_1(t_{i-1})} , x_2(t_n)\\frac{x_2(t_i)}{x_2(t_{i-1})}\\Big)$$\n",
    "\n",
    "Essentially rescaling the market variables according to the variation between day $i$ and $i-1$ we can draw a distribution of the possible changes in the portfolio value $P_i$ and then compute the VaR taking the appropriate percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical VAR is -2.412\n"
     ]
    }
   ],
   "source": [
    "# historical VaR\n",
    "from scipy.stats import norm\n",
    "\n",
    "rets = []\n",
    "for i in range(1, len(aapl)):\n",
    "    rets.append(w[0]*aapl.iloc[i]['rets'] + w[1]*nflx.loc[i]['rets'])\n",
    "\n",
    "price = [aapl.iloc[-1]['adj_close'], nflx.iloc[-1]['adj_close']]\n",
    "\n",
    "portfolio_price = w.dot(price)\n",
    "\n",
    "hist_var = portfolio_price*np.percentile(rets, 1)\n",
    "print ('Historical VAR is {:.3f}'.format(hist_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"historical_var.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation\n",
    "A very useful alternative to the previous approach is using a Monte Carlo simulation to generate the probability distribution of the $\\Delta P$.\n",
    "\n",
    "Imagine we need to compute the 1-day VaR for our example portfolio, the simulation can be done either generating random returns from a distribution with mean and standard deviation obtained from the historical data of each stock, or by simulating the evolution of all the portfolio market variables in one day.\n",
    "\n",
    "Let’s start from the first case: computing mean and standard deviation of each historical data-set. We will then throw various simulated returns from a multivariate Gaussian with such means and variances. One useful aspect of this method is that in principal other distribution could be used instead Gaussians. Once we have the distribution of the returns the VaR can be computed as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated VAR is -2.197\n"
     ]
    }
   ],
   "source": [
    "# MC simulated VaR 1\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mean = [np.mean(aapl['rets']), np.mean(nflx['rets'])]\n",
    "cov = np.cov(aapl['rets'][1:], nflx['rets'][1:-1])\n",
    "\n",
    "mvnorm = multivariate_normal(mean=mean, cov=cov)\n",
    "\n",
    "np.random.seed(1)\n",
    "n_sims = 100000\n",
    "sim_returns = mvnorm.rvs(n_sims)\n",
    "p_returns = [w.dot(s) for s in sim_returns]\n",
    "mc_var = portfolio_price * np.percentile(p_returns, 1)\n",
    "print('Simulated VAR is {:.3f}'.format(mc_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='sim1_var.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result can be compared with the VaR estimated with a simulation of the daily evolution of the stock price. We will use the log-normal evolution described in one of the earlier lessons where $\\mu$ and $\\sigma$ are the mean and variance estimated from the historical series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated VAR is -1.913\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import normal\n",
    "from numpy import exp, sqrt\n",
    "\n",
    "T = 1\n",
    "trials = 100000\n",
    "dP = []\n",
    "\n",
    "for _ in range(trials):\n",
    "    s = 0\n",
    "    for i in range(2):\n",
    "        s += w[i] * price[i] * exp((mean[i] - 0.5 * cov[i][i]) * T + \n",
    "                                   sqrt(cov[i][i]) * sqrt(T) * normal())\n",
    "    dP.append(portfolio_price - s)\n",
    "    \n",
    "mc_var2 = np.percentile(dP, 1)\n",
    "print('Simulated VAR is {:.3f}'.format(mc_var2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sim2_var.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stress Testing and Back Testing\n",
    "In addition to calculating VaR, it can be useful to perform a *stress test*. This essentially implies to estimate how a portfolio would behave under the most extreme market moves seen in the past years.\n",
    "\n",
    "From the historical series we tak the market variables seen in particular days with exceptional large variations with the idea to consider extreme events that can occur in reality more frequently than in simulations despite their low probability (e.g. a 5-standard deviation move should happen once every 7000 years but in practice can be observed twice over 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect one 5-sigma event every 6922 years\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "prob = norm.cdf(-5) * 2 # since I'm fine with +- 5sigma movements\n",
    "nyears = (1/prob) / 252 # number of days for 1 event / working days \n",
    "print (\"Expect one 5-sigma event every {:.0f} years\".format(nyears))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important check that could be done is the so-called back testing which consists of checking how well the VaR estimate would have performed in the past. Basically it has to be tested how often the daily loss exceeded the N-days X% VaR just computed. If it happens on about (100-X)% of the times we can be confident that our estimate is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit VaR\n",
    "\n",
    "Credit VaR is defined in the usual way Value at Risk measures are defined (i.e. as percentile of a loss distribution). \n",
    "\n",
    "In this case we are concerned with the default risk associated to one or multiple counterparties in a specific portfolio, and the loss is defined on the overall exposure to all the counter-parties involved.\n",
    "\n",
    "The exposure $EE$ at the default date $\\tau$ is defined as the sum of the discounted cash flows; the corresponding loss is then given by:\n",
    "$$ L(\\tau, \\hat{T}, T) = (1 − R)\\cdot EE(\\tau)$$\n",
    "where $\\hat{T}$ is the risk horizon and $L$ is non-zero only in scenarios of early default of the counter-party. \n",
    "\n",
    "Given the above definitions we can express the Cr-VaR as the q-quantile of $L(\\tau, \\hat{T}, T)$.\n",
    "In this case the horizon is usually one year and the standard percentile is the 99.9th, so that the returned loss is exceeded only in 1 case out of 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Var and MC Simulation\n",
    "Credit VaR can be calculated through a simulation of the basic financial variables underlying the portfolio up to the risk horizon. The simulation also includes the default of the counter-parties.\n",
    "\n",
    "At the risk horizon, the portfolio is priced in every simulated scenario of the basic financial variables, including defaults, obtaining a number of scenarios for the portfolio value at the risk horizon.\n",
    "\n",
    "It is then straightforward to derive the Credit VaR from the distribution of the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from finmarkets import DiscountCurve, CreditCurve, generate_swap_dates\n",
    "from scipy.stats import uniform\n",
    "\n",
    "bonds = 20\n",
    "S = [1-0.08 for _ in range(bonds)]\n",
    "N = [100 for _ in range(20)]\n",
    "R = 0.4\n",
    "r = 0.01\n",
    "obs_date = date.today()\n",
    "\n",
    "pillars = [obs_date+relativedelta(years=i) for i in range(2)]\n",
    "dfs = [1/(1+r)**i for i in range(2)]\n",
    "dc = DiscountCurve(obs_date, pillars, dfs)\n",
    "ccs = []\n",
    "for i in range(bonds):\n",
    "    ccs.append(CreditCurve(pillars, [1, S[i]]))\n",
    "\n",
    "scenarios = 100000\n",
    "losses = []\n",
    "for _ in range(scenarios):\n",
    "    loss = 0\n",
    "    unif = uniform.rvs(size=bonds)\n",
    "    for i in range(bonds):\n",
    "        if unif[i] > ccs[i].ndp(pillars[-1]):\n",
    "            loss += (1 - R)*N[i]*dc.df(pillars[-1]) \n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[415.84158416]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.percentile(losses, [99.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"credit_var_zcb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be now clear that all the consideration done previously on correlated defaults have an impact also here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit VaR and One Factor Copula Model\n",
    "\n",
    "Consider a portfolio made of similar assets. As an approximation assume that the probability of default is the same for each counter-party and that the correlation between each pair is the same and equal to $\\rho$. \n",
    "\n",
    "If we use the One Factor Copula model to describe the default correlations\n",
    "\n",
    "$$ DP=\\Phi\\left(\\cfrac{\\Phi^{-1}[Q(T)]-M\\sqrt{\\rho}}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "gives us the percentage of defaults by time $T$ given the parameter $M$. \n",
    "Indeed if you a have $n$ counter-parties with the same default probability $DP(t)$ the **percentage of defaults** at time $t$ is $DP$ itself\n",
    "\n",
    "$$\\textrm{% of defaults} = \\textrm{nDefaults}/n = (n\\cdot DP)/n$$\n",
    "\n",
    "Since $M$ is distributed according to a standard normal we can be $X\\%$ certain that its value will be greater than $\\Phi^{-1}(1−X) = -\\Phi^{-1}(X)$, where the last equality holds due to the symmetry of the Gaussian distribution.\n",
    "\n",
    "<img src=\"certain_for_X.png\">\n",
    "\n",
    "But once $T$ has been fixed the only random variable in $DP$ is $M$, therefore we can say that we are $X\\%$ certain that the percentage of losses over $T$ years on a large portfolio will be less than $V(X, T)$ where\n",
    "\n",
    "$$ V(X, T)=\\Phi\\left(\\cfrac{\\Phi^{-1}[Q(T)]+\\Phi^{-1}(X)\\sqrt{\\rho}}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "When $X\\%$ confidence level is used and the time horizon is $T$, a rough estimate of the Credit VaR is therefore $P ( 1 − R ) V ( X, T )$ , where $P$ is the size of the portfolio and $R$ is the recovery rate.\n",
    "\n",
    "Suppose that a bank has a total of €100 million of retail exposures. \n",
    "The 1-year probability of default averages to 2% and the recovery rate averages to 60%. The copula correlation parameter is estimated as 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr-VaR: 5130000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from math import sqrt\n",
    "\n",
    "X = 0.999\n",
    "rho = 0.1\n",
    "R = 0.6\n",
    "DP = 0.02\n",
    "exposure = 100e6\n",
    "\n",
    "num = norm.ppf(DP) + sqrt(rho)*norm.ppf(X)\n",
    "den = sqrt(1-rho)\n",
    "V = norm.cdf(num/den)\n",
    "cr_var = exposure*V*(1-R)\n",
    "\n",
    "print (\"Cr-VaR: {:.0f}\".format(round(cr_var, -4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CreditMetrics\n",
    "\n",
    "Another popular approach to compute Credti VaR is CreditMetrics. It involves estimating a probability distribution of credit losses by carrying out Monte Carlo simulations of the credit rating changes of all counter-parties.\n",
    "\n",
    "Imagine we would like to determine the probability distribution of losses over 1-year period.\n",
    "On each simulation, the credit rating changes and default of each counter-party is computed.\n",
    "\n",
    "The portfolio value is than computed to determine the eventual losses.\n",
    "\n",
    "<img src=\"credit_metrics_table.png\">\n",
    "\n",
    "Clearly credit rate changes cannot be assumed independent, hence a copula approach can be\n",
    "implemented also here. As an example suppose to simulate rating change of a portfolio of 9 bonds with various ratings over 1-year period. The correlation between them is 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal, norm\n",
    "import numpy\n",
    "\n",
    "# AAA, AA, A, BBB, BB, B, CCC, Def\n",
    "table = [[90.81, 8.33, 0.68, 0.06, 0.08, 0.02, 0.01, 0.01],\n",
    "         [0.70, 90.65, 7.79, 0.64, 0.06, 0.13, 0.02, 0.01],\n",
    "         [0.09, 2.27, 91.05, 5.52, 0.74, 0.26, 0.01, 0.06],\n",
    "         [0.02, 0.33, 5.95, 85.93, 5.30, 1.17, 1.12, 0.18],\n",
    "         [0.03, 0.14, 0.67, 7.73, 80.53, 8.84, 1.00, 1.06],\n",
    "         [0.01, 0.11, 0.24, 0.43, 6.48, 83.46, 4.07, 5.20],\n",
    "         [0.21, 0, 0.22, 1.30, 2.38, 11.24, 64.86, 19.79]]\n",
    "\n",
    "#able_gauss = []\n",
    "#for i in range(len(table)):\n",
    "#    temp = []\n",
    "#    s = 0\n",
    "#    for j in range(8):\n",
    "#        s += table[i][j]/100\n",
    "#        if s>1:\n",
    "#            s = 1\n",
    "#        temp.append(norm.ppf(s))\n",
    "#    table_gauss.append(temp)\n",
    "\n",
    "t = numpy.array(table)\n",
    "table_gauss = norm.ppf(np.cumsum(t/100., axis=1))\n",
    "table_gauss[:, -1] = np.inf\n",
    "\n",
    "N = [100, 95, 92, 85, 80, 70, 60]\n",
    "portfolio = [2, 3, 3, 4, 5, 6, 3, 4, 2]\n",
    "R = 0.4\n",
    "\n",
    "p0 = 0\n",
    "for i in portfolio:\n",
    "    p0 += N[i]\n",
    "\n",
    "numpy.random.seed(1)\n",
    "mvnorm = multivariate_normal(mean=[0 for _ in range(9)],\n",
    "                             cov=[[1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 1, 0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 0.2, 1, 0.2, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 0.2, 0.2, 1, 0.2, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1, 0.2, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1, 0.2],\n",
    "                                  [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1]])\n",
    "\n",
    "trials = 1000000\n",
    "x_prob = mvnorm.rvs(size=trials)\n",
    "\n",
    "dp = []\n",
    "for x in x_prob:\n",
    "    p = 0\n",
    "    for j in range(len(portfolio)):\n",
    "        ip = 0\n",
    "        while x[j] > table_gauss[portfolio[j], ip]:\n",
    "            ip += 1\n",
    "        if ip == 7:\n",
    "            p += N[portfolio[j]]*(1-R)\n",
    "        else:\n",
    "            p += N[ip]\n",
    "\n",
    "    r = max(0, -(p - p0))\n",
    "    if r != 0:\n",
    "        dp.append(r)\n",
    "        \n",
    "crvar = numpy.percentile(dp, [99.9])\n",
    "print (crvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"credit_metrics.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Valuation Adjustment\n",
    "\n",
    "Suppose you have a portfolio of derivatives with a counter-party. If the counter-party defaults and the present value of the portfolio at default is positive to the surviving party, then the surviving party only gets a recovery fraction of the portfolio value from the defaulted entity. \n",
    "\n",
    "If however the present value is negative to the surviving party, it has to pay it in full to the liquidators of the defaulted entity. This creates and asymmetry that, once one has done all calculations, says that the value of the deal under counter-party risk is the value without counter-party risk minus a positive adjustment, called Credit Valuation Adjustment (CVA).\n",
    "\n",
    "It can be expressed in the following way:\n",
    "\n",
    "$$\\textrm{CVA} = ( 1 − R )\\int^{T}_{0} D(t)\\cdot EE(t) dP(t) $$\n",
    "\n",
    "where $T$ is the latest maturity in the portfolio, $D$ is the discount factor, $EE$ is the expected exposure or $\\mathbb{E}[ \\textrm{max(0, NPV portfolio)}]$. \n",
    "\n",
    "For an easier computation it is natural to discretize the above integral and use a time grid going from 0 to the maturity of the portfolio:\n",
    "\n",
    "$$\\textrm{CVA} = ( 1 − R ) \\sum_i D(t_i)\\cdot EE(t_i) P(t_{i − 1}, t_i)$$\n",
    "\n",
    "Let us say that Credit VaR measures the risk of losses you face due to the possible default of some counter-parties you are having business with. CVA measures the pricing component of this risk, i.e. the adjustment to the price of a product due to this risk.\n",
    "\n",
    "### Debit Valuation Adjustment\n",
    "\n",
    "The adjustment seen from the point of view of our counter-party is positive, and is called Debit Valuation Adjustment, DVA. It is positive because the early default of the client itself would imply a discount on the client payment obligations, and this means a gain in a way. So the client marks a positive adjustment over the risk free price by adding the positive amount called DVA.\n",
    "\n",
    "Basically when both parties have the possibility to default, they consistently include both defaults into the valuation. Hence every party needs to include its own default besides the default of the counter-party into the valuation. So they will mark a positive CVA to be subtracted and a positive DVA to be added to the default risk free price of the deal. The CVA of one party will be the DVA of the other one and viceversa.\n",
    "\n",
    "$$\\textrm{price = default risk free price + DVA - CVA}$$\n",
    "\n",
    "Now, since\n",
    "$$\\textrm{default risk free price(A) = − default risk free price(A)}$$\n",
    "$$\\textrm{DVA(A) = CVA(B)}$$\n",
    "$$\\textrm{DVA(B) = CVA(A)}$$\n",
    "\n",
    "we get that eventually\n",
    "$$\\textrm{price(A) = − price(B)}$$\n",
    "\n",
    "so that both parties agree on the price, or, we could say, there is money conservation.\n",
    "\n",
    "### CVA Computation\n",
    "The computation of the CVA is easily carried on with Monte Carlo simulation. First simulate the development of your derivatives portfolio (its NPV) at each time point for each MC scenario. Then calculate the CVA using Eone of the previous definitions. Finally average the CVA of all the scenarios to get its best estimate.\n",
    "\n",
    "In case of bonds the computation of the CVA can be further simplified. Indeed in this case the exposure of the investor is equal to the notional of the bond, so it is enough to loop through each days from the observation date to the maturity of the bond and compute the CVA. \n",
    "\n",
    "Imagine a 3-years bond with a notional N = €100. The bond provides yearly coupons of 6%.\n",
    "The bond issuer has the following default probabilities 10%, 20% and 30% for 1, 2 and 3 years respectively (the recovery rate is 40%). The risk free rate is 3%.\n",
    "\n",
    "To compute CVA we need to first define a discount curve and the credit curve corresponding\n",
    "to the issuer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVA: 17.21\n",
      "Bond Price: 74.18\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from finmarkets import DiscountCurve, CreditCurve\n",
    "import math\n",
    "\n",
    "T = 3 \n",
    "r = 0.03\n",
    "R = 0.4\n",
    "N = 100\n",
    "\n",
    "obs_date = date.today()\n",
    "pillars = [obs_date+relativedelta(years=i) for i in range(T+1)]\n",
    "dfs = [math.exp(-r*i) for i in range(T+1)]\n",
    "dc = DiscountCurve(obs_date, pillars, dfs)\n",
    "S = [1, 0.9, 0.8, 0.7]\n",
    "cc = CreditCurve(pillars, S)\n",
    "V = N * math.exp(-r*i)\n",
    "\n",
    "cva = 0\n",
    "d = obs_date\n",
    "while d <= pillars[-1]:\n",
    "    cva += dc.df(d)*(cc.ndp(d) - cc.ndp(d+relativedelta(days=1)))\n",
    "    d += relativedelta(days=1)\n",
    "\n",
    "cva *= (1-R) * N\n",
    "print (\"CVA: {:.2f}\".format(cva))\n",
    "print (\"Bond Price: {:.2f}\".format(V - cva))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the recovery payments are done at the maturity date of the zero coupon bond the formula CVA calculation can be simplified further, indeed\n",
    "\n",
    "$$ \\textrm{CVA} = (1-R)\\cdot N \\cdot \\sum_{i=1}^{n} D(d_i)\\cdot Q(d_{i-1}, d_i) $$\n",
    "becomes\n",
    "$$ \\textrm{CVA} = (1-R)\\cdot N\\cdot D(d_n)\\cdot\\sum_{i=1}^{n} Q(d_{i-1}, d_i) = (1-R)\\cdot N\\cdot Q(d_{0}, d_n)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.450761334882106\n"
     ]
    }
   ],
   "source": [
    "print (N*dc.df(pillars[-1])*(1-R)*0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
