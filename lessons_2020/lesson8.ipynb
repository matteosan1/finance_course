{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit VaR\n",
    "\n",
    "Credit VaR is defined in the usual way Value at Risk measures are defined (i.e. as percentile of a loss distribution). \n",
    "\n",
    "In this case we are concerned with the default risk associated to one or multiple counterparties in our portfolio (while previously VaR was concerned with market risk).\n",
    "\n",
    "To compute Credit VaR we need to consider the exposure $EE$ at the default date $\\tau$ defined as the sum of the discounted cash flows at the default; the corresponding loss is then given by:\n",
    "\n",
    "$$ L(\\tau, \\hat{T}) = (1 − R)\\cdot EE(\\tau)$$\n",
    "\n",
    "where $\\hat{T}$ is the risk horizon and $L$ is non-zero only in scenarios of early default of the counter-party. \n",
    "\n",
    "Given the above definitions we can express the Credit VaR as the q-quantile of $L(\\tau, \\hat{T})$.\n",
    "In this case the time horizon considered is usually one year and the chosen standard percentile is the 99.9th, so that the returned loss is exceeded only in 1 case out of 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Var and MC Simulation\n",
    "Credit VaR can be calculated through a simulation of the basic financial variables underlying the portfolio up to the risk horizon and the simulation clearly includes the default of the counter-parties.\n",
    "\n",
    "The portfolio is priced in every simulated scenario at the risk horizon, including defaults, obtaining the distribution of the losses from which it is then straightforward to derive the Credit VaR.\n",
    "\n",
    "Let's compute Credit VaR in a simple case of a portfolio of 20 zero coupon bonds with 8% default probability ($R=40\\%$) and a risk-free rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[356.43564356]\n"
     ]
    }
   ],
   "source": [
    "# Credit VaR of 20 bonds\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from finmarkets import DiscountCurve, CreditCurve\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "nbonds = 20\n",
    "R = 0.4\n",
    "FV = 100\n",
    "r = 0.01\n",
    "obs_date = date.today()\n",
    "pillars = [obs_date + relativedelta(years=i) for i in range(2)]\n",
    "dfs = [1/(1+r)**i for i in range(2)]\n",
    "dc = DiscountCurve(obs_date, pillars, dfs)\n",
    "S = [1, 0.92]\n",
    "cc = CreditCurve(pillars, S)\n",
    "\n",
    "losses = []\n",
    "trials = 10000\n",
    "for _ in range(trials):\n",
    "    x = uniform.rvs(size=nbonds)\n",
    "    loss = 0\n",
    "    for i in range(nbonds):\n",
    "        if x[i] > cc.ndp(pillars[-1]):\n",
    "            loss += (1-R)*dc.df(pillars[-1])*FV\n",
    "    losses.append(loss)\n",
    "    \n",
    "plt.hist(losses,bins=50)\n",
    "plt.show()\n",
    "\n",
    "print (np.percentile(losses, [99.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be at this point clear that all the consideration done previously on correlated defaults have an impact also here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit VaR and One Factor Copula Model\n",
    "\n",
    "Consider a portfolio made of similar assets. As an approximation assume that the probability of default is the same for each counter-party and that the correlation between each pair is the same and equal to $\\rho$. \n",
    "\n",
    "If we use the One Factor Copula model to describe the default correlations we get\n",
    "\n",
    "$$ Q_M(T)=\\Phi\\left(\\cfrac{\\Phi^{-1}[Q(T)]-M\\sqrt{\\rho}}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "But this expression gives us also the percentage of defaults by time $T$ given the parameter $M$. \n",
    "Indeed if you a have $n$ counter-parties with the same default probability $Q_M(T)$ the **percentage of defaults** at time $t$ is $Q_M$ itself\n",
    "\n",
    "$$\\textrm{% of defaults} = \\textrm{nDefaults}/n = (n\\cdot Q_M)/n$$\n",
    "\n",
    "Since $M$ is distributed according to a standard normal we can be $X\\%$ certain that its value will be greater than $\\Phi^{-1}(1−X) = -\\Phi^{-1}(X)$, where the last equality holds due to the symmetry of the Gaussian distribution (e.g. $\\Phi^{-1}(0.95) = -\\Phi^{-1}(0.05))$.\n",
    "\n",
    "<img src=\"certain_for_X.png\">\n",
    "\n",
    "But once $T$ has been fixed the only random variable in $Q_M$ is $M$, therefore we can say that we are $X\\%$ certain that the percentage of defaults over $T$ years on a large portfolio will be **less** than $V(X, T)$ where\n",
    "\n",
    "$$ V(X, T)=\\Phi\\left(\\cfrac{\\Phi^{-1}[Q(T)]-\\hat{m}\\sqrt{\\rho}}{\\sqrt{1-\\rho}}\\right) = \\Phi\\left(\\cfrac{\\Phi^{-1}[Q(T)]+\\Phi^{-1}(X)\\sqrt{\\rho}}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "If we interpret $X\\%$ as a confidence level and $T$ as the time horizon, a rough estimate of the Credit VaR is therefore $P ( 1 − R ) V ( X, T )$ , where $P$ is the size of the portfolio and $R$ is the recovery rate.\n",
    "\n",
    "Suppose that a bank has a total of €100 million of retail exposures. \n",
    "The 1-year probability of default averages to 2% and the recovery rate averages to 60%. The copula correlation parameter is estimated as 0.1. The associated Credit VaR can be computed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit VaR: 5129484\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from math import sqrt\n",
    "\n",
    "P = 100e6\n",
    "Q = 0.02\n",
    "R = 0.6\n",
    "rho = 0.1\n",
    "\n",
    "var = P*(1-R)*norm.cdf((norm.ppf(Q)+norm.ppf(0.999)*sqrt(rho))/(sqrt(1-rho)))\n",
    "print (\"Credit VaR: {:.0f}\".format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CreditMetrics\n",
    "\n",
    "Another popular approach to compute Credti VaR is CreditMetrics. It involves estimating a probability distribution of credit losses by carrying out Monte Carlo simulations of the credit rating are changes of all counter-parties.\n",
    "\n",
    "Imagine we would like to determine the probability distribution of losses over 1-year period.\n",
    "On each simulation, the new credit ratings of each counter-party is computed (default included). The portfolio value is than computed to determine the eventual losses considering that the asset value depends on the corresponding rating (in case of default we need to compute the loss given default).\n",
    "\n",
    "<img src=\"credit_metrics_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Valuation Adjustment\n",
    "\n",
    "Suppose you have a portfolio of derivatives with a counter-party. If the counter-party defaults and the present value of the portfolio at default is positive to the surviving party, then the surviving party only gets a recovery fraction of the portfolio value from the defaulted entity. \n",
    "\n",
    "If however the present value is negative to the surviving party, it has to pay it in full to the liquidators of the defaulted entity. This creates and asymmetry that, once one has done all calculations, says that the value of the deal under counter-party risk is the value without counter-party risk minus a positive adjustment, called Credit Valuation Adjustment (CVA).\n",
    "\n",
    "It can be expressed in the following way:\n",
    "\n",
    "$$\\textrm{CVA} = ( 1 − R )\\int^{T}_{0} D(t)\\cdot EE(t) dQ(t) $$\n",
    "\n",
    "where $T$ is the latest maturity in the portfolio, $D$ is the discount factor, $EE$ is the expected exposure or $\\mathbb{E}[ \\textrm{max(0, NPV portfolio)}]$ and $dQ$ is the probability of default between $t$ and $t+dt$. \n",
    "\n",
    "For an easier computation it is natural to discretize the above integral and use a time grid going from 0 to the maturity of the portfolio:\n",
    "\n",
    "$$\\textrm{CVA} = ( 1 − R ) \\sum_i D(t_i)\\cdot EE(t_i) Q(t_{i − 1}, t_i)$$\n",
    "\n",
    "It is important to notice that while Credit VaR measures the risk of losses you face due to the possible default of some counter-parties you are having business with, CVA measures the pricing component of this risk, i.e. the adjustment to the price of a product due to this risk.\n",
    "\n",
    "### Debit Valuation Adjustment\n",
    "\n",
    "This very same adjustment seen from the point of view of our counter-party is positive, and is called Debit Valuation Adjustment, DVA. It is positive because the early default of the client itself would imply a discount on the client payment obligations, and this means a gain in a way. So the client marks a positive adjustment over the risk free price by adding the positive amount called DVA.\n",
    "\n",
    "Basically when both parties have the possibility to default, they consistently include both defaults into the valuation. Hence every party needs to include its own default besides the default of the counter-party into the valuation. So they will mark a positive CVA to be subtracted and a positive DVA to be added to the default risk free price of the deal. The CVA of one party will be the DVA of the other one and vice versa.\n",
    "\n",
    "$$\\textrm{price = default risk free price + DVA - CVA}$$\n",
    "\n",
    "Now, since\n",
    "$$\\textrm{default risk free price(A) = − default risk free price(B)}$$\n",
    "\n",
    "$$\\textrm{DVA(A) = CVA(B)}$$\n",
    "\n",
    "$$\\textrm{DVA(B) = CVA(A)}$$\n",
    "\n",
    "we get that eventually\n",
    "$$\\textrm{price(A) = − price(B)}$$\n",
    "\n",
    "so that both parties agree on the price, or, we could say, there is money conservation.\n",
    "\n",
    "### CVA Computation\n",
    "The computation of the CVA is easily carried on with Monte Carlo simulation. First simulate the development of your portfolio (its NPV) at each time point for each MC scenario. Then calculate the CVA using one of the previous definitions. Finally average the CVA of all the scenarios to get its best estimate.\n",
    "\n",
    "In case of zero coupon bonds the computation of the CVA can be further simplified. Indeed in this case the exposure of the investor is equal to the face value of the bond, so it is enough to loop through each days from the observation date to the maturity of the bond and compute the CVA. \n",
    "\n",
    "Imagine a 3-years bond with a face value $FV$ = €100.\n",
    "The bond issuer has the following default probabilities 10%, 20% and 30% for 1, 2 and 3 years respectively (the recovery rate is 40%). The risk free rate is 3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.225686489343786\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from finmarkets import DiscountCurve, CreditCurve\n",
    "import math\n",
    "\n",
    "T = 3 \n",
    "r = 0.03\n",
    "R = 0.4\n",
    "FV = 100\n",
    "obs_date = date.today()\n",
    "pillars = [obs_date + relativedelta(years=i) for i in range(T+1)]\n",
    "dfs = [1/(r+1)**i for i in range(T+1)]\n",
    "dc = DiscountCurve(obs_date, pillars, dfs)\n",
    "S = [1, 0.90, 0.8, 0.7]\n",
    "cc = CreditCurve(pillars, S)\n",
    "\n",
    "cvas = []\n",
    "trials = 10000\n",
    "for _ in range(trials):\n",
    "    cva = 0\n",
    "    for t in range(365*T):\n",
    "        cva += dc.df(obs_date+relativedelta(days=t))*(\n",
    "            cc.ndp(obs_date+relativedelta(days=t)) - cc.ndp(obs_date+relativedelta(days=t+1)))\n",
    "    cvas.append((1-R)*FV*cva)\n",
    "    \n",
    "print (np.mean(cvas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the recovery payments are done at the maturity date of the zero coupon bond the CVA calculation can be simplified further, indeed\n",
    "\n",
    "$$ \\textrm{CVA} = (1-R)\\cdot FV \\cdot \\sum_{i=1}^{n} D(d_i)\\cdot Q(d_{i-1}, d_i) $$\n",
    "becomes\n",
    "$$ \\textrm{CVA} = (1-R)\\cdot FV\\cdot D(d_n)\\cdot\\sum_{i=1}^{n} Q(d_{i-1}, d_i) = (1-R)\\cdot FV\\cdot Q(d_{0}, d_n)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.47254986835687\n"
     ]
    }
   ],
   "source": [
    "# super-simplified form\n",
    "print ((1-R)*FV*0.3*dc.df(pillars[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.51416593531596\n"
     ]
    }
   ],
   "source": [
    "print (FV*dc.df(pillars[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Optimization\n",
    "\n",
    "Portfolio optimization models look for the optimal way to make investments. Usually investors expect \n",
    "either a maximum return for a given level of risk or a given return for a minimum risk so these models\n",
    "are typically based on two criteria: maximization of the expected return and/or minimization of the risk.\n",
    "\n",
    "There is a variety of measures of risk and the most popular is the variance in return. \n",
    "\n",
    "### Some Notation\n",
    "\n",
    "* expected return: \n",
    "    $$\\mathbb{E}(R_{p}) = \\sum _{i}w_{i} \\mathbb{E}(R_{i})$$\n",
    "  where $R_{p}$ is the return on the portfolio, $R_{i}$ is the return on\n",
    "  asset $i$ and $w_{i}$ is the weighting of component asset $i$\n",
    "  (that is, the proportion of asset $i$ in the portfolio) and\n",
    "  $\\sum_{i}w_i = 1$ and $0 \\le w_i \\le 1$;\n",
    "* portfolio return variance:\n",
    "  $$ \\sigma _{p}^{2} = \\sum _{i}\\sum _{j}w_{i}w_{j}\\sigma _{ij}$$ \n",
    "  where\n",
    "  $\\Sigma = \\sigma _{ij}=\\sigma _{i}\\sigma _{j}\\rho _{ij}$ is the (sample)\n",
    "  covariance of the periodic returns on the two assets, $\\rho_{ij}$ is the correlation coefficient. In matrix notation:\n",
    "  $$\\sigma_p^2 = \\mathbf{w^T}\\Sigma\\mathbf{w}$$\n",
    "  where $\\mathbf{w} = (w_1, w_2,\\ldots,w_N)$ is the vector of weights.\n",
    "* portfolio return volatility (standard deviation):\n",
    "  $$ \\sigma _{p}= \\sqrt{\\sigma _{p}^{2}}$$\n",
    "\n",
    "### The Markowitz Mean/Variance Portfolio Model\n",
    "The portfolio model, introduced by Markowitz, assumes an investor has two considerations when constructing an investment portfolio: expected return and variance in return (i.e., risk, since it measures the variability in realized return around the expected return). \n",
    "\n",
    "The Markowitz model requires two major kinds of information: \n",
    "\n",
    "* the estimated expected return for each candidate investment;\n",
    "* the covariance matrix of returns. \n",
    "\n",
    "The latter characterizes not only the individual variability of the return on each investment, but also how each investment’s return tends to move with other investments. \n",
    "\n",
    "Throughout this lesson we will use real market data stored in [portfolio_data.csv](https://drive.google.com/file/d/1srCzNlKVY_LHRpkaKoUynnmI0KImfT6Y/view?usp=sharing).\n",
    "The sample includes, for each entry, a date and the corresponding closing price of five company stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AAPL     AMZN     FB    GOOG       NFLX\n",
      "date                                                    \n",
      "2014-03-27  71.865678  338.470  60.97  558.46  52.025714\n",
      "2014-03-28  71.785450  338.290  60.01  559.99  51.267143\n",
      "2014-03-31  71.769404  336.365  60.24  556.97  50.290000\n",
      "2014-04-01  72.425937  342.990  62.62  567.16  52.098571\n",
      "2014-04-02  72.546280  341.960  62.72  567.00  51.840000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"portfolio_data.csv\", index_col=\"date\")\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"portfolio_sample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\tt{pandas}$ the main characteristics of these time series can be easily computed (e.g. daily returns, covariance matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL    0.239188\n",
      "AMZN    0.415127\n",
      "FB      0.263797\n",
      "GOOG    0.172818\n",
      "NFLX    0.528046\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# returns daily and annualized\n",
    "daily_returns = df.pct_change()\n",
    "returns = daily_returns.mean()*252\n",
    "print (returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          AAPL      AMZN        FB      GOOG      NFLX\n",
      "AAPL  0.051902  0.025037  0.025737  0.022454  0.027760\n",
      "AMZN  0.025037  0.085839  0.041025  0.039501  0.048412\n",
      "FB    0.025737  0.041025  0.069550  0.036127  0.044528\n",
      "GOOG  0.022454  0.039501  0.036127  0.051797  0.040390\n",
      "NFLX  0.027760  0.048412  0.044528  0.040390  0.178298\n"
     ]
    }
   ],
   "source": [
    "# covariance\n",
    "covariance = daily_returns.cov()*252\n",
    "print (covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sample correlations are rather small and all the stocks are positively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating a large number of set of weights to construct portfolios with the five stocks shown before, we can see which is the distribution of these portfolios in terms of return and volatility. In this case no attempt of any optimization whatsoever has been made.\n",
    "\n",
    "In principle investors may use short sales in their portfolios (a portfolio is short in those stocks with negative weights). Although short selling extends the set of possible portfolios we are not going to consider them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"return_variance.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Markowitz model states that the weights $w_i$ should be chosen such that the portfolio has the minimum volatility (variance). So the application of Markovitz model reduces to an optimization problem\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\min}\\{\\sigma_P^2\\}= \\underset{\\mathbf{w}}{\\min}\\{\\mathbf{w^T}\\Sigma\\mathbf{w}\\}$$\n",
    "\n",
    "with the constraint $\\sum_{i}w_i = 1$ and $0 \\le w_i \\le 1$.\n",
    "\n",
    "With have already seen how to solve minimization problems in $\\tt{python}$ so we just need to repeat the usual steps seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.03607029963784209\n",
      "     jac: array([0.0723022 , 0.07233166, 0.0718961 , 0.07199311, 0.07223676])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 63\n",
      "     nit: 9\n",
      "    njev: 9\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.44544146, 0.06252825, 0.12333117, 0.3662157 , 0.00248342])\n"
     ]
    }
   ],
   "source": [
    "# markowitz\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def sum_weights(w): \n",
    "    return np.sum(w) - 1\n",
    "\n",
    "def markowitz(w, cov):\n",
    "    return w.T.dot(cov.dot(w))\n",
    "\n",
    "num_assets = 5\n",
    "constraints = ({'type': 'eq', 'fun': sum_weights},) \n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "weights = [1./num_assets for _ in range(num_assets)]\n",
    "\n",
    "opts = minimize(markowitz, weights, args=(covariance,),\n",
    "                bounds=bounds, constraints=constraints)\n",
    "print (opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected portfolio return: 0.230\n"
     ]
    }
   ],
   "source": [
    "print (\"Expected portfolio return: {:.3f}\".format(np.dot(opts.x, returns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution recommends about 44% of the portfolio be invested in AAPL, about 7% in AMZN, 12% in FB and so on...\n",
    "The expected return is about 23%, with a variance of about 0.036 or,\n",
    "equivalently, a standard deviation of 0.19.\n",
    "\n",
    "In this example we based the model simply on statistical data derived from daily returns. However it could be possible, rather than use historical series for estimating the\n",
    "expected return of an asset, to base this estimate on information about its expected future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Frontier and Parametric Analysis\n",
    "There is no precise way for an investor to determine the “correct” trade off between risk and return. If an investor wants a higher expected return, she generally has to “pay for it” with higher risk. \n",
    "Thus, one is frequently interested in looking at the relative distribution of the two. \n",
    "\n",
    "In finance terminology, we\n",
    "would like to trace out the **efficient frontier** of return and risk. \n",
    "To determine it we need to solve for the minimum variance\n",
    "portfolio over a range of values for the expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier(w, asset_returns, target_return): \n",
    "    portfolio_return = asset_returns.dot(w) \n",
    "    return (portfolio_return - target_return)\n",
    "\n",
    "results = []\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "    \n",
    "for eff in np.arange(0.20, 0.45, 0.005):\n",
    "    constraints = ({'type': 'eq', 'fun': efficient_frontier, \n",
    "                    'args':(returns, eff,)},\n",
    "                   {'type': 'eq', 'fun': sum_weights})\n",
    "    weights = [1./num_assets for _ in range(num_assets)]\n",
    "    opts = minimize(markowitz, weights, args=(covariance,),\n",
    "                    bounds=bounds, constraints=constraints) \n",
    "    \n",
    "    results.append((np.sqrt(opts.x.T.dot(covariance.dot(opts.x))),\n",
    "                    np.sum(returns*opts.x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"efficient_frontier.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticisms to Markowitz Model\n",
    "\n",
    "Despite the significant utility of the Markowitz theory, there are some major limitations in this model:\n",
    "\n",
    "* it is difficult to forecast asset returns with accuracy using historical data. As return estimations have a much larger impact on the asset allocations, small changes in return assumptions can lead to inefficient portfolios. Therefore, the model tends to lead to highly concentrated portfolios (out-of sample weights) that do not offer as much diversification benefits;\n",
    "* the model assumes that asset correlations are linear. In reality, asset correlations move dynamically, changing with the market cycles. During the global financial crisis, asset cor- relations approached almost 1, so if anything, diversification seemed to have insignificant impacts on the portfolios;\n",
    "* last but not the least, the model assumes normality in return distributions. Therefore, it does not factor in extreme market moves which tend to make returns distributions either skewed, fat tailed or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolios with a Risk-Free Asset\n",
    "When one of the asset of the portfolio is risk free, then the efficient frontier has a particularly simple form: a line, the *capital allocation line* (CAL). \n",
    "\n",
    "The slope of the this line measures the trade off between risk and return: a higher slope means that investors receive a higher expected\n",
    "return in exchange for taking on more risk. \n",
    "The CAL aids investors in choosing how much to invest in a risk-free asset and one or more risky assets.\n",
    "\n",
    "The simplest example is a portfolio containing two assets: one risk-free (e.g. treasury bill) and one risky (e.g. a stock).\n",
    "\n",
    "Assume that the expected return of the treasury bill is $\\mathbb{E}[R_f] = 3\\%$ and its risk is 0%. Further, assume that the expected return of the stock is $\\mathbb{E}[R_r] = 10\\%$ and its standard deviation is $\\sigma_r = 20\\%$. The question that needs to be answered for any individual investor is how much to invest in each of these assets.\n",
    "\n",
    "The expected return ($\\mathbb{E}[R_p]$) of this portfolio is calculated as follows:\n",
    "\n",
    "$$\\mathbb{E}[R_p] = \\mathbb{E}[R_f]\\cdot w_f + \\mathbb{E}[R_r] \\cdot ( 1 − w_f )$$\n",
    "where $w_f$ is the relative allocation to the risk-free asset.\n",
    "\n",
    "The calculation of risk for this portfolio is simple because the standard deviation of the treasury bill is 0%. Thus, risk is calculated as:\n",
    "\n",
    "$$\\sigma_p = ( 1 − w_f ) \\cdot \\sigma_r$$\n",
    "\n",
    "In this very simple example, if an investor were to invest 100% into the risk-free asset ($w_f = 1$),\n",
    "the expected return would be 3% and the risk of the portfolio would be 0%. Likewise, investing\n",
    "100% into the stock ($w_f = 0$) would give an investor an expected return of 10% and a portfolio risk\n",
    "of 20%. If the investor allocated 25% to the risk-free asset and 75% to the risky asset, the portfolio\n",
    "expected return and risk calculations would be:\n",
    "\n",
    "$$\\mathbb{E}[R_p] = ( 3\\% \\cdot 25\\% ) + ( 10\\% \\cdot 75\\% ) = 0.75\\% + 7.5\\% = 8.25\\%$$\n",
    "\n",
    "$$\\sigma_p = 75\\%\\cdot 20\\% = 15\\%$$\n",
    "\n",
    "Applying the same steps to our example we can consider an additional risk-free asset with an expected return of 10% and repeat the minimisation to determine the efficient frontier of\n",
    "the resulting portfolio. Notice how the objective function is almost the same while the constraint on the target return now includes also the risk-free asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_assets = 6\n",
    "    \n",
    "def markowitz_with_rf(w, cov):\n",
    "    return w[:-1].T.dot(cov.dot(w[:-1]))\n",
    "\n",
    "def efficient_frontier_with_rf(w, asset_returns, target_return, \n",
    "                               risk_free): \n",
    "    portfolio_return = np.sum(asset_returns*w[:-1]) + risk_free*w[5] \n",
    "    return (portfolio_return - target_return)\n",
    "\n",
    "rf_asset_return = 0.10\n",
    "results_rf = []\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "for eff in np.arange(0.10, 0.45, 0.01):\n",
    "    constraints = ({'type': 'eq', 'fun': efficient_frontier_with_rf, \n",
    "                    \"args\":(returns, eff, rf_asset_return)},\n",
    "                    {'type': 'eq', 'fun': sum_weights})\n",
    "    weights = [1./num_assets for _ in range(num_assets)]\n",
    "    opts = minimize(markowitz_with_rf, weights, \n",
    "                    args=(covariance),\n",
    "                    bounds=bounds, constraints=constraints)\n",
    "    results_rf.append((np.sqrt(opts.x[:-1].T.dot(covariance.dot(opts.x[:-1]))), \n",
    "                     np.sum(returns*opts.x[:-1])+opts.x[5]*rf_asset_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficient frontier has become a straight line, tangent to the\n",
    "frontier of the risky assets only. \n",
    "\n",
    "When the target is 10% the entire investment is allocated to the\n",
    "risk-free asset, as the target increases the fraction of the risky assets grows proportionally to the volatility.\n",
    "It is important to note that in general the relative proportions devoted to risky investments do not change. Only the allocation between the risk-free asset and the risky assets changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sharpe Ratio\n",
    "The goal of an investor who is seeking to earn the highest possible expected return for any \n",
    "level of volatility is to find the portfolio that generates the steepest possible line\n",
    "when combined with the risk-free investment. The slope of this line is called the \n",
    "*Sharpe ratio* of the portfolio.\n",
    "\n",
    "For some portfolio $p$ of risky assets let\n",
    "\n",
    "* $R_p$ its expected return;\n",
    "* $\\sigma_p$ its standard deviation in return;\n",
    "* $r_0$ the return of a reference risk-free asset.\n",
    "\n",
    "A plausible single measure (as opposed to the two measures, risk and return) of attractiveness of a portfolio $p$ is the Sharpe ratio defined as\n",
    "\n",
    "$$ (R_p - r_0 ) / \\sigma_p $$\n",
    "\n",
    "In words, it measures how much additional return we achieved for the additional risk we took on, relative to putting all our money in a risk-free asset.\n",
    "The portfolio that maximizes this ratio has a certain well-defined appeal. Suppose:\n",
    "\n",
    "* $R_\\textrm{target}$ our desired target return;\n",
    "* $w_p$ the fraction of our wealth we place in portfolio $p$ (the rest placed in the risk-free asset).\n",
    "\n",
    "To meet our return target, we must have:\n",
    "\n",
    "$$ ( 1 - w_p ) * r_0 + w_p * R_p = R_\\textrm{target} $$\n",
    "\n",
    "The standard deviation of our total investment is: $w_p\\cdot \\sigma_p$.\n",
    "Solving for $w_p$ in the return constraint, we get:\n",
    "\n",
    "$$ w_p = \\cfrac{( R_\\textrm{target} – r_0)}{( R_p – r_0)} $$\n",
    "\n",
    "Thus, the standard deviation of the portfolio is:\n",
    "\n",
    "$$ w_p\\cdot \\sigma_p = \\Big[\\cfrac{( R_\\textrm{target} – r_0)}{(R_p – r_0)}\\Big]\\cdot \\sigma_p $$\n",
    "\n",
    "Minimizing the portfolio standard deviation means:\n",
    "\n",
    "$$ \\min(\\Big[\\cfrac{( R_\\textrm{target} – r_0)}{(R_p – r_0)}\\Big]\\cdot \\sigma_p)\\implies\\max(\\cfrac{R_p – r_0}{\\sigma_p}) $$\n",
    "\n",
    "So, regardless of our risk/return preference, the money we invest in risky assets should be invested in the risky portfolio that maximizes the Sharpe ratio since it is the one that minimize the variance at the same time ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -1.259317843963551\n",
      "     jac: array([-0.37875988, -0.37936528, -0.26915939,  0.02855793, -0.37932482])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 42\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.19754268e-01, 5.43974232e-01, 4.33680869e-19, 6.41847686e-17,\n",
      "       3.36271500e-01])\n"
     ]
    }
   ],
   "source": [
    "num_assets = 5\n",
    "rf_asset_return = 0.10\n",
    "\n",
    "def negativeSharpeRatio(w, asset_returns, rf_asset_return, cov): \n",
    "    p_ret = np.sum(asset_returns*w)\n",
    "    p_var = np.sqrt(w.T.dot(cov.dot(w)))\n",
    "    ratio = -(p_ret - rf_asset_return) / p_var\n",
    "    return ratio\n",
    "\n",
    "constraints = ({'type': 'eq', 'fun': sum_weights})\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "weights = [1./num_assets for _ in range(num_assets)]\n",
    "opts = minimize(negativeSharpeRatio, weights, \n",
    "                args=(returns, rf_asset_return, covariance),\n",
    "                bounds=bounds, constraints=constraints)\n",
    "print (opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe ratio:  1.259317843963551\n"
     ]
    }
   ],
   "source": [
    "print (\"Sharpe ratio: \", -opts.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sharpe_ratio.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the relative proportions of the stocks are the same as in the previous case where we explicitly included the risk-free asset (0.12, 0.54, 0., 0., 0.33).\n",
    "\n",
    "So the optimization using the Sharpe ratio gives us a portfolio that is\n",
    "on the minimum volatility efficient frontier, and gives the maximum\n",
    "return relative to putting all our money in the risk-free asset.\n",
    "\n",
    "Usually, any Sharpe ratio greater than 1.0 is considered acceptable to good by investors. A ratio higher than 2.0 is rated as very good. A ratio of 3.0 or higher is considered excellent. A ratio under 1.0 is sub-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capital Asset Pricing Model\n",
    "\n",
    "The Capital Asset Pricing Model (CAPM) describes the relationship between the expected return of assets and the systematic risk of the market.\n",
    "\n",
    "CAPM indicates that the expected return of an asset or a portfolio is equal to the risk-free return plus a risk premium. The assumption of CAPM is that investors are rational and want to maximize return and reduce risk as much as possible and so its goal is to calculate what return an investor can expect to make for a given risk premium over the risk-free rate.\n",
    "\n",
    "Mathematically, we can define CAPM formula as follows\n",
    "$$r_i = r_f + \\beta_i(r_m-r_f)$$\n",
    "where:\n",
    "\n",
    "* $r_i$ is the expected return of the $i^{th}$ security;\n",
    "* $r_f$ is the risk-free rate with zero standard deviation (an example of a risk-free asset includes Treasury Bills as they are backed by the U.S. government);\n",
    "* $\\beta_i$ is a measure of a stock's volatility in relation to the overall market (e.g. S\\&P 500). In other words, it represents the slope of the regression line, which is the market return vs. the individual stocks return. $\\beta$ is used in the CAPM to describe the relationship between systematic risk, or market risk, and the expected return of an asset. By definition, we say that the overall market has a beta of 1.0 and individual stocks are ranked by how volatile they are relative to the market. If $\\beta$ of an individual stock = 1.0, this means its price is perfectly correlated with the market, i $\\beta < 1.0$, which is referred to as \"defensive\", this indicates the security is theoretically less volatile than the market, while if $\\beta > 1.0$, or \"aggressive\", this indicates the assets price is more volatile than the market;\n",
    "* $r_m - r_f$ is the risk premium ($r_m$ denotes the market return and includes all securities in the market.\n",
    "\n",
    "To use CAPM it is enough to pick individual stocks or portfolios, and compare them to different indexes (as proxies for the whole market). The point is to find stocks that have high $\\beta$, and portfolios that have high $\\alpha$. High $\\beta$ values mean that the stock fares better than index, so those stocks have a chance at beating the market. $\\alpha$ values above zero mean that your portfolio gives positive return no matter what the market does.\n",
    "\n",
    "In order to see an example of CAPM application we use [capm.csv](https://drive.google.com/file/d/1G4U8foyhq9agGPs8cg83-aY4qU8K3jJI/view?usp=sharing) file which contains the historical series of S&P500 and of some stocks.\n",
    "As usual with $\\tt{pandas}$ we can inspect the file and compute some useful quantities like daily and annualized expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AAPL     AMZN          BA    GOOG         IBM        MGM  \\\n",
      "date                                                                        \n",
      "27/03/2014  71.865678  338.470  111.078480  558.46  167.346339  24.629343   \n",
      "28/03/2014  71.785450  338.290  112.205402  559.99  167.892905  24.609560   \n",
      "31/03/2014  71.769404  336.365  113.133986  556.97  169.691285  25.578908   \n",
      "01/04/2014  72.425937  342.990  115.586169  567.16  171.463219  26.241625   \n",
      "02/04/2014  72.546280  341.960  115.676323  567.00  170.625738  26.409777   \n",
      "\n",
      "                    T    TSLA        SP500  \n",
      "date                                        \n",
      "27/03/2014  28.801410  207.32  1849.040039  \n",
      "28/03/2014  28.892032  212.37  1857.619995  \n",
      "31/03/2014  28.892032  208.45  1872.339966  \n",
      "01/04/2014  28.908509  216.97  1885.520020  \n",
      "02/04/2014  29.139184  230.29  1890.900024  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "capm = pd.read_csv(\"capm.csv\", index_col='date')\n",
    "print (capm.head())\n",
    "daily_returns = capm.pct_change()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate $\\beta$ and CAPM for each Stock\n",
    "To recap $\\beta$ is the slope of the regression line of the market return vs stock return plot (and $\\alpha$ is the intercept of this line with the $y$ axis).\n",
    "\n",
    "These quantities can be calculated with $\\tt{numpy.polyfit}$ passing as inputs the market and the stock return lists. \n",
    "\n",
    "<img src=\"capm_fit.png\">\n",
    "\n",
    "With the daily returns and $\\beta$ of each stock, we can calculate the capital asset pricing model. First, we can calculate the average daily return of the market and annualize this return by multiplying it by the number of trading days in a year.\n",
    "Assuming a risk-free rate of 1%, we can then calculate CAPM using its definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL: 10.3%\n",
      "AMZN: 10.8%\n",
      "BA  : 10.3%\n",
      "GOOG: 10.7%\n",
      "IBM :  8.7%\n",
      "MGM : 13.8%\n",
      "T   :  5.9%\n",
      "TSLA: 11.9%\n"
     ]
    }
   ],
   "source": [
    "rm = np.mean(daily_returns.iloc[1:]['SP500'])*252 \n",
    "rf = 0.01\n",
    "betas = {}\n",
    "alphas = {}\n",
    "ERs = {}\n",
    "for i, k in enumerate(capm.columns):\n",
    "    if k == \"SP500\": \n",
    "        continue\n",
    "    betas[k], alphas[k] = np.polyfit(daily_returns['SP500'][1:], daily_returns[k][1:], 1)\n",
    "    ERs[k] = rf + (betas[k] * (rm-rf))\n",
    "    \n",
    "for k, v in ERs.items(): \n",
    "    print (\"{:4}: {:4.1f}%\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the $\\beta$ of each individual stock we can calculate the CAPM for a portfolio made of the same stocks (we assume equal weights for simplicity).\n",
    "It is enough indeed to perform a weighted sum of of the expected return according to the model of each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3%\n"
     ]
    }
   ],
   "source": [
    " print (\"{:.1f}%\".format(sum(ERs.values())*1/len(ERs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected return of the portfolio is roughly 10\\% and this is what an investor should expect according to CAPM.\n",
    "\n",
    "As we have seen the whole model is about plotting a line in a scatter plot. It’s not a very complex model. Assumptions under the model are even more simplistic. For example:\n",
    "\n",
    "* expect that all investors are rational and they avoid risk;\n",
    "* everyone have full information about the market;\n",
    "* everyone have similar investment horizons and expectations about future movements;\n",
    "* stocks are all correctly priced.\n",
    "\n",
    "Moreover, this is a model from the 1950s. Market dynamics were different back then. And of course, this is a retrospective model. We cannot know how future stock prices move and how the market behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Parity Portfolio\n",
    "\n",
    "An alternative approach to Markowitz theory is given by the *risk parity*. A risk parity portfolio is an investment allocation strategy which focuses on the allocation of risk, rather than the allocation of capital. \n",
    "\n",
    "A risk parity (equal risk) portfolio is characterised by having equal risk contributions to the total risk from each individual asset. \n",
    "Risk parity allocation is also referred to as equally-weighted risk contributions portfolio method. Equally-weighted risk contributions is not about having the same volatility, it is about having each asset contributing in the same way to the portfolio overall volatility. \n",
    "\n",
    "For this we will have to define the contribution of each asset to the portfolio risk. **This allocation strategy has gained popularity in the last decades since it is believed to provide better risk adjusted return than capital based allocation strategies.**\n",
    "\n",
    "Consider a portfolio of $N$ assets: $x_1,\\ldots , x_N$ where as usual the weight of the asset $x_i$ is denoted by $w_i$. The $w_i$ form the allocation vector $\\mathbf{w}$. Let us further denote the covariance matrix of the assets as $\\Sigma$. The volatility of the portfolio is then defined as:\n",
    "\n",
    "$$\\sigma_p={\\sqrt {\\mathbf{w^T}\\Sigma \\mathbf{w}}} = \\sum _{i=1}^{N}\\sigma _{i}\\qquad\\textrm{with}~\\sigma _{i} = w_{i}\\cdot \\cfrac{\\partial\\sigma_p}{\\partial w_{i}}={\\cfrac {w_{i}(\\Sigma \\mathbf{w})_{i}}{\\sqrt {\\mathbf{w^T}\\Sigma \\mathbf{w}}}}$$\n",
    "so that $\\sigma _{i}$ can be interpreted as the contribution of asset $i$ in the portfolio to the overall risk of the portfolio.\n",
    "Equal risk contribution then means $\\sigma _{i} =\\sigma _{j}$ for all\n",
    "$i,j$ or equivalently $\\sigma _{i}=\\sigma_p/N$. So\n",
    "\n",
    "$$\\sigma _{i} = \\cfrac{\\sigma_p}{N}={\\cfrac {w_{i}(\\Sigma \\mathbf{w})_{i}}{\\sqrt {\\mathbf{w^T}\\Sigma \\mathbf{w}}}}\\implies w_{i} = \\frac {\\sigma_p^{2}}{(\\Sigma \\mathbf{w})_{i}N}$$\n",
    "\n",
    "Since we want the previous expression to be true for each $i$, the solution for the weights can be found by solving the minimisation problem\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\min } \\sum _{i=1}^{N}\\left[w_{i}-{\\frac {\\sigma_p^{2}}{(\\Sigma \\mathbf{w})_{i}N}}\\right]^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\sigma_p={\\sqrt {\\mathbf{w^T}\\Sigma \\mathbf{w}}} & =\n",
    "\\sqrt{\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{11} & \\sigma_{21} \\\\\n",
    "\\sigma_{12} & \\sigma_{22} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1} & w_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "}\\\\\n",
    "&=\n",
    "\\sqrt{\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1}\\sigma_{11} + w_{2}\\sigma_{21} & w_{1}\\sigma_{12} + w_{2}\\sigma_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "} \\\\\n",
    "&= \\sqrt{\n",
    "w_{1}w_{1}\\sigma_{11} + w_{2}w_{1}\\sigma_{21} + w_{1}w_{2}\\sigma_{12} + w_{2}w_{2}\\sigma_{22} }\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\cfrac{\\partial\\sigma_p}{\\partial w_1} = \\cfrac{1}{2}\\cdot\\cfrac{2\\cdot w_1\\sigma_{11} + 2\\cdot w_{2}\\sigma_{21}}{\\sigma_p} = \\cfrac{w_1\\sigma_{11} + w_{2}\\sigma_{21}}{\\sigma_p} = \\cfrac{(\\Sigma \\mathbf{w})_{1}}{\\sigma_p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 2.2543755363349716e-07\n",
      "     jac: array([-7.06474804e-04,  1.97343062e-04, -4.26326934e-05, -8.04037852e-06,\n",
      "        1.11882164e-03])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 38\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.25895309, 0.18118756, 0.19670379, 0.22208923, 0.14106633])\n"
     ]
    }
   ],
   "source": [
    "num_assets = 5\n",
    "def risk_parity(w, cov):\n",
    "    variance = w.T.dot(cov.dot(w)) \n",
    "    sum = 0\n",
    "    N = len(w)\n",
    "    for i in range(N):\n",
    "        sum += (w[i] - (variance/(N*cov.dot(w)[i])))**2 \n",
    "    return sum\n",
    "\n",
    "args = (covariance,)\n",
    "constraints = ({'type': 'eq', 'fun': sum_weights}) \n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "weights = [1./num_assets for _ in range(num_assets)]\n",
    "opts = minimize(risk_parity, weights, args=(covariance,),\n",
    "                bounds=bounds, constraints=constraints)\n",
    "print (opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk contribution for asset 0: 19.974%\n",
      "Risk contribution for asset 1: 19.999%\n",
      "Risk contribution for asset 2: 19.990%\n",
      "Risk contribution for asset 3: 19.993%\n",
      "Risk contribution for asset 4: 20.044%\n"
     ]
    }
   ],
   "source": [
    "sigma_i = []\n",
    "for i in range(num_assets):\n",
    "    std = np.sqrt(opts.x.T.dot(covariance.dot(opts.x))) \n",
    "    a = opts.x[i]*covariance.dot(opts.x)[i] \n",
    "    sigma_i.append(a/std)\n",
    "                   \n",
    "for i in range(num_assets):\n",
    "    print (\"Risk contribution for asset {}: {:.3f}%\".format(i, sigma_i[i]/sum(sigma_i)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Budget Allocation\n",
    "The same technique can be used if we would like to calculate a portfolio with risk budget allocation. If we consider the previous equation\n",
    "\n",
    "$$\\sigma _{i}=\\cfrac{\\sigma_p}{N}$$\n",
    "\n",
    "where we set the risk contribution fraction to every asset to $1/N$;\n",
    "now we can simply replace $1/N$ with the desired fraction ($f_i$) for each asset:\n",
    "\n",
    "$$\\sigma _{i}=f_i \\cdot \\sigma_p$$\n",
    "so that the relation to minimise becomes:\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\min } \\sum _{i=1}^{N}\\left[w_{i}-{\\frac {f_i \\cdot \\sigma_p^{2}}{(\\Sigma \\mathbf{w})_{i}}}\\right]^{2} $$\n",
    "\n",
    "Translating it to $\\tt{python}$ we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 4.798712878867406e-08\n",
      "     jac: array([-3.11920407e-04,  3.24431247e-04,  9.84173665e-05, -9.10631775e-05,\n",
      "        4.14667119e-04])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 45\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.34604471, 0.17973628, 0.19407208, 0.16911722, 0.11102971])\n"
     ]
    }
   ],
   "source": [
    "def risk_budget(w, target_risk, cov):\n",
    "    variance = w.T.dot(cov.dot(w)) \n",
    "    sum = 0\n",
    "    N = len(w)\n",
    "    for i in range(N):\n",
    "        sum += (w[i] - (target_risk[i]*variance)/(cov.dot(w)[i]))**2 \n",
    "    return sum\n",
    "\n",
    "f_i = [0.3, 0.2, 0.2, 0.15, 0.15] \n",
    "args = (f_i, covariance)\n",
    "constraints = ({'type': 'eq', 'fun': sum_weights})          \n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "weights = [1./num_assets for _ in range(num_assets)]\n",
    "\n",
    "opts = minimize(risk_budget, weights, args=(f_i, covariance), \n",
    "                bounds=bounds, constraints=constraints)\n",
    "\n",
    "print (opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk contribution for asset 0: 29.986%\n",
      "Risk contribution for asset 1: 20.009%\n",
      "Risk contribution for asset 2: 19.999%\n",
      "Risk contribution for asset 3: 14.992%\n",
      "Risk contribution for asset 4: 15.013%\n"
     ]
    }
   ],
   "source": [
    "sigma_i = []\n",
    "for i in range(num_assets):\n",
    "    std = np.sqrt(opts.x.T.dot(covariance.dot(opts.x))) \n",
    "    a = opts.x[i]*covariance.dot(opts.x)[i] \n",
    "    sigma_i.append(a/std)\n",
    "    \n",
    "for i in range(num_assets):\n",
    "    print (\"Risk contribution for asset {}: {:.3f}%\".format(i, sigma_i[i]/sum(sigma_i)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Diversification Portfolio\n",
    "\n",
    "In addition to minimum variance, and risk parity/budgeting, maximum diversification is also another well known risk based asset allocation technique.\n",
    "\n",
    "Diversification is a common topic in portfolio construction. Rather than serving as the sole, quantifiable objective, it is most often either pursued in tandem with another objective, such as return maximization, or pursued simply by including more asset classes or adding constraints based on intuition.\n",
    "\n",
    "But it does not have to be this way; diversification can be pursued explicitly as the sole objective in portfolio construction.\n",
    "\n",
    "In a 2008 paper, the diversification ratio $D$ of a portfolio has been defined as:\n",
    "\n",
    "$$D=\\cfrac{\\mathbf{w^T}\\boldsymbol{\\sigma}}{\\sqrt {\\mathbf{w^T}\\Sigma \\mathbf{w}}}$$\n",
    "\n",
    "where $\\boldsymbol{\\sigma}$ is the vector of volatilities and $\\Sigma$ is the covariance matrix. The term in the denominator is the volatility of the portfolio and the term in the numerator is the weighted average volatility of the assets. More diversification within a portfolio decreases the denominator and leads to a higher diversification ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -1.3603935531589488\n",
      "     jac: array([-0.00038704,  0.00023621, -0.00032043,  0.00083792,  0.00020067])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 37\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.3495816 , 0.17887765, 0.1566318 , 0.12398934, 0.19091962])\n"
     ]
    }
   ],
   "source": [
    "num_assets = 5\n",
    "def diversification_ratio(w):\n",
    "    w_vol = np.dot(np.sqrt(np.diag(covariance)), w.T)\n",
    "    port_vol = np.sqrt(np.dot(w.T, np.dot(covariance, w)))\n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    return -diversification_ratio\n",
    "\n",
    "bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "cons = ({'type': 'eq', 'fun': sum_weights},)\n",
    "#cons = cons + ({'type': 'ineq', 'fun':  long_only_constraint},)\n",
    "weights = [1./num_assets for _ in range(num_assets)]\n",
    "\n",
    "opts = minimize(diversification_ratio, weights, bounds=bounds, \n",
    "                constraints=cons)\n",
    "print (opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversification:  1.3603935531589488\n"
     ]
    }
   ],
   "source": [
    "ret = np.sum(returns*opts.x)\n",
    "vol = np.sqrt(np.dot(opts.x.T, np.dot(covariance, opts.x))) \n",
    "print (\"Diversification: \", -opts.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"max_div.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
