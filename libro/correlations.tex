\chapter{Modeling Correlation between Risks}

In credit derivative valuation and credit risk management, one of the fundamentally important issues is the estimate of default probabilities and their correlations. For this, generally speaking, there are two ways: using historical default data or using mathematical models.

Historical default data has played an important role in the estimate of default probabilities. However, because credit events are rare, there is very limited default data available. Moreover, historical data only reflects the past default pattern and it may not be a proper indicator of the future. This makes the estimate of default probabilities from historical data difficult and inexact. To use this same data to estimate default correlations is even more difficult and inexact.

The market trend now is towards the use of mathematical models that don't rely on historical default data. In Chapter~\ref{credit_default_swaps} we have seen how it is possible to derive default probabilities from market data, here we will see how copula can be used to model their correlations to valuate credit derivatives. 

\section{One Factor Gaussian Copula Model}
While there are several types of copula function models, the first introduced was the \emph{one-factor Gaussian copula model}. This model is quite simple and has, above all, the advantage that can be solved analytically. In essence the one-factor Gaussian copula, is nothing but a numerical algorithm to generate samples of normally distributed random variables that have a given (Gaussian) pair-wise correlation. 

Consider a portfolio of \(N\) bonds and assume that the marginal default probabilities are known for each issuer. Define:

\begin{itemize}
	\tightlist
	\item
	\(t_i\), the time of default of the \(i^{th}\) company:
	\item
	\(Q_i(t)\), the cumulative probability that company \(i\) will default
	before time \(t\); that is, the probability that \(t_i \le t\).
\end{itemize}

To generate a one-factor model for the default times (\(t_i\)) we define random
variables \(X_i\) \((1\le i \le N)\)
\begin{equation}
X_i = a_i M + \sqrt{1-a_i^2}Z_i,\qquad i = 1, 2,\ldots, N
\label{eq:normalized_var}
\end{equation}
where \(M\) and the \(Z_i\) are independent zero-mean unit-variance  distributions (hence $X_i$ are also distributed with zero-mean and unit standard-deviation) and \(-1 \le a_i \lt 1\).

Eq.~\ref{eq:normalized_var} defines a correlation structure between the \(X_i\) which are dependent on a single common factor \(M\). The $Z_i$ term is usually called the idiosyncratic component of default. The particular functional form of Eq.~\ref{eq:normalized_var} has been chosen to ensure the desired correlation structure among the $X_i$. Indeed the correlation between \(X_i\) and \(X_j\) is

\begin{equation*}
\rho(X_i, X_j) = \cfrac{\mathbb{E}[(X_i-\mu_i)(X_j-\mu_j)]}{\sigma_{i}\sigma_{j}} =\mathbb{E}[X_i X_j] = a_i a_j \mathbb{E}[M^2] = a_i a_j
\end{equation*}
where we just exploit the definition of $X_i$ and its properties.

%Assume that the $i^{th}$ company has defaulted by the time $t_i$ if $X_i$ is below a threshold value $\bar{x}_i(t_i)$.
If $F_i$ is the cumulative distribution function of $X_i$, with a percentile to percentile transformation (see Section~\ref{distribution-transformation}) we can map \(X_i\) to \(t_i\), so that $Q_i(t_i) = P(X_i\le x)=F_i(x)$. Therefore a point \(X_i = x\) is transformed to \(t_i = t\) with \(x = F_i^{-1}[Q_i(t)]\) connecting the marginal distribution of the copula model to the default times.

Let's note that, \textbf{conditional} on $M$, the $N$ default events are \textbf{independent}. So we can write
\begin{equation}
\begin{split}
Q_i^{\textrm{corr}}(t_i|M) = P(X_i\le x|M) &= P(a_i M + \sqrt{1-a_i^2}Z_i\le x) =\\
&= P\left(Z_i\le \cfrac{x-a_i M}{\sqrt{1-a_i^2}}\right)
=H_i\left(\cfrac{F^{-1}[Q(t_i)]-a_i M}{\sqrt{1-a_i^2}}\right)
\end{split}
\label{eq:generic_copula}
\end{equation}
where $H_i$ is the cumulative distribution function of the $Z_i$.

Although in principle any distribution could be used for \(M\) and the \(Z\)'s (provided they have zero mean and unit variance), one common choice is to let them be standard normal distributions (resulting in a Gaussian copula).
So we can rewrite Eq.~\ref{eq:generic_copula} as

\begin{equation}
Q_i^{\textrm{corr}}(t_i|M) = \Phi\left(\cfrac{\Phi^{-1}[Q(t_i)]-a_i M}{\sqrt{1-a_i^2}}\right)
\label{eq:gaussian_one_factor_copula}
\end{equation}
where $\Phi$ denotes the cumulative distribution function of the standard normal distribution.

If we call $\mathcal{C}(t_1,\ldots,t_N)$ the joint distribution of the default times of the $N$ bonds in the portfolio then

\begin{equation}
\mathcal{C}(t_1,\ldots,t_N, \rho)=\Phi_{N}(\Phi^{-1}(Q_1(t_1)),\ldots,\Phi^{-1}(Q_N(t_N)), \boldsymbol{\rho})
\label{eq:gaussian_copula}
\end{equation}
where $\boldsymbol{\rho}$ represents the correlation matrix of the default probabilities. Eq.~\ref{eq:gaussian_copula} is the one factor Gaussian copula model (one factor because there is only a random variable, $M$, which determines the correlation between $X_i$).

Clearly different choices of distributions result in different copula models, and in various natures of the default dependence. For example, copulas where the \(M\) have heavy tails generate models where there is a greater likelihood of a clustering of early defaults for several companies.

\subsection{Standard Market Model}\label{standard-market-model}

Consider the following two assumptions are made:

\begin{itemize}
	\tightlist
	\item
	all the companies have the same default intensity (hazard rates), i.e, \(\lambda_i = \lambda\) (which means they all have the same default probabilities $Q_i = Q$);
	\item
	the pairwise default correlations are the same, i.e \(a_i = a\); in other words the contribution of the market
	component $M$ is the same for all the companies and the correlation between any two companies is constant, \(\rho = a^2\).
\end{itemize}

Under these assumptions, given the market situation \(M = m\), all the companies have the same cumulative default probability \(Q_i^{\textrm{corr}}(t_i|m)=P(X_i < x|m)\). Moreover, for a given value of the market component \(M\), the defaults are mutually independent for all the underlying companies. 

In this case the one factor Gaussian model is also called \emph{Market Standard Model}. Later in this Chapter we will also see how the correlation used in Eq.~\ref{eq:gaussian_one_factor_copula} can be implied from the market quotes of credit derivatives.

Letting \(l(t|m)\) be the total defaults that have occurred by time \(t\) conditional on the market condition \(M = m\), then \(l(t|m)\) follows a binomial distribution (see Appendix~\ref{binomial-distribution}), and the probability of default of $j$ companies can be expressed as

\begin{equation}
DP(l(t|m) = j) = \cfrac{N!}{j!(N-j)!}(Q_i^{\textrm{corr}}(t_i|m))^j(1-Q_i^{\textrm{corr}}(t_i|m))^{N-j},\qquad  j=0, 1, 2,\ldots,N
\end{equation}

In general to evaluate any function $g(DP(l(t))$ regardless the parameter $M$ (e.g. an NPV or a fair value which depend on the default probability, remember~\ref{sec:cds_valuation}) is necessary to average according to all the possible values that $M$ can take. The average is performed with an integral which is the weighted sum of $g$ with the probability distribution ($f_M(m)$) of the random variable $M$  

\begin{equation}
g(DP(l(t) = j)) = \int_{-\infty}^{\infty}{g(DP(l(t|m) = j))\cdot f_M(m)dm}
\label{eq:gaussian_quadrature}
\end{equation}

In the general case where the default probabilities of each company are not the same it is possible to determine $DP(l(t|M)=j)$ through an iterative procedure before proceeding with the integration of Eq.~\ref{eq:gaussian_quadrature}. An example of this iterative technique will be shown in Section~\ref{sec:expected_losses}.

\subsection{Extensions of the One Factor Copula Model}
Many other one-factor model have been tried: t-Student copula, Clayton copula, \ldots In general we can define a new model by simply choosing particular functional forms for $M$ and $Z_i$ in Eq.~\ref{eq:normalized_var} provided they are with zero mean and standard deviation one. 

If instead of the single factor $M$ there are two or more, Eq.~\ref{eq:normalized_var} becomes

\begin{equation}
X_i = a_1 M_1 + a_2 M_2 + \sqrt{1 - a_1^2 - a^2_2}Z_i
\end{equation}
and similarly
\begin{equation}
Q^{\textrm{corr}}(t|M_1, M_2) = \Phi\left(\cfrac{\Phi^{-1}[Q(t)]-a_1 M_1 - a_2 M_2}{\sqrt{1 - a_1^2 - a^2_2}}\right)
\end{equation}
In terms of computation time, this kind of models are proportionally slower with the increase of the number of factors, since the integration of Eq.~\ref{eq:gaussian_quadrature} has to be carried on each factor of the model.

\section{Basket Default Swaps}\label{basket-default-swaps}

A Basket Default Swap is a credit derivative on a portfolio of reference entities. The simplest basket default swaps are the so called first-to-default, second-to-default, \ldots, nth-to-default swaps. 

This kind of contracts are very similar to normal CDS except for the protection they offer. With respect to a basket of reference entities, a first-to-default swap provides insurance for only the first default, a second-to-default swap provides insurance for only the second default, or a nth-to-default swap provides insurance for only the $n^{th}$ default. 

For example, in the last case, the seller does not make any payment to the protection buyer for the first $n-1$ defaulted reference entities, and makes it only for the $n^{th}$ defaulted reference entity. Once there has been this payment the swap terminates.

The cost of protection in nth-to-default basket is obviously dependent on default correlations. Suppose that a basket of 100 reference entities is used to define a 5-year nth-to-default CDS and that each reference entity has a 2\% probability of defaulting during the next 5 years. When the default correlation between the reference entities is zero, the binomial distribution (see Appendix~\ref{binomial-distribution}) shows that the probability of one or more defaults during 5 years is 86.74\% and the probability of ten or more defaults is 0.0034\%.

\begin{ipython}
from scipy.stats import binom

b = binom(100, 0.02)
print("P(>=1) : {}".format(1 - b.cdf(0)))
print("P(>=10): {}".format(1 - b.cdf(9)))
\end{ipython}
\begin{ioutput}
P(>=1) : 0.8673804441052471
P(>=10): 3.441680604299169e-05
\end{ioutput}

A first-to-default is therefore quite valuable whereas a tenth-to-default CDS is worth almost nothing.

As the default correlation increases the probability of one or more defaults declines but that for ten or more defaults increases. In the limit where default correlations are perfect the probability of one or more defaults equals the probability of ten or more and it is 2\%. This is because in this extreme situation the reference entities are essentially the same: either they all default (with 2\% probability) or none of them default (with 98\% probability). Similarly for the value of the basket default swaps.

\subsection{Basket CDS Valuation under Gaussian Copula Model}
\label{basket-cds-valuation-under-market-standard-model}
We now present some numerical results for an nth-to-default basket. We assume that principals and expected recovery rates are the same for all underlying reference assets. The valuation procedure is similar to that for a regular CDS where there is only one reference entity.

There the valuation is based on the probability that a default occurred between times \(t_1\) and \(t_2\), see Section~\ref{sec:cds_valuation}. Here instead the valuation will be based on the probability that the $n^{th}$ default happened between times \(t_1\) and \(t_2\): the buyer of protection makes quarterly payments at a
specified rate until the $n^{th}$ default occurs, in which case the seller pays \(F\cdot(1-R)\), or the end life of the contract is reached. 

In the following we are going to develop a \texttt{BasketDefaultSwap} class that is able to compute the basket NPV and its break-even spread. The idea is:
\begin{itemize}
\item compute the correlated default probability of each name according to 
\begin{equation}
Q^{\textrm{corr}}(t|M) = \Phi\left(\cfrac{\Phi^{-1}[Q(t)]-\sqrt{\rho} M}{\sqrt{1-\rho}}\right)
\end{equation}
\item compute the probability of having at least $j$ defaults using the binomial distribution
\begin{equation}
DP(l(t|m) \ge j) = \sum_{k=j}^{N}\left[  \cfrac{N!}{k!(N-k)!}Q^{\textrm{corr}}(t|M)^k(1-Q^{\textrm{corr}}(t|M))^{N-k}\right]
\end{equation}
\item create a \textbf{credit curve}, $CC(DP)$ with those probabilities;
\item re-use the \texttt{CreditDefaultSwap} class methods to compute NPV and break-even rate (with the appropriate discount curve $DC$ and the credit curve determine above), through integration of
\begin{equation}
\mathrm{NPV}(DC, CC(DP)) = \int_{-\infty}^{\infty}{\mathrm{NPV}(DC, CC(DP)) f_M(m)dm} 
\end{equation}
and similarly for the break-even rate.
\end{itemize}

Since to valuate a basket default swap we need to perform integrals let's see briefly how it works in \texttt{python}.

To integrate a function \(f\) we can use \(\tt{scipy.integrate.quad}\) which takes in input: the function to integrate, the integration limits and, if needed, optional arguments of the integrand (see Section~\ref{sec:kwargs_args}). Since it is a numerical integration the result will be a tuple with the actual value and the associated error (we will always be interested in the first item only though).

For example imagine to integrate
\[f(x) = \int_{-2}^{4}(ax^{3} + b)~dx\]
for \(a=3\) and \(b=5\)

\begin{ipython}
from scipy.integrate import quad

def func(x, a, b):
    return a*x**3 + b

s = quad(func, -2, 4, args=(3, 5))
print (s)
\end{ipython}
\begin{ioutput}
(210.0, 2.475694446663456e-12)
\end{ioutput}

Back to the implementation of \texttt{BasketDefaultSwaps}

\begin{ipython}
from finmarkets import CreditCurve, CreditDefaultSwap
from scipy.stats import norm, binom
from numpy import sqrt, exp, inf
from scipy.integrate import quad

class BasketDefaultSwaps:
    def __init__(self, notional, names, rho, start_date, spread,
                 maturity, tenor=3, recovery=0.4):
        self.names = names
        self.rho = rho
        self.cds = CreditDefaultSwap(notional, start_date, spread,
                                     maturity, tenor, recovery)

    def one_factor_model(self, M, f, Q_dates, Q, dc, j):
        DP = norm.cdf((norm.ppf(Q) - sqrt(self.rho)*M)/(sqrt(1-self.rho)))
        b = binom(self.names, DP)
        S = 1-(1-b.cdf(j-1))
        cc = CreditCurve(Q_dates, S)
        return f(dc, cc)*norm.pdf(M)

    def breakeven(self, Q_dates, Q, dc, ndefaults):
        s = quad(self.one_factor_model, -inf, inf,
                 args=(self.cds.breakevenRate, Q_dates, Q, dc, ndefaults))
        return s[0]

    def npv(self, Q_dates, Q, dc, ndefaults):
        s = quad(self.one_factor_model, -inf, inf,
        args=(self.cds.npv, Q_dates, Q, dc, ndefaults))
        return s[0]
\end{ipython}

\begin{finmarkets}
The \texttt{BasketDefaultSwaps} class has to go into the module \texttt{finmarkets} to increase its capabilities.
\end{finmarkets}

Consider a 5-year 1st-to-default basket of ten reference entities in the situation where the Gaussian copula correlation is 0.3 and the expected recovery rate, \(R\), is \(40\%\). The term structure of interest rates is assumed to be flat at 5\%. The default probabilities for the ten entities are generated by Poisson processes with constant default intensities (hazard rates), \(\lambda_i=0.01\), so that

\[ Q(t) = 1 - e^{-\lambda t} \]

Using the \texttt{BasketDefaultSwap} class determine the breakeven spread.

\begin{ipython}
from finmarkets_tot import DiscountCurve
from datetime import date
from dateutil.relativedelta import relativedelta
import numpy as np

n_cds = 10
rho = 0.3
l = 0.01
observation_date = date.today()
pillar_dates = [observation_date + relativedelta(years=i) for i in range(6)]
dfs = [1/(1+0.05)**i for i in range(6)]
dc = DiscountCurve(observation_date, pillar_dates, dfs)
Q = [1-np.exp(-(l*t)) for t in range(6)]
ndefaults = 3
basket = BasketDefaultSwaps(1, n_cds, rho, observation_date, 0.01, 5)

print(basket.breakeven_rate(pillar_dates, Q, dc, ndefaults))
\end{ipython}
\begin{ioutput}
0.0017750563139599736
\end{ioutput}

\section{Collateralized Debt Obligation}\label{collateralized-debt-obligation}

A Collateralized Debt Obligation (CDO) is a credit derivative where the issuer gather risky assets and repackage them into discrete classes, called \emph{tranches}, which are defined on the level of credit risk assumed by the investor. These tranches of securities become the final investment product.

Tranches are named to reflect their risk profile: \emph{senior}, \emph{mezzanine} and \emph{subordinated/equity} and are delimited by an attachment ($L$) and a detachment point ($U$), which represent the percentages of the \textbf{total} principal defining their boundaries. For example, a 5-10\% tranche has an attachment point of 5\% and a detachment point of 10\%. 

Each of these tranches has a different level of seniority relative to the others in the sense that a senior tranche has coupon and principal payment priority over a mezzanine, while a mezzanine tranche has coupon and principal payment priority over an equity one. 

Indeed they receive returns using a set of rules known as \emph{waterfall}. Incomes of the portfolio are first used to provide returns to the most senior tranche, then to the next and so on. So senior tranches are generally safest because they have the first claim on the collateral, although they'll offer lower coupon rates.

It is important to note that a CDO only redistributes the total risk associated with the underlying pool of assets to the priority ordered tranches; it neither reduces nor increases the total risk associated with the pool.

There are various kind of CDOs:
\begin{itemize}
	\item in a \textbf{Cash CDO} the reference portfolio consists of corporate bonds owned by the CDO issuer. Cash flows from collateral are used to pay principal and interests to investors. If such cash flows prove inadequate, rewards are paid to tranches according to their seniority. Figure~\ref{fig:cdo_structure} shows a typical structure of a Cash CDO;

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/cdo_structure}
	\caption{Typical structure of a Cash CDO, with the reference portfolio made of bonds,
		which form the contract collateral.}
	\label{fig:cdo_structure}
\end{figure}

	\item in a \textbf{Synthetic CDO} the underlying reference portfolio is no longer "physical" (e.g. portfolio of bonds or loans), instead it is a \emph{fictitious} portfolio consisting of a number of reference obligations each with an associated notional amount. The value of a synthetic CDO usually comes from insurance premiums of credit default swaps paid for by investors. The investment logic is that sellers assume the underlying assets will perform while investors that will sooner or later default.
\end{itemize}

\subsection{Cash CDO Expected Losses}
\label{sec:expected_losses}

Consider a Cash CDO made of 125 bonds with a maturity of 1 year. Each bond pays a coupon of one unit after 1 year and it has not yet defaulted (the recovery rate $R$ is assumed to be 0). We are interested in the following three tranches: equity ([0, 3] defaults), mezzanine ([4, 6] defaults) and senior ([7, 9] defaults), see Fig.~\ref{fig:cdo_ex_1} (note that in this particular case tranches are identified through the number of defaults and not with principal percentages). 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/ex_cdo_1}
	\caption{Structure of the simple Cash CDO considered in the example.}
	\label{fig:cdo_ex_1}
\end{figure}

Assume also that one year default probabilities are identical for each bond ($Q$) and that the correlation between each pair is also identical and equal to $\rho$. Under these assumptions we are in the position to use the Gaussian Copula Model. 

We are interested in the expected losses for each tranche as a function of the correlation $\rho$.

The probability of having $l$ defaults, conditional to the market parameter $M$ will follow a binomial distribution given by
\begin{equation}
p(l|M) = \binom{N}{l}\mathcal{Q}_M^l (1-\mathcal{Q}_M)^{N-l}
\label{eq:def_prob_ex_cdo_1}
\end{equation}
where $N$ is the number of bonds in the portfolio and 
\begin{equation}
\mathcal{Q}_M = \Phi\left(\cfrac{\Phi^{-1}(Q)-\sqrt{\rho}M}{\sqrt{\-\rho}}\right)
\end{equation}
where $\Phi$ is the standard normal CDF and $Q$ the marginal probability of default within 1 year of a single name.

From the definition of each tranche with have that the expected losses are
\begin{itemize}
	\item $\mathbb{E}_{\textrm{equity loss}}(l, \mathcal{Q}_M)=3\cdot p(l\ge 3|M) + \sum_{k=1}^{2}k\cdot p(l=k|M)$
	\item $\mathbb{E}_{\textrm{mezzanine loss}}(l, \mathcal{Q}_M)=3\cdot p(l\ge 6|M) + \sum_{k=1}^{2}k\cdot p(l=k+3|M)$
	\item $\mathbb{E}_{\textrm{senior loss}}(l, \mathcal{Q}_M)=3\cdot p(l\ge 9|M) + \sum_{k=1}^{2}k\cdot p(l=k+6|M)$
\end{itemize}
	
Each probability can be calculated by integrating the above with respect to $M$
	
\begin{equation} 
\mathbb{E}_{\mathrm{tranche}} = \int_{-\infty}^{\infty}{\mathbb{E}_{\mathrm{tranche}}(l, \mathcal{Q}_M) f_M(m)dm}
\end{equation}

Let's see the corresponding \texttt{python} implementation: first we import the necessary modules and define the needed constants.

\begin{ipython}
from scipy.stats import binom, norm
from scipy.integrate import quad
import numpy as np

N = 125
C = 1
R = 0
q = 0.02
tranches = [[1,3],[4, 6],[7,9]]
\end{ipython}

Then we define a function \texttt{p} which implements the expected losses for each tranche according to Eq.~\ref{eq:def_prob_ex_cdo_1}.
The function depends on the parameter \texttt{M}, and takes as inputs the correlation \texttt{rho} and the tranche attach-detach limits.
	
\begin{ipython}
def p(M, rho, lims):
    qM = norm.cdf((norm.ppf(q)-np.sqrt(rho)*M)/(np.sqrt(1-rho)))
    pN = binom(N, qM)
    loss = 3*(pN.cdf(N) - pN.cdf(lims[1]-1))
    for i in range(lims[0], lims[1]):
        index = i-lims[0]+1
        loss += index*pN.pmf(i)
    return norm.pdf(M)*loss
\end{ipython}

Finally we loop over a range of possible values for the correlation on each tranche to draw the plot of the expected losses vs correlation, shown in Fig.~\ref{fig:losses_rho}.

\begin{ipython}
res = [[],[],[]]
for i in range(len(tranches)):
    for rho in np.arange(0, 1.05, 0.05):
        if rho == 1.0:
            rho = 0.99
        v = quad(p, -np.inf, np.inf, args=(rho, tranches[i]))
        res[i].append(v[0])
\end{ipython}

Some considerations can be done from these results. First of all, as expected, the equity tranche is the riskier, producing the highest level of loss. 

The 
\begin{equation}
\mathbb{E}_{\textrm{equity loss}} \ge \mathbb{E}_{\textrm{mezzanine loss}} \ge \mathbb{E}_{\textrm{senior loss}}
\end{equation}
relation holds only if each tranche has the same notional exposure (in our example 3).

Then we can notice that, in the equity tranche, losses are decreasing in $\rho$. When the correlation is low, indeed, the probability to have few defaults is higher than that for many. As the correlation increases, there will be more and more "simultaneous" defaults so also other tranches will start to suffer losses. In the extreme case of correlation equal to 1 all the tranches behave the same and expected losses curves join together. 

When considering all the tranches covering the entire number of names, the last tranche (the one with detachment point of 100\%) is always increasing in $\rho$. Again this can be explained with correlated defaults. In this case also, the total expected losses on the three tranches is independent of $\rho$. This is not an accident but it is due to the fact that every default scenario is affecting a tranche so that the total loss remain constant.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/losses_vs_rho}
	\caption{Expected losses for each tranche as a function of the correlation parameter.}
	\label{fig:losses_rho}
\end{figure}

\begin{attention}
As noticed at the end of Section~\ref{standard-market-model}, in case the entities of the reference portfolio haven't the same default probabilities it is possible to determine Eq.~\ref{eq:def_prob_ex_cdo_1} numerically with an iterative procedure. The next snippet of code shows an example where ten names have different probabilities.

\begin{attpython}
N=10
# probabilities of default of the each name
q = [0.01, 0.02, 0.15, 0.22, 0.03, 0.01, 0.024, 0.008, 0.015, 0.04]

# p[k] probability of k defaults
p = [0 for _ in range(0, N+1)]
p[0] = (1-q[0])
p[1] = q[0]
for i in range(2, N+1):
    for j in range(1, i+1):
        p[j] = p[j-1]*q[i-1] + p[j]*(1-q[i-1])
    p[0] = p[0]*(1-q[i-1])
print (p)
\end{attpython}
\begin{ioutput}
[0.5655235318063276, 0.350403844356834, 0.11955743831716371,
 0.0330669128164737, 0.007898678014359, 0.00081191731818153,
 5.350491409986e-05, 2.81423807427e-06, 1.2554827537575e-07,
 5.216612301099e-09, 2.08664492043e-10]
\end{ioutput}
\end{attention}

\subsection{Synthetic CDO Valuation}
Imagine a Synthetic CDO made of $N$ references in the portfolio, each one with the same notional amount (the total CDO notional is $F$).
When the $i^{th}$ reference defaults the portfolio incurs in a loss of $\frac{F}{N}(1-R)$ (the recovery rate is assumed to be the same for all names).

The tranche loss function $TL^{L,U}(l)$ for a given time $t$ is a function of the number of defaults $l$ occurred up to that time and is given by
\begin{equation}
TL_{t}^{L,U}=\mathrm{max}(\mathrm{min}(\cfrac{l}{N}\cdot F(1-R), U)-L, 0)
\end{equation}
where $\frac{l}{N}F(1-R)$ is the total portfolio loss, if it is greater than $U$ then the tranche loss is $U$, conversely if it is lower than $L$ there is no loss.

So for example suppose $L=3\%$ and $U=7\%$ and suppose also that the portfolio loss is $5\%$. Then the tranche loss is 2\% of the total portfolio notional (or 50\% of the tranche notional $=7\%-3\%=4\%$).

When an investor \emph{sells protection} on a tranche she is guaranteeing to reimburse any realized losses on the tranche to the \emph{protection buyer} (to better understand this concept it is useful to think of the protection as an \emph{insurance}). 
In return, the protection seller receives a premium at regular intervals (typically every three months) from the protection buyer.

\subsubsection{Premium Leg}
The premium leg payments are made at the end of each time interval and are proportional to the \textbf{remaining notional} in the tranche (this is another important difference with respect to CDS, where the contract ends as soon as a default occurs).

We can then write the NPV of the premium leg as

\begin{equation}
\mathrm{NPV}_{\mathrm{premium}}^{L,U}=S\sum^{n}_{i=1}D(d_i)\cfrac{(d_i - d_{i-1})}{360}\left(F(U-L)-\mathbb{E}[TL_{d-1}^{L,U}]\right)
\label{eq:cdo_npv_premium}
\end{equation}
where $n$ is the number of payment dates, $D(d_i)$ is the discount factor, $S$ is the annualized premium. The expected value represents the tranche loss at time $d_{i-1}$. Note that for simplicity we are ignoring that the default may take place at any time between each payment date.

\subsubsection{Default Leg}
The default leg represents the cash flows paid to the protection buyer upon losses occurring in the considered tranche. 

The NPV of the leg can be expressed as
\begin{equation}
\mathrm{NPV}_{\mathrm{default}}^{L,U}=\sum_{i=1}^{n}D(d_i)\left(\mathbb{E}[TL_{d_i}^{L,U}]-\mathbb{E}[TL_{d_{i-1}}^{L,U}]\right)
\label{eq:cdo_npv_default}
\end{equation}
where the argument in parenthesis is the expected losses between time $d_{i-1}$ up to $d_i$. 

From Equations~\ref{eq:cdo_npv_premium} and~\ref{eq:cdo_npv_default} it is clear that the key ingredient for the valuation of a CDO is the calculation of $\mathbb{E}[TL_{d_i}^{L,U}]$ which appears in both. Luckily using the Gaussian copula it is relatively easy to compute it. Indeed we know that 
\begin{equation}
TL_{t}^{L,U}=\mathrm{max}(\mathrm{min}(\frac{l}{N}\cdot F(1-R), U)-L, 0)
\label{eq:tl}
\end{equation}
where the only random variable is the number of defaults $l$. We also know that 
\begin{equation}
\mathbb{E}[TL_{t}^{L,U}] = \sum_{l=0}^{N}TL_{t}^{L,U}\cdot \int_{-\infty}^{\infty} DP(l(t|M)=j) \phi(M)dM
\label{eq:etl}
\end{equation}

And has we have already seen this calculation can be carried on without too much effort. The large popularity of the Gaussian copula just resides in this, it allows to compute very quickly very complicated contracts like CDOs which usually involve a large number of correlated names.

\subsection{CDO Fair Value}
The \emph{fair value} of a CDO tranche is that value of the premium $S^*$ for which the expected value of the premium leg equals the expected value of the default leg and for what we have seen depends on the expected value of the tranche loss function.

Given Equations~\ref{eq:cdo_npv_premium} and~\ref{eq:cdo_npv_default} it can be expressed as

\begin{equation}
S^* = \cfrac{\mathrm{NPV_{default}}^{L,U}}{\sum^{n}_{i=1}D(d_i)\cfrac{(d_i - d_{i-1})}{360}\left(F(U-L)-\mathbb{E}[TL_{d-1}^{L,U}]\right)}
\label{eq:cdo_fair_value}
\end{equation}

Equation~\ref{eq:cdo_fair_value} defines the CDO fair value, but can also be used to calibrate the implied correlation parameter from the market. This can be obtained by plugging into the equation the market premium value and solve for the correlation parameter $\rho$. It must be noted though that the derivation of the implied correlation coefficient is not that easy since the assumed model is so simple that may not fit well to real data. Indeed it is most likely that there will be different implied $\rho$s for each tranche (while the model assumes the same correlation everywhere).

As a last consideration we have to notice that all equations shown previously have been derived under the assumptions of the same notional and recovery rate for each entity in the portfolio. Nonetheless their generalization to different notional and recovery rate is pretty straightforward.
 
%Each tranche has an attachment percentage and a detachment
%percentage. When the cumulative percentage loss of the portfolio reaches
%the attachment percentage, investors in the tranche start to lose their
%principal, and when the cumulative percentage loss of principal reaches
%the detachment percentage, the investors in the tranche lose all their
%principal and no further loss can occur to them.
%
%A common analogy compares the cash flow from the CDO's portfolio of securities (say mortgage payments from mortgage-backed bonds) to water flowing into cups of the investors where senior tranches were filled first and overflowing cash flowed to junior tranches, then equity tranches. If a large portion of the mortgages enter default, there is insufficient cash flow to fill all these cups and equity tranche investors face the losses first.

The following code implements a \texttt{CollDebtObligation} class.

\begin{ipython}
from finmarkets import DiscountCurve, CreditCurve, generate_swap_dates
from scipy.integrate import quad
from scipy.stats import norm, binom
import numpy as np
from numpy import exp, sqrt
from datetime import date
from dateutil.relativedelta import relativedelta

class CollDebtObligation:
    def __init__(self, notional, names, tranches, rho, cc,
                 start_date, spreads, maturity, tenor=3, recovery=0.4):
        self.notional = notional
        self.names = names
        self.tranches = tranches
        self.payment_dates = generate_swap_dates(start_date, maturity * 12, tenor)
        self.spreads = spreads
        self.rho = rho
        self.recovery = recovery
        self.cc = cc

    def one_factor_model(self, M, Q, l, L, U):
        P = norm.cdf((norm.ppf(Q) - sqrt(self.rho) * M) / (sqrt(1 - self.rho)))
        b = binom(self.names, P)
        return b.pmf(l) * norm.pdf(M) * max(min(l/self.names *
               self.notional * (1 - self.recovery), U) - L, 0)

    def expected_tranche_loss(self, d, L, U):
        Q = 1 - self.cc.ndp(d)
        v = 0 
        for l in range(self.names+1):
            i = quad(self.one_factor_model, -np.inf, np.inf,
                args=(Q, l, L, U))[0]
            v += i
        return v

    def npv_premium(self, tranche, dc):
        L = self.tranches[tranche][0] * self.notional
        U = self.tranches[tranche][1] * self.notional
        v = 0
        for i in range(1, len(self.payment_dates)):
            ds = self.payment_dates[i - 1]
            de = self.payment_dates[i]
            D = dc.df(de)
            ETL = self.expected_tranche_loss(ds, L, U)
            v += D * (de - ds).days / 360 * max((U - L) - ETL, 0)
        return v * self.spreads[tranche]

    def npv_default(self, tranche, dc):
        U = self.tranches[tranche][1] * self.notional
        L = self.tranches[tranche][0] * self.notional
        v = 0
        for i in range(1, len(self.payment_dates)):
            ds = self.payment_dates[i - 1]
            de = self.payment_dates[i]
            ETL1 = self.expected_tranche_loss(ds, L, U)
            ETL2 = self.expected_tranche_loss(de, L, U)
            v += dc.df(de) * (ETL2 - ETL1)
        return v

    def npv(self, tranche, dc):
        return self.npv_default(tranche, dc) - self.npv_premium(tranche, dc)

    def fair_value(self, tranche, dc):
        num = self.npv_default(tranche, dc)
        den = self.npv_premium(tranche, dc) / self.spreads[tranche]
        return num / den
\end{ipython}

\begin{finmarkets}
Clearly this class too has to go into the \texttt{finmarkets} module.
\end{finmarkets}
Let's test the class with a CDO with 1 year maturity and a reference portfolio of 125 names. Each of them have the same default probabilities, defined through a credit curve (\texttt{cc}) and the correlation is set to 0.3. The risk free rate is flat at 5\%. The tenor has been chosen to 12 months mainly for performance reasons since the computation of the expected losses implies the integration in Equations~\ref{eq:tl} and \ref{eq:etl}, for each name, and for each payment date and this can be quite time consuming.
Tranches and premiums are defined as follows:
\begin{itemize}
	\item equity: [0.0, 0.03] (spread 0.15);
	\item mezzanine: [0.03, 0.06] (spread 0.07);
	\item senior: [0.06, 0.09] (spread 0.03);
	\item super-senior: [0.09, 1.0] (spread 0.01).
\end{itemize}

In the following example we are going to evaluate the fair value of each tranche of the contract.

\begin{ipython}
from finmarkets import DiscountCurve, CreditCurve
from datetime import date
from dateutil.relativedelta import relativedelta

pillar_dates = []
df = []
observation_date = date.today()
for i in range(2):
    pillar_dates.append(observation_date + relativedelta(years=i))
    df.append(1 / (1 + 0.05) ** i)
dc = DiscountCurve(observation_date, pillar_dates, df)
cc = CreditCurve([observation_date + relativedelta(years=i) for i in range(5)],
                 [1, 0.99, 0.97, 0.95, 0.93])
tranches = [[0.0, 0.03], [0.03, 0.06], [0.06, 0.09], [0.09, 1.0]]
spreads = [0.15, 0.07, 0.03, 0.01]
cdo = CollDebtObligation(100e6, 125, tranches, 0.3, cc,
                         observation_date, spreads, 1, 12)
          
for i in range(len(tranches)):
    print ("Tranche {} ({}): {:.5f}".format(i, tranches[i], cdo.fair_value(i, dc)))
\end{ipython}
\begin{ioutput}
Tranche 0 ([0.0, 0.03]): 0.15942
Tranche 1 ([0.03, 0.06]): 0.02505
Tranche 2 ([0.06, 0.09]): 0.00773
Tranche 3 ([0.09, 1.0]): 0.00017
\end{ioutput}

As expected the equity tranche has the higher fair value being the riskier, while the senior tranche is the safest hence the one with lower premium.

\section{Complex Correlation Structures and the Financial Crisis}
\label{complex-correlation-structures-and-the-financial-crisis}

In the derivation of the Gaussian Copula Model we have used the normal distribution. However, we could have used other and more complex copulas as well. For example we might want to assume the correlation is non-symmetric which is useful in finance when correlations become very strong during market crashes and returns very negative.

In fact, Gaussian copulas are said to have played a key role in the 2008 financial crisis as tail-correlations were severely underestimated. Consider a set of mortgages in a CDO: they are clearly correlated, since if one mortgage fails, the likelihood that another failing is increased. In the early 2000s, the banks only knew how to model the marginals of the default rates. Then an (in)famous paper by Li~\cite{bib:copula_li} suggested to use Gaussian copulas to model the correlations between those marginals. Rating agencies relied on this model so heavily, that severely underestimated the risk and gave false ratings\ldots

If you are interested in the argument read this paper~\cite{bib:copula_and_2008} for an excellent description of Gaussian copulas and the Financial Crisis, which argues that different copula choices would not have made a difference but instead the assumed correlation was way too low.

\section*{Exercises}
\input{correlation_ex_text}

\begin{thebibliography}{9}
\bibitem{bib:cdo} J. C. Hull, \emph{Options, Futures and Other Derivatives, 7th Ed.}, Credit Derivatives (Ch. 23), Pearson Prentice Hall, 2009
\bibitem{bib:copula_and_2008} S. Watts, \href{http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf}{\emph{Gaussian Copula and Financial Crisis}} [Online]
\bibitem{bib:copula_li}D. X. Li, \href{http://www.maths.lth.se/matstat/kurser/fmsn15masm23/default.pdf}{\emph{On Default Correlation: A Copula Function Approach}}, RiskMetrics Group, 2000 [Online]
\end{thebibliography}
