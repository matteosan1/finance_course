\chapter{Portfolio Optimization}\label{portfolio-optimization}

Portfolio optimization models look for the optimal way to make investments. Usually investors expect either a maximum return for a given level of risk or a given return for a minimum risk so these models are typically based on two criteria: maximization of the expected return and/or minimization of the risk.

While the concept of return is straightforward there are a variety of risk measures. The most popular one is the variance in return and we will mainly focus on it in this Chapter.

Some notations that will be used later are

\begin{itemize}
\tightlist
\item
  portfolio expected return: 
  \begin{equation} 
  	\mathbb{E}(R_{p}) = \sum _{i}w_{i} \mathbb{E}(R_{i}) = \mathbf{w}\cdot \mathbb{E}(\mathbf{R}) = \mathbf{w}^T \mathbb{E}(\mathbf{R})=
      \begin{bmatrix}
      w_1 \\ 
      w_2 \\ 
      \vdots \\
      w_n
      \end{bmatrix}
      \begin{bmatrix}
      \mathbb{E}(R_1) & \mathbb{E}(R_2) & \cdots & \mathbb{E}(R_n)
      \end{bmatrix}
  \end{equation} 
  where \(R_{p}\) is the return on the portfolio, \(R_{i}\) is the return on asset \(i\) and \(w_{i}\) is the weighting of component asset \(i\) (that is, the proportion of asset \(i\) in the portfolio) and \(\sum_{i}w_i = 1\) and \(0 \le w_i \le 1\);
\item
  portfolio return variance:
  \begin{equation}
  \begin{aligned}
  \sigma _{p}^{2} = &\sum _{i}\sum _{j}w_{i}w_{j}\sigma _{ij} = \mathbf{w}^T\Sigma\mathbf{w} =
  \begin{bmatrix}
  w_1 \\ 
  w_2 \\ 
  \vdots \\
  w_n
  \end{bmatrix}
  \begin{bmatrix}
  \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
  \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
  \vdots & & \\
  \sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn} \\
  \end{bmatrix}
  \begin{bmatrix}
  w_1 & w_2 & \cdots & w_n
  \end{bmatrix} =\\ 
  &\begin{bmatrix}
  \sigma_{11} *w_1 + \sigma_{12} *w_2 + \cdots + \sigma_{1n}*w_n \\
  \sigma_{21} *w_1 + \sigma_{22} *w_2  +\cdots + \sigma_{2n}*w_n \\
  \vdots \\
  \sigma_{n1} *w_1 + \sigma_{n2}*w_2 + \cdots + \sigma_{nn}*w_n \\
  \end{bmatrix}
  \begin{bmatrix}
  w_1 & w_2 & \cdots & w_n
  \end{bmatrix}
  \end{aligned}
  \end{equation}
  where \(\sigma\) is the (sample) standard deviation of the periodic returns on an asset, and \(\rho _{ij}\) is the correlation coefficient between the returns on assets \(i\) and \(j\). For a brief introduction to matrices see Chapter~\ref{sec:matrices};
\item
  portfolio return volatility (standard deviation):
  \begin{equation}
  	\sigma _{p}= \sqrt{\sigma _{p}^{2}}
  \end{equation}
\end{itemize}

\section{Modern Portfolio Theory}
\label{the-markowitz-meanvariance-portfolio-model}

Although investors may expect a particular return when buying a stock, they also may be disappointed or pleasantly surprised, because fluctuations in stock prices result in fluctuating returns. 

The Modern Portfolio Theory (MPT), introduced by Markowitz, defines risk as the possibility that actual returns will deviate from expected returns (the degree of potential fluctuation determines the degree of risk).
So it assumes that an investor has two considerations when constructing an investment portfolio: expected return and variance in return (i.e. measure of risk). 

Hence the Markowitz model requires two major information:

\begin{itemize}
\tightlist
\item the estimated expected return for each candidate investment;
\item the covariance matrix of returns, which characterizes not only the individual variability of the return on each investment, but also how each investment's return tends to move with the others (correlation).
\end{itemize}

In the following examples we are going to use real data
from:  AAPL (Apple), AMZN (Amazon), FB (Facebook), GOOG (Google), NFLX (Netflix). It can be downloaded with \texttt{yfinance}. Otherwise it can be directly used from \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/portfolio_data.csv}{portfolio\_data.csv}.

\begin{ipython}
import yfinance as yf

proxy = yf.Tickers(['AAPL', 'AMZN', 'FB', 'GOOG', 'NFLX'])
df = proxy.history(start='2014-03-27', end='2018-03-27')['Close']
# uncomment the following line if reading from file
# df = pd.read_csv("portfolio_data.csv", index_col="date")

print (df.head())
\end{ipython}
\begin{ioutput}
                AAPL       AMZN        FB       GOOG      NFLX
date
2014-03-27 17.202097 338.470001 60.970001 556.930969 52.025715
2014-03-28 17.182892 338.290009	60.009998 558.456787 51.267143
2014-03-31 17.179056 336.369995	60.240002 555.445007 50.290001
2014-04-01 17.336205 342.989990	62.619999 565.607117 52.098572
2014-04-02 17.365013 341.959991	62.720001 565.447571 51.840000
\end{ioutput}
 
Data, shown in Figure~\ref{fig:stocks}, is made of the historical series of the closing prices in the last 5 years. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/portfolio_sample}
\caption{Historical series of the closing price of five companies. To compare them, prices have been normalized to the first value in each series}
\label{fig:stocks}
\end{figure}

The main quantites (e.g. daily returns, covariance matrix,\ldots) can be easily computed with \texttt{pandas}.
Variances can be added across time intervals if return in one interval is uncorrelated with those in others. The correlation of returns across time intervals (called \emph{autocorrelation}) is in general close to zero for most assets. This means that variances will grow with the length of the forecast horizon and the risk will grow with the square root of the forecast horizon. 
Thus, a 5\% annual risk is equivalent to a 2.5\% risk over the first quarter or a 10\% risk over four years. 

This relationship can be used to “annualize” risk, i.e. standardize risk numbers to an annual period. The observed daily return standard deviation ($\sigma_{daily}$) can be converted to annual risk according to
\begin{equation}
\sigma_{yearly} = \sigma_{daily}\cdot\sqrt{252}
\end{equation}

\begin{ipython}
daily_returns = df.pct_change()
returns = daily_returns.mean()*252
print (returns)
\end{ipython}
\begin{ioutput}
AAPL    0.240921
AMZN    0.414775
FB      0.263708
GOOG    0.173464
NFLX    0.527628
dtype: float64
\end{ioutput}

\begin{ipython}
covariance = daily_returns.cov()*252
print (covariance)
\end{ipython}
\begin{ioutput}
          AAPL      AMZN        FB      GOOG      NFLX
AAPL  0.051967  0.025182  0.026075  0.022764  0.028064
AMZN  0.025182  0.085876  0.041196  0.039654  0.048576
FB    0.026075  0.041196  0.069723  0.036337  0.044753
GOOG  0.022764  0.039654  0.036337  0.052001  0.040630
NFLX  0.028064  0.048576  0.044753  0.040630  0.178332
\end{ioutput}
    
\subsection{Portfolio Simulation}
In order to "simulate" a portfolio of $n$ assets it is enough to throw $n$ random weights with the only constraint that they had to sum up to 1. 
In Figure~\ref{fig:mc_portfolio} a large number of simulated portfolios, each made of different proportion of the five assets mentioned above, are shown in a return vs volatility plot. 
Note that in these simulation no attempt of any optimization whatsoever has been made yet.

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.7\textwidth]{figures/return_variance}
\caption{Scatter plot of expected return vs volatility of a large number of simulated portfolios.}
\label{fig:mc_portfolio}
\end{figure}

Investors may use \emph{short sales} in their portfolios (a portfolio is short in those stocks with negative weights). 
Although short selling extends the set of possible portfolios we are not going to consider it here.

\section{Optimisation}\label{optimization}

MPT model states that \textbf{the weights of a portfolio should be chosen such that its volatility (or its variance) is minimised}. 
The application of this model reduces to a minimization problem: given the covariance matrix of the portfolio $\Sigma$, we need to find

\begin{equation}
\underset{\mathbf{w}}{\min}\{\sigma_p^2\} = \underset{\mathbf{w}}{\min}\{\mathbf{w}^T\Sigma\mathbf{w}\}
\end{equation}
with the constraints \(\sum_{i}w_i = 1\) and \(0 \le w_i \le 1\).

In \texttt{python} we have already seen how to solve minimisation problems (see bootstrapping in Chapter~\ref{sec:swaps-and-bootstrapping}) so it is enough to repeat the same steps:

\begin{itemize}
\tightlist
\item define an objective function (i.e. the portfolio variance);
\item define a set of constraints (i.e. $\sum w_i = 1$);
\item set an initial guess for the weights;
\item run the algorithm with \texttt{scipy.optimize.minimize}.
\end{itemize}

\begin{ipython}
import numpy as np
from scipy.optimize import minimize

def sum_weights(w):
    return np.*@sum@*(w) - 1

def risk(w, cov):
    return w.T.dot(cov.dot(w))

num_assets = 5
constraints = ({'type': 'eq', 'fun': sum_weights},)
bounds = tuple((0, 1) for asset in range(num_assets))
weights = [1./num_assets for _ in range(num_assets)]
opts = minimize(risk, weights, args=(covariance,),
                bounds=bounds, constraints=constraints)
print (opts)
print ("Expected portfolio return: {:.3f}".format(returns.dot(opts.x))
\end{ipython}
\begin{ioutput}
    fun: 0.036290306589982405
    jac: array([0.07274766, 0.07278468, 0.07233336, 
                0.07242217, 0.0726624 ])
message: 'Optimization terminated successfully.'
   nfev: 63
    nit: 9
   njev: 9
 status: 0
success: True
x: array([0.44644822, 0.06472903, 0.12215803, 0.36453326, 0.00213147])

Expected portfolio return: 0.231
\end{ioutput}

The optimization recommends to devote about 44\% of the portfolio to AAPL, about 6\% to AMZN, 12\% to FB and so on\ldots The expected return is about 23\%, with a variance of about 0.036 or, equivalently, a standard deviation of 0.19.

In this example we based the model simply on straightforward statistical data derived from daily returns. However it could be possible, rather than just use historical data, to base this estimate on information about expected future performance of the asset.

\subsection{Limits of the Markowitz Model}
\label{limits-of-the-markowitz-model}

Despite the significant utility of the Markowitz theory, there are some major limitations in this model:

\begin{enumerate}
\tightlist
\item the tendency to produce extreme portfolios combining extreme shorts with extreme longs. As a result,portfolio managers generally do not trust these extreme weights. This problem is typically caused by
estimation errors in the mean return vector and covariance matrix;
\item the portfolio weights tend to be extremely sensitive to very small changes in the expected returns. For example, even a small increase in the expected return of just one asset can dramatically alter the optimal composition of the entire portfolio;
\item the presence of heavy tails in the return distributions can result in significant errors in covariance estimates as well.
\end{enumerate}

Extensions of the Markowitz model are defined in~\cite{bib:post_modern_theory} and~\cite{bib:black_litterman}, although they are beyond the scope of these lectures. 

\section{Efficient Frontier}
\label{efficient-frontier}
There is no precise way for an investor to determine the “correct” trade off between risk and return. The desired higher expected return needs to be paid for with higher risk. Thus, one is frequently interested in looking at the relative distribution of the two.

In finance terminology, means to trace the \emph{efficient frontier of return and risk}. This can be done in two alternative ways: either solving for the \emph{minimum} variance portfolio over a range of expected return values or solving for the \emph{maximum} return portfolio over a range of variance values.

The following example computes the efficient frontier plot (shown in Fig.~\ref{fig:efficient_frontier}) using the first method with the return ranging from 0.20 to 0.45.
In this case an additional constraint on the portfolio return has to be added beside the one on the sum of weights.

\begin{ipython}
def efficient_frontier(w, asset_returns, target_return):
    portfolio_return = asset_returns.dot(w)
    return (portfolio_return - target_return)

results = []
for t_return in np.arange(0.20, 0.45, 0.005):
    constraints = ({'type': 'eq', 'fun': efficient_frontier,
                    'args':(returns, t_return,)},
                   {'type': 'eq', 'fun': sum_weights})
    weights = [1./num_assets for _ in range(num_assets)]
    opts = minimize(risk, weights, args=(covariance,),
                bounds=bounds, constraints=constraints)
    results.append((np.sqrt(opts.x.T.dot(covariance.dot(opts.x))),
                    returns.dot(opts.x)))
\end{ipython}

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/efficient_frontier}
\caption{Efficient frontier for our example portfolio obtained minimizing the the variance and requiring an expected return between 0.02 and 0.45.}
\label{fig:efficient_frontier}
\end{figure}

Efficient portfolios offer investors the highest possible expected return for a given level of risk. 
An investor seeking high expected returns and low volatility should invest only in efficient portfolios and will choose from the set of efficient portfolios based on her risk tolerance.
    
\subsection{Portfolios with a Risk-Free Asset}
\label{portfolios-with-a-risk-free-asset}

When one of the investments available is a risk-free asset, then the efficient frontier transform to a particularly simple form. The risk–return combinations of the risk-free investment and a risky portfolio lie on a straight line connecting the two investments: the \emph{capital allocation line} (CAL). The slope of the CAL measures the trade off between risk and return: a higher slope means investors receive a higher expected return in exchange for taking on more risk.

The capital allocation line aids investors in choosing how much to invest in a risk-free asset and one or more risky assets.

The simplest example of such kind of portfolios is the one containing only two assets: a risk-free Treasury bill and a stock. Assume that the expected return of the Treasury bill is \(\mathbb{E}(R_f)=3\%\) (its risk is 0\%). Further, assume that the expected return of the stock is \(\mathbb{E}(R_r)=10\%\) and its standard deviation is \(\sigma_r=20\%\). The question that needs to be answered for any individual investor is how much to invest in each of these assets.

The expected return (\(\mathbb{E}(R_p)\)) of this portfolio is calculated as follows:

\begin{equation*} 
\mathbb{E}(R_p) = \mathbb{E}(R_f)\cdot w_f + \mathbb{E}(R_r)\cdot (1- w_f) 
\end{equation*}
where \(w_f\) is the relative allocation to the risk-free asset.

The calculation of this portfolio risk is simple because the standard deviation of the Treasury bill is 0\%. Thus

\begin{equation*} 
\sigma_p = (1-w_f)\cdot \sigma_r 
\end{equation*}
In this simple example, if an investor invested 100\% into the risk-free asset (\(w_f=1\)), the expected return would be 3\% and the risk of the portfolio would be 0\%. Otherwise, investing 100\% into the stock (\(w_f=0\)) would give an investor an expected return of 10\% and a portfolio risk of 20\%. If the investor allocated 25\% to the risk-free asset and 75\% to the risky asset, the portfolio expected return and risk calculations would be

\[ \mathbb{E}(R_p) = (3\% \cdot 25\%) + (10\% \cdot 75\%) = 0.75\% + 7.5\% = 8.25\% \]

\[ \sigma_p = 75\% \cdot 20\% = 15\% \]
\noindent
If you plot these three points they lay on a line.

If we added a risk-free asset, with an expected return of 10\%, to the five risky ones considered so far we could repeat the Markowitz minimisation to determine the efficient frontier of the resulting portfolio. 

\begin{ipython}
num_assets = 6
returns_rf = np.append(returns.values, 0.10)
cov_rf = np.column_stack((covariances.values, np.array([0, 0, 0, 0, 0])))
cov_rf = np.row_stack((cov_rf, np.array([0, 0, 0, 0, 0, 0])))
print (cov_rf)

result_rf = []

for t_ret in np.arange(0.1, 0.4, 0.01):
    weights = [1/n_assets for _ in range(num_assets)]
    bounds = [(0, 1) for _ in range(num_assets)]
    constraints = [{'type':'eq', 'fun':sum_weights},
                   {'type':'eq', 'fun':target_return, 'args':(returns_rf, t_ret)}]

    opts = minimize(risk, weights, bounds=bounds, 
                    constraints=constraints, args=(cov_rf))

    result_rf.append((np.sqrt(risk(opts.x, covariance)),
                      returns.dot(opts.x)))
\end{ipython}
\begin{ioutput}
[[0.05190222 0.02503721 0.02573699 0.02245413 0.02775968 0.        ]
 [0.02503721 0.08583929 0.04102487 0.03950122 0.04841167 0.        ]
 [0.02573699 0.04102487 0.06955025 0.03612685 0.04452847 0.        ]
 [0.02245413 0.03950122 0.03612685 0.05179662 0.04038995 0.        ]
 [0.02775968 0.04841167 0.04452847 0.04038995 0.17829826 0.        ]
 [0.         0.         0.         0.         0.         0.        ]]
\end{ioutput}

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/cal}
\caption{Comparison of efficient frontier with a risk-free asset (red) and with risky asset only (blue).}
\label{fig:cal}
\end{figure}
    
As expected the efficient frontier has become a straight line, tangent to the frontier of the risky assets (Fig.~\ref{fig:cal}). When the target is 10\% the entire investment is allocated to the risk-free asset, as the target increases the fraction of risky assets grows proportionally to the volatility. 

It is important to notice that in general the relative proportions among the risky investments do not change with or without the risk-free asset. Only the allocation between the risk-free and the risky parts varies.

\section{The Sharpe Ratio}
\label{the-sharpe-ratio}
The goal of an investor who is seeking to earn the highest possible expected return for any level of volatility is to find the portfolio that generates the steepest possible line when combined with the risk-free investment. This line slope is called the \emph{Sharpe ratio} of the portfolio.

For a portfolio of risky assets let be

\begin{itemize}
\tightlist
\item \(R_r\) its expected return;
\item \(\sigma_r\) its standard deviation in return;
\item \(r_f\) the return of a risk-free asset.
\end{itemize}

A plausible single measure of attractiveness of a portfolio (as opposed to the two measures, risk and return proposed by MPT model) is the Sharpe ratio:

\begin{equation} 
\mathcal{S} = \cfrac{R_r - r_f}{\sigma_r} 
\end{equation}
\noindent
In words, it measures how much additional return is achieved for the higher risk taken on, relative to investing all in the risk-free asset. 

The portfolio that maximizes this ratio has some interesting properties. Suppose that

\begin{itemize}
\tightlist
\item
  \(R_\textrm{target}\) the desired target return;
\item
  \(w_r\) the fraction of wealth placed in the portfolio (the rest placed in the risk-free asset).
\end{itemize}
\noindent
To meet the target return the weights need to be chosen such that:

\begin{equation*} 
(1 - w_r) * r_f + w_r * R_r =R_\textrm{target} 
\end{equation*}
\noindent
The standard deviation of the investment is: \(w_r\cdot \sigma_r\). Solving for \(w_r\) in the equation above, we get:

\begin{equation*} 
	w_r = \cfrac{R_\textrm{target} - r_f}{R_r - r_f} 
\end{equation*}
Thus, the standard deviation of the portfolio is:

\begin{equation*} 
w_r\cdot \sigma_r = \left(\cfrac{R_\textrm{target} - r_f}{R_r - r_f}\right)\cdot \sigma_r 
\end{equation*}
Minimising the portfolio standard deviation means:

\begin{equation} 
\textrm{min}\left\{\cfrac{R_\textrm{target} - r_f}{R_r - r_f}\cdot \sigma_r\right\} = \textrm{min}\left[\cfrac{R_\textrm{target} - r_f}{\mathcal{S}}\right]
\end{equation}

Since in the above formula both $R_{\textrm{target}}$ and $r_f$ are constant, the minimization is equivalent to the maximization of the denominator of the expression, hence of the Sharpe ratio $\mathcal{S}$.

\begin{equation} 
\textrm{min}\left\{\cfrac{R_\textrm{target} - r_f}{\mathcal{S}}\right\}
\implies\textrm{max}\left\{\cfrac{R_r - r_f}{\sigma_r}\right\}
\end{equation}

So, regardless of investor risk/return preference, \emph{the investment should go in the portfolio that maximises the Sharpe ratio}, because it will be the one that minimize the risk (i.e. standard deviation) and maximise the return at the same time.

Let's compute the Sharpe portofolio with our sample.

\begin{ipython}
num_assets = 5
rf_return = 0.10

def negativeSharpeRatio(w, asset_returns, cov, r0):
    Rp = asset_returns.dot(w)
    sigma = np.sqrt(w.T.dot(cov.dot(w)))
    return -(Rp - r0) / sigma

constraints = ({'type': 'eq', 'fun': sum_weights})
bounds = tuple((0, 1) for asset in range(num_assets))
weights = [1./num_assets for _ in range(num_assets)]
opts = minimize(negativeSharpeRatio, weights,
                args=(returns, covariance, rf_return),
                bounds=bounds, constraints=constraints)
print (opts)
print ("Sharpe ratio: ", -opts.fun)
\end{ipython}
\begin{ioutput}
    fun: -1.2577787922793253
    jac: array([-0.37974137, -0.38030942, -0.26630181,  0.02906457, 
                -0.38027261])
message: 'Optimization terminated successfully.'
   nfev: 42
    nit: 6
   njev: 6
 status: 0
success: True
      x: array([1.24962448e-01, 5.40550205e-01, 0.00000000e+00, 
                1.26201133e-16, 3.34487347e-01])

Sharpe ratio:  1.2577787922793253
\end{ioutput}

Figure~\ref{fig:sharpe_ratio} shows the optimization results. Notice that in general the relative proportions of the stocks are the same as in the previous case (at the same level of return) where we explicitly included a risk free asset (0.12, 0.54, 0., 0., 0.33).

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/sharpe_ratio}
\caption{Sharpe portfolio (green cross) compared to the efficient frontier with a risk-free asset (red) and with risky asset only (blue).}
\label{fig:sharpe_ratio}
\end{figure}

So using the Sharpe ratio gives a portfolio that is on the efficient frontier, and gives the maximum return relative to putting all our money in the risk-free asset so is on the CAL too. Graphically it sits in the only place that belongs to both curves: the tangent point.

Usually, any Sharpe ratio greater than 1.0 is considered acceptable to good by investors. A ratio higher than 2.0 is rated as very good. A ratio of 3.0 or higher is considered excellent. A ratio under 1.0 is sub-optimal.

\section{Portfolio Diversification}

A security total risk can be divided into \emph{unsystematic}, the risk portion peculiar to the company that can be diversified away, and systematic, the non-diversifiable portion that is related to the movement of the stock market and is therefore unavoidable. 

Diversification is a common topic in portfolio construction and allows to combine risky stocks so that the resulting portfolio is less risky than the sum of its components. Although such diversification is a familiar notion, it may be worthwhile to review the manner in which diversification reduces risk.

Suppose there are two companies located on an isolated island whose chief industry is tourism. One company manufactures suntan lotion; its stock predictably performs well in sunny years and poorly in rainy ones. The other company produces umbrellas; its stock performs equally poorly in sunny years and well in rainy ones. Each company earns a 12\% average return.

In purchasing either stock, investors incur a great amount of risk because of the price variability driven by fluctuations in weather conditions. Investing half the funds in the suntan lotion stock and half in the umbrella manufacturer stock, however, results in a return of 12\% regardless of which weather condition prevails. Portfolio diversification thus transforms two risky stocks, each with an average return of 12\%, into a riskless portfolio certain of earning the expected 12\%.

Unfortunately, the perfect negative relationship between the returns on these two stocks is very rare in real world. To some extent, corporate securities move together, so complete elimination of risk through simple portfolio diversification is impossible. However, as long as some lack of parallelism in the returns of securities exists, diversification will always reduce risk.
Empirical studies have demonstrated that risk can be virtually eliminated in portfolios of 30 to 40 randomly selected stocks. Of course, if investments are made in closely related industries, more securities are required to eradicate it.

When using the standard variation of the portfolio return as a measure of the risk, as in the Markowitz model, it is easy to show how diversification allows to reduce the risk. 
Indeed the standard deviation of a portfolio is not the weighted average of the standard deviations of the component stocks.
For example, suppose the correlation between the return of stocks 1 and 2 is $\rho_{12}$. If the portfolio is equally weighted then

\begin{equation}
\sigma_{P} = \sqrt{(0.5\cdot\sigma_1 )^2 + (0.5\cdot\sigma_2 )^2 + 2\cdot(0.5\cdot\sigma_1)(0.5\cdot\sigma_2)\rho_{12}} \lt (0.5\cdot\sigma_1 ) + (0.5\cdot\sigma_2 )
\end{equation}

The inequality holds unless $\rho_{12}=1$, so in general, for risk, the whole is less than the sum of its parts. 
This is the key to portfolio diversification.

Figure~\ref{fig:diversification} shows the risk of a portfolio made up of IBM and General Electric stocks against the fraction of GE stocks in the portfolio. The curved line represents the risk of the portfolio; the straight line is the sum of the two contributions. The risk of GE is 27.4\% per year, the risk of IBM is 29.7\% per year, and the two are 62.9\% correlated. The gap between the lines is an indication of the benefit of diversification in reducing risk.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/diversification}
\caption{Risk of a portfolio made up from IBM and General Electric against the fraction of GE stock in the portfolio. The curved line represents the risk of the portfolio; the straight line represents the sum of the two single risks.}
\label{fig:diversification}
\end{figure}

%We can see the power of diversification in another example. Given a  portfolio of $N$ stocks, each with risk $\sigma$ and uncorrelated returns, the risk of an equal-weighted portfolio of these stocks will be
%\begin{equation}
%\sigma_P = \cfrac{\sigma}{\sqrt{N}}
%\end{equation}
%
%Assume now that the correlation between the returns of all pairs of stocks is equal to $\rho$. Then the risk of an equally weighted portfolio is:
%\begin{equation}
%\sigma_P = \sigma\cdot\sqrt{\cfrac{1+\rho(N-1)}{N}};\quad \lim_{N \to +\infty} \sigma_P = \sigma\cdot\sqrt{\rho}
%\label{eq:risk_correlation}
%\end{equation}
%which can be further simplified when considering a portfolio contains a very large number of correlated stocks
%
%To get a feel for this, consider the example of an equal-weighted portfolio of the 20 Major Market Index constituent stocks. In December 1992, these stocks had an average risk of 27.8\%, while the equal-weighted portfolio has a risk of 20.4\%. Equation~\ref{eq:risk_correlation} then implies an average correlation between these stocks of 0.52. 

%As we have already seen no measure of unsystematic risk appears in the risk premium, of CAPM model, since it is assumed that diversification has eliminated it.
%
%In the Markowitz model instead diversification is achieved by seeking to combine in a portfolio assets with returns that are less than perfectly positively correlated, in an effort to lower portfolio risk (variance) without sacrificing return, through the reduction of the correlation matrix $\Sigma$.

\section{Risk Parity Portfolio}
\label{risk-parity-portfolio}

An alternative approach to optimize a portfolio is given by the \emph{risk parity}. A risk parity portfolio is an investment allocation strategy which focuses on the allocation of risk, rather than on the allocation of capital. 
Such a portfolio is characterised by having equal risk contributions to the total risk from each individual asset. 

This allocation strategy has gained popularity in the last decades since it is believed to provide better risk adjusted return than capital based allocation strategies.

Risk parity allocation is also referred to as equally-weighted risk contributions portfolio method. Equally-weighted risk contributions is not about \emph{having the same volatility}, it is about having each asset contributing in the same way to the portfolio overall volatility. For this we will have to define the contribution of each asset to the portfolio risk. 

Let's go over a very basic example to better illustrate how to construct a simple risk parity portfolio. Consider a portfolio of \(N\) assets: \(x_{1}, \ldots, x_N\) where as usual the weight of the $i^{th}$ asset is denoted by \(w_{i}\) and all the \(w_{i}\) form the allocation vector \(\mathbf{w}\). Let us further denote the covariance matrix of the assets as \(\Sigma\). The volatility of the portfolio is then defined as:

\begin{equation} 
\sigma_p={\sqrt {\mathbf{w}^T\Sigma \mathbf{w}}} = \sum_{i=1}^{N}\sigma _{i}\qquad\textrm{with}~\sigma _{i} = w_{i}\cdot \cfrac{\partial\sigma_p}{\partial w_{i}}={\cfrac {w_{i}(\Sigma \mathbf{w})_{i}}{\sqrt {\mathbf{w}^T\Sigma \mathbf{w}}}}
\end{equation}
so that \(\sigma _{i}\) can be interpreted as the contribution of the $i^{th}$ asset to the overall risk of the portfolio.

\begin{attention}
\subsubsection{Derivation of $\sigma_i$}
Expressing explicitly in matrix form the standard deviation of the portfolio we get
\[
\begin{split}
\sigma_p={\sqrt {\mathbf{w}^T\Sigma \mathbf{w}}} & =
\sqrt{
	\begin{bmatrix}
	w_{1} \\
	w_{2}
	\end{bmatrix}
	\begin{bmatrix}
	\sigma_{11} & \sigma_{21} \\
	\sigma_{12} & \sigma_{22} 
	\end{bmatrix}
	\begin{bmatrix}
	w_{1} & w_{2} \\
	\end{bmatrix}
}\\
&=
\sqrt{
	\begin{bmatrix}
	w_{1} \\
	w_{2}
	\end{bmatrix}
	\begin{bmatrix}
w_{1}\sigma_{11} + w_{2}\sigma_{12} & w_{1}\sigma_{21} + w_{2}\sigma_{22} \\
	\end{bmatrix}
} \\
&= \sqrt{
w_{1}w_{1}\sigma_{11} + w_{2}w_{1}\sigma_{12} + w_{1}w_{2}\sigma_{21} + w_{2}w_{2}\sigma_{22} }
\end{split}
\]
Now performing the derivative with respect to $w_1$ we obtain
\[\cfrac{\partial\sigma_p}{\partial w_1} = \cfrac{1}{2}\cdot\cfrac{2\cdot w_1\sigma_{11} + 2\cdot w_{2}\sigma_{21}}{\sigma_p} = \cfrac{w_1\sigma_{11} + w_{2}\sigma_{21}}{\sigma_p} = \cfrac{(\Sigma \mathbf{w})_{1}}{\sigma_p}\]
	
Summing up $\sum_{i=1}^{N} w_i\cdot\cfrac{\partial\sigma_p}{\partial w_i}$ we get back $\sigma_p$
\end{attention}

Equal risk contribution then means \(\sigma _{i} =\sigma _{j}\) for all \(i,j\) or equivalently \(\sigma _{i}=\sigma_p/N\). So

\begin{equation}
\sigma _{i} = \cfrac{\sigma_p}{N}={\cfrac {w_{i}(\Sigma \mathbf{w})_{i}}{\sqrt {\mathbf{w}^T\Sigma \mathbf{w}}}}\implies w_{i} = \frac {\sigma_p^{2}}{(\Sigma \mathbf{w})_{i}N}
\label{eq:risk_parity_weights}
\end{equation}
Since we want the previous expression to be true for each $i$, the solution for the weights can be found by solving the minimisation problem

\begin{equation} 
\underset{\mathbf{w}}{\min } \sum _{i=1}^{N}\left[w_{i}-{\frac {\sigma_p^{2}}{(\Sigma \mathbf{w})_{i}N}}\right]^{2} 
\end{equation}
\noindent
where ideally it is required to be 0 the difference between the squared sum of the weights and the theoretical values expressed by Eq~\ref{eq:risk_parity_weights}.

Going back to our data sample let's find out the weights to give us a risk parity portfolio.

\begin{ipython}
num_assets = 5
def risk_parity(w, cov):
    variance = w.T.dot(cov.dot(w))
    *@sum@* = 0
    N = len(w)
    for i in range(N):
        *@sum@* += (w[i] - (variance/(N*cov.dot(w)[i])))**2
    return *@sum@*
	
args = (covariance,)
constraints = ({'type': 'eq', 'fun': sum_weights})
bounds = tuple((0, 1) for asset in range(num_assets))
weights = [1./num_assets for _ in range(num_assets)]
opts = minimize(risk_parity, weights, args=(covariance,),
                bounds=bounds, constraints=constraints)
print (opts)
\end{ipython}
\begin{ioutput}
    fun: 2.2881862766486147e-07
    jac: array([-6.98851511e-04,  1.92391042e-04, -4.08403758e-05, 
                -2.54432521e-05,  1.13250609e-03])
message: 'Optimization terminated successfully.'
   nfev: 38
    nit: 5
   njev: 5
 status: 0
success: True
x: array([0.25863039, 0.18154282, 0.19666705, 0.22190633, 
          0.14125342])
\end{ioutput}

\begin{ipython}
sigma_i = []
std = np.sqrt(opts.x.T.dot(covariance.dot(opts.x)))
for i in range(num_assets):
    a = opts.x[i]*covariance.dot(opts.x)[i]
	sigma_i.append(a/std)
	
for i in range(num_assets):
    print ("Risk contribution for asset {}: {:.3f}%".format(i, sigma_i[i]
                                                            /sum(sigma_i)*100))
\end{ipython}
\begin{ioutput}
Risk contribution for asset 0: 19.974%
Risk contribution for asset 1: 19.999%
Risk contribution for asset 2: 19.990%
Risk contribution for asset 3: 19.992%
Risk contribution for asset 4: 20.045%
\end{ioutput}

Figure~\ref{fig:risk_parity} shows the fraction of risk allocated to each asset with the corresponding weight within the portfolio.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/risk_parity}
\caption{Fraction of risk allocated among the assets of a portfolio. The blue numbers show the corresponding weight of each asset.}
\label{fig:risk_parity}
\end{figure}

\subsection{Risk Budget Allocation}
\label{risk-budget-allocation}

The same technique can be used if we would like to calculate a portfolio with risk budget allocation. In this case we want to associate to each asset a particular level of risk.We can now change the previous equation which was setting every asset risk contribution fraction to $1/N$

\begin{equation} 
\sigma _{i}=\cfrac{\sigma_p}{N} 
\end{equation}
and replace it with the desired fraction of risk (\(f_i\)) specific for each asset

\begin{equation} 
\sigma _{i}=f_i \cdot \sigma_p 
\end{equation}
so that the relation to minimise becomes

\begin{equation} 
\underset{\mathbf{w}}{\min} \sum _{i=1}^{N}\left[w_{i}-{\frac {f_i \cdot \sigma_p^{2}}{(\Sigma \mathbf{w})_{i}}}\right]^{2} 
\end{equation}
\noindent
Translating it into \texttt{python} we get:
\begin{ipython}
def risk_budget(w, target_risk, cov):
    variance = w.T.dot(cov.dot(w))
    *@sum@* = 0
    N = len(w)
    for i in range(N):
        *@sum@* += (w[i] - (target_risk[i]*variance)/(cov.dot(w)[i]))**2
    return *@sum@*
	
f_i = [0.3, 0.2, 0.2, 0.15, 0.15]
args = (f_i, covariance)
constraints = ({'type': 'eq', 'fun': sum_weights})
bounds = tuple((0, 1) for asset in range(num_assets))
weights = [1./num_assets for _ in range(num_assets)]
opts = minimize(risk_budget, weights, args=(f_i, covariance),
                bounds=bounds, constraints=constraints)
print (opts)
\end{ipython}
\begin{ioutput}
    fun: 4.058673684147486e-08
    jac: array([-2.64817707e-04,  3.30937403e-04,  2.14530647e-05, 
                -7.65372775e-05,  3.67853561e-04])
message: 'Optimization terminated successfully.'
   nfev: 45
    nit: 6
   njev: 6
 status: 0
success: True
      x: array([0.3459366 , 0.1800917 , 0.19394454, 0.16890483, 
                0.11112233])
\end{ioutput}
\begin{ipython}  
sigma_i = []
std = np.sqrt(opts.x.T.dot(covariance.dot(opts.x)))
for i in range(num_assets):
    a = opts.x[i]*covariance.dot(opts.x)[i]
    sigma_i.append(a/std)
	
for i in range(num_assets):
    print ("Risk contribution for asset {}: {:.3f}%".format(i, sigma_i[i]
                                                            /sum(sigma_i)*100))    
\end{ipython}
\begin{ioutput}
Risk contribution for asset 0: 29.988%
Risk contribution for asset 1: 20.010%
Risk contribution for asset 2: 19.996%
Risk contribution for asset 3: 14.993%
Risk contribution for asset 4: 15.012%
\end{ioutput}

Figure~\ref{fig:risk_allocation} shows the amount of risk associated to each asset and its weight within the portfolio. 
Indeed for each stock we have allocated the desired amount of risk.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/risk_allocation}
\caption{Fraction of risk allocated among the assets of a portfolio. The blue numbers show the corresponding weight of each asset.}
\label{fig:risk_allocation}
\end{figure}

\subsection{Maximum Diversification Portfolio}
\label{maximum-diversification-portfolio}

Diversification is most often either pursued in tandem with other objective, such as return maximization, or pursued simply by including more asset classes or adding constraints based on intuition.

But it does not have to be this way and diversification can be pursued explicitly as the sole objective in portfolio construction.
In a 2008 paper~\cite{bib:diversification}, the diversification ratio $D$ of a portfolio has been defined as

\begin{equation}
D=\cfrac{\mathbf{w}^T\boldsymbol{\sigma}}{\sqrt {\mathbf{w}^T\Sigma \mathbf{w}}} 
\end{equation}
where $\boldsymbol{\sigma}$ is the vector of volatilities and $\Sigma$ is the covariance matrix. The denominator represents portfolio volatility and the numerator the asset weighted average volatilities. More diversification within a portfolio decreases the denominator and leads to a higher diversification ratio.
Let's construct a portfolio that maximize this ratio.

\begin{ipython}
def diversification_ratio(w):
    w_vol = np.dot(np.sqrt(np.diag(covariance)), w.T)
    port_vol = np.sqrt(np.dot(w.T, np.dot(covariance, w)))
    diversification_ratio = w_vol/port_vol
    return -diversification_ratio
	
bounds = tuple((0, 1) for asset in range(num_assets))
cons = ({'type': 'eq', 'fun': sum_weights},)
#cons = cons + ({'type': 'ineq', 'fun': long_only_constraint},)
weights = [1./num_assets for _ in range(num_assets)]
opts = minimize(diversification_ratio, weights, bounds=bounds,
                constraints=cons)
print (opts)
\end{ipython}
\begin{ioutput}
    fun: -1.3580745811820554
    jac: array([-0.00035974,  0.00029349, -0.00035594,  0.00077944,  
                 0.00016446])
message: 'Optimization terminated successfully.'
   nfev: 36
    nit: 5
   njev: 5
 status: 0
success: True
      x: array([0.34867985, 0.18062199, 0.15557008, 0.1235694, 0.19155868])
\end{ioutput}
\begin{ipython}
ret = np.sum(returns*opts.x)
vol = np.sqrt(opts.x.T.dot(np.dot(covariance, opts.x))) 
print ("Return: ", ret)
print ("Vol: ", vol)
print ("Diversification: ", -opts.fun)
\end{ipython}
\begin{ioutput}
Return:  0.32143376694380044
Vol:  0.20743461253140413
Diversification:  1.360393553158368
\end{ioutput}

Figure~\ref{fig:max_div} shows how the maximum diversified portfolio compares to the efficient frontier.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{figures/max_div}
\caption{Portfolio constructed with the maximum diversification technique shown in the return/variance plane.}
\label{fig:max_div}
\end{figure}

\section{Capital Asset Pricing Model}
\label{sec:capm}
The Capital Asset Pricing Model (CAPM) describes the relationship between asset expected returns and \emph{systematic risk} of the market. No measure of unsystematic risk appears in the risk premium for in the world of CAPM diversification has already eliminated it.

Sharpe~\cite{bib:capm_sharpe} and Lintner~\cite{bib:capm_lintner} developed the Capital Asset Pricing Model whose central insight is that the riskiness of an asset is not measured by the standard deviation of its return but by its beta. In particular, there is a linear relationship
between the expected return of any security (or portfolio) and the expected return of the market
portfolio. It is given by

\begin{equation}
r_i = r_f + \beta_i(r_m-r_f)
\label{eq:capm}
\end{equation}
where:
\begin{itemize}
\item $r_i$ is the expected return of the $i^{th}$ security;
\item $r_f$ is the risk-free rate with zero standard deviation (e.g. risk-free asset includes Treasury Bills as they are backed by the U.S. government);
\item $r_m - r_f$ is the risk premium, $r_m$ denotes the market return including all securities in the market, whose proxy can be an index like SP500;
\item $\beta_i$ is a measure of $i^{th}$ asset volatility in relation to the overall market. $\beta$ is used in the CAPM to describe the relationship between market risk, and expected return.
\end{itemize}
	
The relationship between risk ($\beta$) and expected return is called \emph{Security Market Line} (SML). An example of this line is shown in Fig.~\ref{fig:sml}.
    
In the freely competitive financial markets described by CAPM, no security can sell for long at prices low enough to yield more than its appropriate return on the SML (\emph{undervalued} asset). The security would then be very attractive compared with other securities of similar risk, and investors would bid its price up until its expected return fell to the appropriate position on the SML. Conversely, investors would sell off any stock selling at a price high enough to put its expected return below its appropriate position (\emph{overvalued} asset). The resulting reduction in price would continue until the stock’s expected return rose to the level justified by its systematic risk.
        
\begin{figure}[htb]
 	\centering
   	\includegraphics[width=0.7\textwidth]{figures/sml}
   	\caption{Security market line.}
   	\label{fig:sml}
\end{figure}
    
The key point in CAPM is the determination of $\beta$. This can be achieved with the measurement of the \emph{regression line} slope, in the market vs individual stock return plot.

\subsection{Linear Regression}

Given two sets of measurements $X$ and $y$ the linear regression determines the best parameter $\alpha$ and $\beta$ such that

\begin{equation}
y=\beta X + \alpha
\end{equation}
by minimizing the sum of the squared differences between predicted and true $y$ values.
Figure~\ref{fig:linear_regression} shows an example of regression.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/linear_regression}
	\caption{Example of linear regression for the determination of $\beta$.}
	\label{fig:linear_regression}
\end{figure}

Notice that $X$ can be a vector of measurements, in that case we talk about \emph{multiple linear regression}, and $\beta$ is a vector of coefficients each associated with a $X$ component. 
 
\subsubsection{Example}

The Chinese Yuan (CNY) was pegged to the US Dollar (USD) prior to July 2005. Then, China announced that the exchange rate would be set with reference to a basket of other currencies, allowing for a movement of up to 0.3\% within any given day. The actual currencies and their basket weights are unannounced by China. Figure~\ref{fig:yuan_rate} reports the Yuan exchange rate to USD between 1999 and 2013.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/yuan_exchange_rate}
\caption{Yuan exchange rate to USD from January 1999 to December 2013.}
\label{fig:yuan_rate}
\end{figure}

From an empirical point of view, there are several important questions:
\begin{itemize}
\item for any given period, what is the implicit reference basket for the Chinese currency ?
\item has the reference basket changed over time ?
\item has the Chinese currency depreciated with respect to the dollar? If so, how much and when?
\end{itemize}

A possible approach for evaluating the implicit exchange rate regime of the Yuan involves the regression of the changes in the target currency on changes in the values of possible currencies in the reference basket.

Consider the dataset \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/exchange_rates.csv}{exchange\_rates.csv} containing daily currencies exchange rates in the 1999 to 2013 period (data has been fetched from the Federal Reserve Archive~\cite{bib:fred}).

To apply this methodology the original dollar-based exchange rates have been converted using the Swiss Franc. This allows currency moves of the dollar to be be used to explain moves in the Yuan. The choice of Swiss Franc is consistent with evaluations with respect to a stable currency. The dataframe already has columns with daily logarithmic variations.

\begin{ipython}
import pandas as pd

data = pd.read_csv("exchange_rates.csv", index_col="DATE")
print (data.head())
\end{ipython}
\begin{ioutput}
            DEXCHUS  DEXJPUS  DEXKOUS  DEXMAUS  DEXUSEU  DEXUSUK  DEXTHUS  \
DATE                                                                        
1999-01-04   8.2793   112.15   1187.5      3.8   1.1812   1.6581    36.20   
1999-01-05   8.2795   111.15   1166.0      3.8   1.1760   1.6566    36.18   
1999-01-06   8.2795   112.78   1160.0      3.8   1.1636   1.6547    36.50   
1999-01-07   8.2798   111.69   1151.0      3.8   1.1672   1.6495    36.30   
1999-01-08   8.2796   111.52   1174.0      3.8   1.1554   1.6405    36.45   
...
            log_ret_THB_SFR  log_ret_USD_SFR  
DATE                                          
1999-01-04              NaN              NaN  
1999-01-05        -0.002599        -0.002047  
1999-01-06        -0.002666        -0.011472  
1999-01-07        -0.006288        -0.000794  
1999-01-08        -0.003565        -0.007689  

[5 rows x 24 columns]
\end{ioutput}
\noindent
It is necessary to remove possible \texttt{NaN} values in the dateframe.

\begin{ipython}
# replace NaN with previous non-null value in column
df = df.ffill()
df = df.dropna()
\end{ipython}

Figure~\ref{fig:rate_variation} shows CNY and USD logarithmic daily variation with respect to Swiss Franc (SFR).
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/log_variation_exch}
\caption{Logarithmic daily variation of CNY and USD from January 1999 to December 2013 with respect to Swiss Franc.}
\label{fig:rate_variation}
\end{figure}

To implement the linear regression model it can be used the \texttt{statsmodel} package. After the import it is necessary to define the $X$ vector (i.e. the log returns) and $y$ (i.e. the Yuan rate to SFR). Then add a constant to the model which represents the rate variation not correlated to other currency variations.

First, we fit the regression model for the period prior to July 2005 when the Chinese currency was pegged to the US dollar. 

\begin{ipython}
import statsmodels.api as sm

X = df.loc[df.index < '2005-06-30' ,
           ['log_ret_YEN_SFR', 'log_ret_EUR_SFR', 
            'log_ret_GBP_SFR', 'log_ret_USD_SFR']]
y = df.loc[df.index < '2005-06-30' ,'log_ret_CNY_SFR']
X = sm.add_constant(X)

est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython} 
\begin{ioutput}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:        log_ret_CNY_SFR   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
==============================================================================
                      coef    std err          t      P>|t|
-------------------------------------------------------------
const            -1.97e-07   1.74e-06     -0.113      0.910 
log_ret_YEN_SFR    -0.0001      0.000     -0.482      0.630 
log_ret_EUR_SFR    -0.0003      0.001     -0.377      0.706 
log_ret_GBP_SFR    -0.0001      0.000     -0.231      0.817 
log_ret_USD_SFR     1.0002      0.000   2549.895      0.000 
=============================================================
\end{ioutput} 

The most interesting (for us) part of the summary is: 

\begin{itemize}
\item R-squared: the closer to 1 the higher is the linear correlation between $y$ and $X$;
\item coeff column: the $\beta$ resulting from the regression, represent the mean change in the response variable $y$ for one unit of change in the predictor variable $X$ (const is the corresponding $\alpha$);
\item  P>|t| column: the p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.
Conversely, a larger (insignificant) p-value suggests that changes in the predictor are not associated with changes in the response.
\end{itemize}

In our example R-squared is 1, and the only largely significant predictor is USD (p-value 0) confirming that indeed CNY was anchored to US dollar prior July 2005.

Second, we fit the regression model for the first six months following the announcement of the change in currency policy.

\begin{ipython}
X = df.loc[(df.index > '2005-07-01') &(df.index < '2005-12-31'),
           ['log_ret_YEN_SFR', 'log_ret_EUR_SFR', 
            'log_ret_GBP_SFR', 'log_ret_USD_SFR',
            'log_ret_WON_SFR', 'log_ret_MYR_SFR', 
            'log_ret_THB_SFR']]
y = df.loc[(df.index > '2005-07-01') &(df.index < '2005-12-31'),
           'log_ret_CNY_SFR']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython}
\begin{ioutput}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:        log_ret_CNY_SFR   R-squared:                       0.948
Model:                            OLS   Adj. R-squared:                  0.945
==============================================================================
                      coef    std err          t      P>|t|
-------------------------------------------------------------
const              -0.0001      0.000     -0.907      0.366
log_ret_YEN_SFR    -0.0097      0.037     -0.260      0.796
log_ret_EUR_SFR     0.0586      0.093      0.631      0.529
log_ret_GBP_SFR    -0.0313      0.045     -0.694      0.489
log_ret_USD_SFR     0.1994      0.150      1.333      0.185
log_ret_WON_SFR     0.1823      0.036      5.086      0.000
log_ret_MYR_SFR     0.7494      0.144      5.207      0.000
log_ret_THB_SFR    -0.0667      0.060     -1.119      0.265
=============================================================
\end{ioutput}

R-squared is quite close to 1 so there is some correlation between $y$ and $X$.
During this six-month period, there is evidence of the Yuan departing from a US Dollar peg. The exchange rates with the statistically significant regression parameters (lowest p-values) are for the Korean Won (WON) and the Malaysian Ringgit (MYR).

To examine for further changes in the implicit reference basket, it could be possible to fit the same model for other periods from 2006 through 2013.

Finally it is possible to measure the annualized trend in the Yuan exchange rate relative to the other currencies in the studied period. The annualization is performed on the $\alpha$ coefficient which is the only one not related to other currency variations (i.e. the idiosyncratic part of the rate variation).

\begin{ipython}
import numpy as np

print ("{:.4f}".format(np.exp(252*np.log(1+est.params[0]))-1))
\end{ipython}
\begin{ioutput}
-0.0297
\end{ioutput}
\noindent
So the Yuan depreciated in the studied period.

\subsection{Regression in CAPM}

The regressed coefficient estimates can be expressed as 

\begin{equation}
\beta \approx \cfrac{\textrm{cov}(X,y)}{\textrm {var}(X)}
\end{equation}

In the case of CAPM the line estimates the stock returns $y$ given the global market returns $X$ and so provides insights about how \emph{volatile}, or how risky, a stock is relative to the rest of the market.

In CAPM $\beta$ calculation is used to help investors understand whether a stock moves in the same direction as the rest of the market but for it to provide any useful clue, the market proxy should be related to the stock.

If $\beta$ of an individual stock = 1.0, means its price is perfectly correlated with the market, if $\beta < 1.0$, which is referred to as "defensive", indicates the security is theoretically less volatile than the market (provides lower returns, so it is less risky), while if $\beta > 1.0$, or "aggressive", indicates the assets price is more volatile than the market.

Those who use CAPM pick individual stocks or portfolios, and compare them to different indexes. The point is to find stocks that have high $\beta$, and portfolios that have high $\alpha$. High $\beta$ means the stock fares better than index with positive market and performs worse for negative market (contrary low $\beta$ gives lower performance for positive market and "better" returns in negative market), so those stocks have a chance at beating the market. $\alpha$ values above zero mean that a portfolio outperforms the market whatever it does.

\subsection{CAPM Example}

Let's apply CAPM model to a couple of securities, the market proxy is the SP500 index while the risk free rate is approximated by the 3 months Treasury rate.
Input data can be gathered with \texttt{yfinance}

\begin{ipython}
import yfinance as yf

proxy = yf.Tickers(['AAPL', 'AMZN', 'BA', 'GE', 'IBM', 'MGM', 
                    'XOM', '^GSPC', 'CL=F', '^IRX'])
capm = proxy.history(start='2000-01-01', end='2014-01-01')
capm = capm['Close']
\end{ipython}
\noindent
or downloaded with \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/capm.csv}{capm.csv}.

In case data is downloaded from Yahoo Finance it is necessary to create some additional columns, like logarithmic returns and excess of return over the risk free rate.

\begin{ipython}
import numpy as np

# fill NaN with previous non-null value
capm = capm.ffill()
capm['ret_RF'] = np.log1p(0.01*capm['^IRX'].pct_change())
capm['ret_GE'] = np.log1p(capm['GE'].pct_change()) - capm['ret_RF']
capm['ret_XOM'] = np.log1p(capm['XOM'].pct_change()) - capm['ret_RF']
capm['ret_SP500'] = np.log1p(capm['^GSPC'].pct_change()) - capm['ret_RF']
capm['ret_CL=F'] = np.log1p(capm['CL=F'].pct_change())

capm = capm.dropna()
\end{ipython}

Consider the General Electrics (GE) stock and apply CAPM to the series of its returns. Figure~\ref{fig:ge_returns} reports the historical series of closing price for GE and SP500.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_ge}
\caption{Historical series of closing price for General Electrics stock (left) and SP500 (right).}
\label{fig:ge_returns}
\end{figure}
 
Using \texttt{statsmodel} it is possible to determine $\beta$ and $\alpha$ for GE stock. The model is made of GE excess of returns ($y$) and market excess of returns ($X)$ 

\begin{ipython}
X = capm['ret_SP500']
y = capm['ret_GE']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython}
\begin{ioutput}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 ret_GE   R-squared:                       0.576
Model:                            OLS   Adj. R-squared:                  0.576
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0001      0.000     -0.567      0.571      -0.001       0.000
ret_SP500      1.1700      0.017     67.495      0.000       1.136       1.204
==============================================================================
\end{ioutput}

From the summary results that $\beta$ is statistically significant and equal to 1.17 while the intercept value $\alpha$ is not having a large p-value. Fig.~\ref{fig:capm_fit} reports the dataset and the linear regression fit.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/capm_fit}
\caption{Linear regression of market return vs stock return for GE security.}
\label{fig:capm_fit}
\end{figure}

If you imagine to have a portfolio, and that we have the $\beta$s of each individual stock to apply CAPM it is enough to perform a weighted sum of the expected return according to the model of each stock.

\subsection{Criticism to CAPM}
As we have seen the whole model is about plotting a line in a scatter plot, it’s not a very complex model. Assumptions under the model are even more simplistic. For example:
\begin{itemize}
\tightlist
\item expect that all investors are rational and they avoid risk;
\item everyone have full information about the market;
\item everyone have similar investment horizons and expectations about future movements;
\item stocks are all correctly priced.
\end{itemize}

Moreover, this is a model from the 1960s. Market dynamics were different back then. And of course, this is a retrospective model. We cannot know how future stock prices move and how the market behaves.
Interesting extension of CAPM involves \emph{Bayesian regression}\cite{bib:bayesian_regression} but it will not be discussed here.

\section{Multifactor Models}

In its original formulation the Capital Asset Pricing Model (CAPM) treats the market return as the only factor, proxied here by the SP500 Index. Nevertheless a stock’s return can depend also on other macro-economic factors, such commodity prices, interest rates, economic growth (GDP).

The model hence can be generalized by 

\begin{equation}
r=\alpha + \beta_1 f_1 + \beta_2 f_2 +\ldots + \beta_n f_n
\end{equation}
where each $f_i$ is a \emph{factor}.

Consider the example of previous Section and improve the model by adding the crude oil price as a second factor. Perform again the linear regression

\begin{ipython}
X = capm[['ret_SP500', 'ret_CL=F']]
y = capm['ret_GE']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython} 
\begin{ioutput}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 ret_GE   R-squared:                       0.577
Model:                            OLS   Adj. R-squared:                  0.577
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0001      0.000     -0.512      0.609      -0.001       0.000
ret_SP500      1.1817      0.018     66.881      0.000       1.147       1.216
ret_CL=F      -0.0324      0.010     -3.309      0.001      -0.052      -0.013
==============================================================================
\end{ioutput}

The regression coefficient for the oil factor (ret\_CL=F) is statistically significant and negative. Over the analysis period, price changes in GE stock are negatively related to the price changes in oil.
Let's apply the same model now to Exxon (XOM) stock.

\begin{ipython}
X = capm[['ret_SP500', 'ret_CL=F']]
y = capm['ret_XOM']

X = sm.add_constant(X)
est = sm.OLS(y, X).fit()
print(est.summary())
\end{ipython} 
\begin{ioutput}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                ret_XOM   R-squared:                       0.561
Model:                            OLS   Adj. R-squared:                  0.560
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0002      0.000      1.185      0.236      -0.000       0.001
ret_SP500      0.8248      0.014     57.919      0.000       0.797       0.853
ret_CL=F       0.1436      0.008     18.174      0.000       0.128       0.159
==============================================================================
\end{ioutput}

The R-squared for XOM is slightly lower than for GE. Its relationship to the market index is less strong (lower t value).
The regression coefficient for the oil factor (ret\_CL=F) is statistically significant and, unlike GE, positive.

\subsection{Fama-French Three-Factor Model}
﻿
This model was proposed in 1993 by Eugene Fama and Kenneth French to describe stock returns~\cite{bib:fama_french}. The three-factor model is
\begin{equation}
r=\alpha + \beta_m \textrm{MKT} +\beta_s \textrm{SMB}+ \beta_h \textrm{HML}
\end{equation}
where
\begin{itemize}
\tightlist
\item MKT: is the excess return of the market (e.g. the value-weighted return of all firms listed on the NYSE, AMEX, or NASDAQ minus the 1-month Treasury Bill rate);
\item SMB (Small Minus Big): measures the excess return of stocks with small market cap over those with larger market cap;
\item HML (High Minus Low): measures the excess return of value stocks over growth stocks. Value stocks have high book to price ratio (B/P) than growth stocks.
\end{itemize}

This setting represents the original Fama-French model. This very same approach has been also extended with the Fama-French Five-Factor model which comprises two more factors:
\begin{itemize}
	\tightlist
\item RMW (Robust Minus Weak): measures the excess returns of firms with high operating profit margins over those with lower profits;
\item CMA (Conservative Minus Aggressive): measures the excess returns of firms investing less over those investing more.
\end{itemize}
Finally, momentum is another commonly used factor. It captures excess returns of stocks with highest returns over those with lowest returns.

The application of such models goes along the lines of the CAPM with the only difference being the calculation of a multidimensional regression.

\section{PCA1 Portfolio}
\label{portfolio-optimization-and-pca}

In Section~\ref{sec:pca} the Principal Component Analysis (PCA) has been introduced and here we are going to apply PCA on equity return covariance matrix to construct principal component portfolios because they have some interesting characteristics. This means that the portfolio weights will be based on the eigenvectors of the covariance matrix.

From the theory we know that the first principal component (PC1) is a factor that captures the maximal amount of variance (i.e. the linear combination of assets that has highest possible variance). The second principal component factor (PC2) is the second most variable portfolio that is orthogonal to the first and so on.

From the discussion of the CAPM model in Section~\ref{sec:capm} we learnt that the market factor ($\beta$) is the primary driver of the stock market returns, as it tends to explain most of the returns of any given stock in any given day.
So it is expected that applying PCA to daily stock returns, the first principal component approximates the market risk premium from the CAPM model.

In this analysis we are going to use the equities forming the \emph{Dow Jones 30} index, so the first step is to download their returns with \texttt{yfinance} package (the data availability is from March 2019 to today).
	
\begin{ipython}
import yfinance as yf
tickers = ["BA","CAT","CVX","CSCO","KO",
           "DOW","XOM","GS","HD","INTC","IBM",
           "JNJ","JPM","MCD","MRK","MSFT","NKE",
           "PFE","PG","RTX","TRV","UNH","VZ",
           "V","WBA", "WMT", "^DJI"]

proxy = yf.Tickers(tickers)           
df = proxy.history(start='2014-03-27',
                   end='2021-03-31')['Close'].pct_change().dropna()
\end{ipython}
	
Since the package returns already daily returns we can directly compute the covariance matrix from them. Also we determine eigenvalues and eigenvectors of this matrix.

\begin{ipython}
import numpy as np

equities = df.iloc[:, :-1]
cov = equities.cov()
eigVals, eigVecs = np.linalg.eig(cov)
indices = [i for i in reversed(np.argsort(eigVals))]
l = [eigVals[i]/np.*@sum@*(eigVals)*100 for i in reversed(np.argsort(eigVals))]
\end{ipython}
	
Looking at the explained variance plot (see Fig.~\ref{fig:explained_variance}) we can notice how the first principal component explains almost 60\% then the value falls quickly down.

\begin{figure}[htb]
	\centering
	\includegraphics[width=.7\textwidth]{figures/portfolio_pca_expl_var}
	\caption{Explained variance plot. Notice how the explained variance decreases steeply after the first component, and also how the returned components are not ordered by eigenvalue.}
	\label{fig:explained_variance}
\end{figure}
		
Next we can re-scale the PC's eigenvectors to sum up to 1 so they can be used as portfolio weights.
	
\begin{ipython}
norm_eigVecs = [v/np.linalg.norm(v) for v in eigVecs.T]
\end{ipython}
	
Finally we can construct a portfolio using the weights derived from PCs. In other words each asset takes a weight equal to its corresponding component in the PC eigenvector. Figure~\ref{fig:pca_weights} reports the weights of each asset in PC1 (the first principal component).
	
\begin{figure}[htb]
	\centering
	\includegraphics[width=.7\textwidth]{figures/portfolio_pca_pc1_weights}
	\caption{Weights of the PC1 portfolio. Each asset is weighed according to the corresponding component in the first principal component eigenvector.}
	\label{fig:pca_weights}
\end{figure}
	
Then we calculate the cumulative return of the portfolio and compare it to the market return. This process is repeated for the first three components.

\begin{ipython}
for i, index in enumerate(indices[0:3]):
    pc_daily_ret = equities.dot(norm_eigVecs[index])
    pc_cum_ret = pc_daily_ret.cumsum()
    market_cum_ret = df['^DJI'].cumsum()
\end{ipython}
	
From Fig.~\ref{fig:ret_pc1} to~\ref{fig:ret_pc3} it is clear how the PC1 portfolio return looks identical to the market. The second and third PC portfolio returns look quite different instead. It is not easy to interpret them though. 

\begin{figure}[htbp]
	\centering
		\subfloat[Cumulative return of PC1 (left) compared to the market return (right).\label{fig:ret_pc1}]{%
		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc1_vs_market}
	}\\
	\subfloat[Cumulative return of PC2 (left) compared to the market return (right).
	\label{fig:ret_pc2}]{%
		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc2_vs_market}
	}\\
	\subfloat[Cumulative return of PC3 (left) compared to the market return (right). 
	\label{fig:ret_pc3}]{%
		\includegraphics[width=0.7\textwidth]{figures/cum_ret_pc3_vs_market}
	}
	\caption{Comparison of cumulative return of various principle components portfolios to the market return.}
    \label{fig:dummy2}
\end{figure}

It can be verified that PC1 portfolio still looks like the market also for other time horizon, repeating the same steps using weekly or monthly data.

\section*{Exercises}
\input{portfolio_ex_text}

\begin{thebibliography}{9}
\bibitem{bib:post_modern_theory}\href{https://en.wikipedia.org/wiki/Post-modern_portfolio_theory}{\emph{Post Modern Theory}}, Wikipedia [Online]
\bibitem{bib:black_litterman}\href{https://en.wikipedia.org/wiki/Black\%E2\%80\%93Litterman_model}{\emph{Black-Litterman Model}}, Wikipedia [Online]
\bibitem{bib:bayesian_regression}W. Kohersen, \href{https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7}{\emph{Introduction to Bayesian Linear Regression}}, Toward Data Science [Online]
\bibitem{bib:fred}\href{https://fred.stlouisfed.org/}{\emph{FRED Economic data}}, 1991 [Online]
\bibitem{bib:capm_lintner} Lintner, J. \emph{“The Valuation of Risky Assets and the Selection of Risky Investments in Stock Portfolio and Capital Budgets}, 1965, Review of Economics and
Statistics, 47: 13-37.
\bibitem{bib:capm_sharpe} Sharpe, W. \emph{Capital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk}, 1964, Journal of Finance, 19: 425-442.
\bibitem{bib:fama_french} E. F. Fama, K. R. French \emph{Common risk factors in the returns on stocks and bonds}, Journal of Financial Economics, 1993, doi:10.1016/0304-405X(93)
\bibitem{bib:diversification} Y. Choueifaty, Y.Coignard, \href{ https://www.tobam.fr/wp-content/uploads/2014/12/TOBAM-JoPM-Maximum-Div-2008.pdf}{\emph{Toward Maximum Diversification}}, 2008
\end{thebibliography}
