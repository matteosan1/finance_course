\chapter{Modeling Risk Correlation}

In credit derivative valuation and credit risk management, one of the fundamentally important issues is the estimate of default probabilities and their correlations. For this, generally speaking, there are two ways: using historical default data or using mathematical models.

Historical default data has played an important role in the estimate of default probabilities. However, because credit events are rare, there is very limited default data available. Moreover, historical data only reflects the past default pattern and it may not be a proper indicator of the future. This makes the estimate of default probabilities from historical data difficult and inexact. To use this same data to estimate default correlations is even more difficult and inexact.

The market trend now is towards the use of mathematical models that don't rely on historical default data. In Chapter~\ref{credit_default_swaps} we have seen how it is possible to derive default probabilities from market data, here we will see how copula can be used to model their correlations to valuate credit derivatives. 

\section{Default Correlation}\label{sec:default_correlation}
Correlation is a precise mathematical concept that has only meaning in reference to random variables. If $X$ and $Y$ are random variables, the \emph{correlation coefficient}, $\rho$, between $X$ and $Y$ is defined as
\begin{equation}
\rho = \cfrac{\mathbb{E}[(X-\mu_Y)(Y-\mu_Y)]}{\sigma_X \sigma_Y}
\label{eq:correlation_coefficient}
\end{equation}
where $\mu$ and $\sigma$ denote the corresponding mean and standard deviation, and $\mathbb{E}$ is the expected value operator.

Statements such as company $A$ and company $B$ are correlated are meaningless. One needs to refer to a quantifiable variable associated with the two companies (e.g. stock price, revenue growth or credit default spread) for that statement to make sense.
In the same spirit, stating that the default behavior of companies $A$ and $B$ is correlated does not carry a lot of meaning unless one specifies a random variable that captures what \emph{default behavior} means. %In summary, before
%we can talk about default correlation, we need to define a random variable
%that somehow captures \emph{default behavior}.

Consider two assets. Two random variables $P_1$ and $P_2$ (e.g. the default probabilities) can be used to generate default scenarios for each asset. If $n$ of such possible default scenario are generated two index variables $v_1$ and $v_2$ capture the default pattern of each asset: each variable, actually a vector, will be a sequence of $n$ 1’s and 0’s according to the default state of the corresponding asset.

Using an appropriate random sample it is possible to determine the correlation coefficient between $v_1$ and $v_2$ to then estimate the default correlation ($\rho_D$) between asset 1 and 2.

\begin{equation}
\rho_D = \cfrac{n\sum (v_{1i} v_{2i}) - \sum v_{1i} \sum v_{2i}}{\sqrt{(n\sum v_{1i}^2 - (\sum v_{1i})^2)(n\sum v_{2i}^2 - (\sum v_{2i})^2)}}
\label{eq:discrete_correlation_coefficient}
\end{equation}
Equation~\ref{eq:discrete_correlation_coefficient} is just a discrete version of Eq.~\ref{eq:correlation_coefficient}.

The challenge to do a Monte Carlo relies on the ability to generate realistic default scenarios. That is, being able to be loyal to $p$ (the average default probability of the pool) and $\rho_D$ (the assets’ default correlation.) Note that if we are dealing with two assets, the default correlation is captured by one number. In the case of $N$ assets, $\rho_D$ is a symmetric matrix. 

\section{Copula}

Copulas (or copul\ae~in Latin) are an interesting mathematical tool to represent correlations between probability distributions. They can be used to describe complex dependencies in multivariate risk models, when more basic tools are inappropriate. Another commonly used application of copulas is sampling from correlated random variables.

\subsection{Distribution Transformation}
\label{distribution-transformation}

Distribution transformation is a very useful tool which will be extensively used with the copula. The technique we are going to outline transforms the distribution of every random variable into uniform and vice versa and is called \emph{inverse transform} or (probability integral transform).

This method involves computing the quantile function (see Section~\ref{sec:quantile-function}) of the original distribution; in other words, computing the cumulative distribution function ($F_X(x)$) (which maps a number to a probability between 0 and 1) and then inverting that function. 

\begin{attention}
\subsubsection{Demonstration}
Let's start with a random variable $X$ with an arbitrary distribution and let $F_X$ be the cumulative distribution function of $X$ (\(F(x) = P(X \leq x)\)). 
We would like to find a transformation $T:[0,1]\rightarrow\mathbb{R}$ such that $T(U)=X$, where $U$ is the uniform distribution in $[0,1]$. 
\begin{equation*}
F_{X}(x)= P(X\leq x)=P(T(U)\leq x)= P(U\leq T^{-1}(x))=T^{-1}(x),{\text{ for }}x\in \mathbb {R}
\end{equation*}
where the last step used that $P(U\leq y)=y$ when $U$ is uniform on $(0,1)$.

So we got $F_{X}$ to be the inverse function of $T$, or, equivalently $T(u)=F_{X}^{-1}(u)$, $u\in [0,1]$.
\end{attention}

Therefore to transform a generic random variable to uniform (and vice versa) we can use
\begin{equation}
x = F_{X}^{-1}(u)\quad\mathrm{and}\quad u = F_X(x)	
\end{equation} 
Let's see few examples of how this can be done in \(\tt{python}\). Imagine we want to transform an uniform distribution into a Gaussian. The transformation takes uniform samples of a variable \(u\) between 0 and 1, interpreted as a probability, and then returns $F^{-1}(u)$. 

Table~\ref{tab:transformation} shows the samples of $U$ and their representation on the standard normal distribution determined using the algorithm explained above.

\begin{ipython}
from scipy.stats import uniform, norm

uniform_samples = [0.5, .975, 0.995, 0.999999]
for u in uniform_samples:
    print (norm.ppf(u))
\end{ipython}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \(\mathbf{u}\) & \(\mathbf{F^{-1}(u)}\) \\
    \hline
    0.5 & 0 \\
    \hline
    .975 & 1.95996 \\
    \hline
    .995 & 2.5758 \\
    \hline
    .999999 & 4.75342 \\
    \hline
    \(1-2^{-52}\) & 8.12589 \\
    \hline
  \end{tabular}
  \caption{Table of samples from the uniform distribution and their corresponding Gaussian quantiles.}
\label{tab:transformation}
\end{table}

Now let's see how this can be done directly with an entire distribution. Let's start again from the uniform distribution. This can be sampled either using \texttt{random.random} like in Section~\ref{pseudo-random-numbers} or with \texttt{scipy.stats.uniform}. 
Each distribution defined in \texttt{scipy.stats} has a convenient method \texttt{rvs(size=aSize)} (random variable sample) to sample from it as many time as specified by the arguments \texttt{size}. Usually the second method is preferred since it exploits \texttt{numpy.array} and let us avoid a lot of for-loop cycles.

In any case in the left plot of Fig.~\ref{fig:uniform_and_gauss} the resulting distribution is shown.

\begin{ipython}
x = uniform(0, 1).rvs(10000)
\end{ipython}

Next we want to transform these samples so that instead of uniform they are normally distributed. As we have just seen the transform that does this is the inverse of the cumulative density function of the normal distribution, or \((\tt{ppf(x))}\) in \texttt{python}. 

In the right plot of Fig.~\ref{fig:uniform_and_gauss} the Gaussian obtained with the code below is shown

\begin{ipython}
x_trans = norm.ppf(x)
\end{ipython}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1.\textwidth]{figures/uniform_gauss}
	\caption{On the left the generated uniform distribution, on the right its Gaussian transform.}
	\label{fig:uniform_and_gauss}
\end{figure}

If we plot them together in a 2D plot we can get a sense of what is going on using the inverse CDF transformation. Indeed it stretches the outer regions of the uniform to yield a normal distribution. This is shown in Fig.~\ref{fig:uniform_to_gauss}. 
    
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/uniform_to_gauss}
  \caption{2D plot showing the transformation that maps our initial uniform distribution to the resulting Gaussian.}
  \label{fig:uniform_to_gauss}
\end{figure}
    
The nice thing of this technique is that it can be used for any arbitrary (univariate) probability distribution, like for example t-Student~\cite{bib:t_student} or Gumbel~\cite{bib:gumbel}. A similar transformation from uniform to t-Student is shown in Fig.~\ref{fig:uniform_to_tstudent}.

\begin{ipython}
from scipy.stats import t

x_trans = t(4).ppf(x)
\end{ipython}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/tstudent_to_uniform}
  \caption{2D plot showing the transformation that maps a uniform distribution to a t-Student.}
  \label{fig:uniform_to_tstudent}
\end{figure}

Clearly to do the opposite transformation, that is from an arbitrary distribution to the uniform, we can just apply the inverse of the inverse CDF, that is the CDF itself\ldots

In such a way we can go from distribution A to distribution B passing through a uniform distribution rather quickly. An example is shown in Fig.~\ref{fig:a_to_b_to_a}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/a_to_b_to_a}
	\caption{Example of transformation, it starts with a uniform distribution, goes into a Gaussian and back to the initial uniform.}
	\label{fig:a_to_b_to_a}
\end{figure}
\newpage

\section{Copula Definition}
\label{copula}

A \emph{copula} is a multivariate distribution which examines the association or the dependence between many variables. It allows the joining of multiple univariate distributions to a single multivariate distribution.

%Copula provide risk managers and market analysts with a measure of the dependence between various financial variables that is not subject to the same limitations as correlation. Correlation only works well with the normal distribution, but financial markets don't always exhibit properties of the normal distribution.

To create a copula, two or more unknown distributions are mapped to a well-known distribution, such as the normal distribution, whose properties are well established. As a result, a joint probability distribution is created while still maintaining the individual (constituent) marginal distributions

In probability theory a copula $\mathcal{C}(F_1, F_2, \ldots, F_n, \rho)$ is a multivariate (multidimensional) cumulative distribution function whose marginal probability distributions (i.e. the probability distribution of each dimension) are uniform on the interval $[0, 1]$ ($F_i = U_i =\mathrm{uniform}(0,1)$), \(\rho\) represents the correlation between each variable.

%Copulas are used to describe the dependencies between random variables and have been widely used in quantitative finance to model risk. 
Copulas are popular since allow to easily model and estimate the distribution of random vectors by representing marginals and their correlation separately.
Essentially we can split a complicated problem into simpler components.

Despite the probably obscure and daunting definition given above the concept of copula is quite simple so it will be clarified later on with a couple of practical examples.

\subsection{Sklar's Theorem}

A key part of copulas is \emph{Sklar’s theorem}. The theorem states that any multivariate joint distribution can be written in terms of univariate marginal distribution functions and a copula which describes the dependence structure between the variables. Mathematically, the theorem can be stated as follows: suppose we have only two random variables, $X$ and $Y$. If $F(x,y)$ is a joint distribution function with continuous marginals 
$F_x(x)=u$ and $F_y(y)=v$, then the joint distribution $F(x,y)$ can be written in terms of a unique function $\mathcal{C}(u,v)$:
\begin{equation}
F(x,y)=\mathcal{C}(u,v)
\end{equation}
\noindent
where $\mathcal{C}(u,v)$ is the copula of $F(x,y)$.

The copula function describes how the multivariate function $F(x,y)$ is derived from or coupled with the marginal distribution functions $F_x(x)$ and $F_y(y)$.

%As a final note, let's state \emph{Sklar's theorem} which assesses that any multivariate joint distribution can be written in terms of univariate marginal functions and a copula which describes the dependence structure between the variables.

\subsection{Example Case}
\label{example-problem-case}

Imagine to have to measure two correlated variables. For example, we look at various rivers and for every river at the maximum water level of that river over a certain time-period. In addition, we also count how many months each river caused floods.

For the probability distribution of water level of the river we know that maximums are Gumbel~\cite{bib:gumbel} distributed, while the number of floods can be modeled according to a Beta~\cite{bib:beta}.

Clearly it is pretty reasonable to assume that maximum water level and number of floods are going to be correlated, however we don't know how we could model the correlated probability distribution.
Indeed above we only specified the distributions for individual variables, irrespective of the other one (i.e. the marginals), but in reality we are dealing with a \emph{joint distribution} of both of these together. And here is where copulas come to our rescue.

Copulas essentially bridges the marginals distributions (which by definition have no correlation) into a joint probability distribution specifying the correlation separately. 

\section{Adding Correlation with Gaussian Copulas}
\label{adding-correlation-with-gaussian-copulas}

At the beginning of this Chapter we have seen how to convert anything uniformly distributed to an arbitrary probability distribution, so we could
\begin{itemize}
\tightlist
\item
  simulate a sample from a multivariate distribution with the specific correlation structure;
\item
  transform the sample so that the marginals are uniform;
\item
  finally transform each uniform marginal to whatever we need to.
\end{itemize}

In this example we sample from a multivariate (two-dimensional) normal with a 0.5 correlation, see Fig.~ \ref{fig:multivariate_with_correlation}. This is not mandatory though, since we could have modeled correlations with different distribution either.

\begin{ipython}
from scipy.stats import multivariate_normal

mvnorm = multivariate_normal(mean=[0, 0] , cov=[[1, 0.5],
                                                [0.5, 1]])
x = mvnorm.rvs(100000)
\end{ipython}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/2d_gaussian}
  \caption{2D normal distribution with a correlation factor of 0.5.}
  \label{fig:multivariate_with_correlation}
\end{figure}
    
Now apply what we have just learned to make the marginals uniform using the \(\tt{cdf}\) function of the normal distribution:

\begin{ipython}
x_unif = norm.cdf(x)
\end{ipython}

Plots like the one shown in Fig.~\ref{fig:copula} are how copulas are usually visualized. Since we have used a multivariate standard normal to model the correlation this is an example of \textbf{Gaussian Copula}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/copula_2d}
\quad
\includegraphics[width=0.5\textwidth]{figures/copula_3d}
\caption{Graphical representations of the copula, 2D on the left, 3D on the right.}
\label{fig:copula}
\end{figure}

Finally we can just transform the obtained marginals, again through uniform, to what we want (i.e. Gumbel and Beta in our river example):

\begin{ipython}
from scipy.stats import gumbel_l, beta

m1 = gumbel_l()
x1 = m1.ppf(x_unif[:][0])
m2 = beta(a=10, b=3)
x2 = m2.ppf(x_unif[:][1])
\end{ipython}

Left plot of Figure~\ref{fig:gumbel_beta_with_corr} shows the two marginals and the joint distribution.
    
To see that it is actually working as expected we should now compare the joint distributions with and without correlation. Looking at Figure~\ref{fig:gumbel_beta_with_corr} the difference is quite clear.

\begin{ipython}
# sample from Gumbel
x1 = m1.rvs(10000)
# sample from Beta
x2 = m2.rvs(10000)
\end{ipython}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/gumbel_vs_beta}
  \quad
  \includegraphics[width=0.45\textwidth]{figures/gumbel_vs_beta_corr}
  \caption{Marginalized Gumbel and Beta distributions with their joint when they is are (left) and are not (right) correlated.}
  \label{fig:gumbel_beta_with_corr}
\end{figure}
    
Using the uniform distribution as a common base for our transformations we can easily introduce correlations and flexibly construct complex probability distributions. 

Everything discussed in this Section can be easily generalized to higher dimensions.

\section{Generate Correlated Distributions}
\label{generate-correlated-distributions}

In order to generate numbers from correlated distributions it is enough to follow similar steps to those described in the previous Chapter:

\begin{itemize}
\tightlist
\item
  generate a random vector \(\mathbf{x}=(x_1, x_2,\ldots)\) from the  original multivariate distribution with the desired correlation;
\item
  determine the single \(U_i(x_i)\) by applying \texttt{cdf} to each \(x_i\);
\item
  transform again each \(U_i(x_i)\) to the desired marginal distributions using \texttt{ppf};
\item
  each component of the vector \(\mathbf{x}\) is now converted to a set of random numbers drawn from the desired distributions, with the appropriate correlation.
\end{itemize}

Imagine three companies ($A$, $B$ and $C$) which have a cumulative probability of defaulting within the next two years of 10\%.
Let's compare the probability to have the three of them all defaulting within the next two years in two cases: independent default probabilities and perfectly correlated probabilities.

In the first case (independent probabilities), the odds to get three defaults within two years will be the product of the single probabilities, hence:

\[P_{\mathrm{uncorr}} = 10\% \cdot 10\% \cdot 10\% = 0.1 \%\]

We can verify this in \texttt{python} by applying the method outlined above: first generate a random sample from an uncorrelated multivariate normal distribution, then transform each sampled vector into uniform with the \texttt{norm.cdf} function (i.e. we convert the sample into cumulative probabilities) and then count how many times the three of them are lower than 10\% to check whether a default occurred in next two years (in this case, since we are interested in cumulative probabilities we can neglect the last transformation and work directly with the uniform marginals).

\begin{ipython}
from scipy.stats import multivariate_normal, uniform, norm
import numpy

numpy.random.seed(10)
trials = 10000
mvnorm_no_corr = multivariate_normal(mean=[0, 0, 0], cov=[[1, 0, 0],
                                                          [0, 1, 0],
                                                          [0, 0, 1]])
defaults = 0
x = mvnorm_no_corr.rvs(trials)
x_trans = norm.cdf(x)
for i in range(len(x_trans)):
    if x_trans[i][0] < 0.1 and x_trans[i][1] < 0.1 and x_trans[i][2] < 0.1:
        defaults += 1
print ("Defaults w/o correlation: {:.2f}%".format(defaults/trials*100))
\end{ipython}
\begin{ioutput}
Defaults w/o correlation: 0.10%
\end{ioutput}
\noindent
The result is 0.1\% as expected.

If we repeat the same Monte Carlo experiment with perfectly correlated default probabilities we have

\begin{ipython}
mvnorm_corr = multivariate_normal(mean=[0,0,0], 
                                  cov=[[1, 0.999999, 0.999999],
                                       [0.999999, 1, 0.999999],
                                       [0.999999, 0.999999, 1]])

defaults = 0
x = mvnorm_corr.rvs(trials)
x_trans = norm.cdf(x)
for v in x_trans:
    if max(v) < 0.1:
        defaults += 1
print ("Defaults w/ correlation: {:.2f}%".format(defaults/trials*100))
\end{ipython}
\begin{ioutput}
Defaults w/ correlation: 9.89%
\end{ioutput}
In this case the result is close to 10\%, like we had only one single company defaulting. 
Indeed given the perfect correlation between the probabilities either there is no default or three "simultaneous" defaults with 10\% probability.

%\subsubsection{Independent Defaults}\label{independent-defaults}
%
%If the default times of the number of companies are independent,
%default probabilities can be
%calculated through multiplication and integration of the single default
%probability curves without Monte Carlo simulation.
%
%As an example, consider the probability to have two defaults among four
%companies. Let \(\tau_i\) be the default time of name \(i\) and \(F_i(t)\)
%its distribution. Then the probability that name 1 defaults second in
%the basket before time \(t\) is:
%
%\begin{equation}
%\begin{split}
%&\mathbb{P}((\tau_2\lt\tau_1)\cap (\tau_1\lt t)\cap (\tau_1\lt\tau_3)\cap (\tau_1\lt\tau_4)) +\\
%&\mathbb{P}((\tau_3\lt\tau_1)\cap (\tau_1\lt t)\cap (\tau_1\lt\tau_2)\cap (\tau_1\lt\tau_4)) =\\
%&\int_0^t{F_2 (s)\cdot (1-F_3 (s)) \cdot (1-F_4 (s))~dF_1(s)} +  \int_0^t{F_3 (s)\cdot (1-F_2 (s)) \cdot (1-F_4 (s))~dF_1(s)}
%\end{split}
%\label{eq:indep_default}
%\end{equation}
%
%Suppose the default probabilities of three companies, $A$, $B$ and $C$ are
%given as in the following table (in each interval are linear):
%
%\begin{center}
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		time in years & A & B & C \\
%		\hline
%		\hline
%		0 & 0 & 0 & 0 \\
%		1 & 0.022032 & 0.0317 & 0.035 \\
%		2 & 0.046242 & 0.0655 & 0.075 \\
%		3 & 0.07266 & 0.1022 & 0.121 \\
%		4 & 0.101233 & 0.142 & 0.153 \\
%		5 & 0.131885 & 0.1752 & 0.205 \\
%		\hline
%	\end{tabular}
%\end{center}
%and suppose that the default events of the three companies are
%independent. The integral in Eq.~\ref{eq:indep_default} can be solved by substitution:
%
%\[ \int_{x_0}^{x_1}{(1-F_B(x))(1-F_C(x))dF_A(x)}\]
%
%Setting \(t=m_A x + q_A\) it becomes:
%
%\[ \int_{m_A x_0 + q_A}^{m_A x_1 + q_A}{(1-F_B(x(t)))(1-F_C(x(t)))dt}\qquad\Big(\textrm{with}~x(t) = \cfrac{t -q_A}{m_A}\Big) \]
%and similarly for company $B$ and $C$.
%
%To convert it into \texttt{python} we can use \texttt{scipy.integrate.quad} to
%perform the integral and \texttt{numpy.interp} to determine the
%intermediate default probabilities.
%
%\begin{codebox}
%	\begin{Verbatim}[commandchars=\\\{\}]
%	\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{integrate} \PY{k}{import} \PY{n}{quad}
%	\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{interp}
%	
%	\PY{n}{default\PYZus{}rates} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.022032}\PY{p}{,} \PY{l+m+mf}{0.046242}\PY{p}{,} \PY{l+m+mf}{0.07266}\PY{p}{,} \PY{l+m+mf}{0.101233}\PY{p}{,} \PY{l+m+mf}{0.131885}\PY{p}{)}\PY{p}{,}
%	\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{B}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.0317}\PY{p}{,} \PY{l+m+mf}{0.0655}\PY{p}{,} \PY{l+m+mf}{0.1022}\PY{p}{,} \PY{l+m+mf}{0.142}\PY{p}{,} \PY{l+m+mf}{0.1752}\PY{p}{)}\PY{p}{,}
%	\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.035}\PY{p}{,} \PY{l+m+mf}{0.075}\PY{p}{,} \PY{l+m+mf}{0.121}\PY{p}{,} \PY{l+m+mf}{0.153}\PY{p}{,} \PY{l+m+mf}{0.205}\PY{p}{)}\PY{p}{\PYZcb{}}
%	
%	\PY{k}{def} \PY{n+nf}{func}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{default}\PY{p}{,} \PY{n}{companies}\PY{p}{,} \PY{n}{t}\PY{p}{)}\PY{p}{:}
%	\PY{n}{m} \PY{o}{=} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
%	\PY{n}{q} \PY{o}{=} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{m} \PY{o}{*} \PY{p}{(}\PY{n}{t}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%	\PY{n}{t} \PY{o}{=} \PY{p}{(}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{q}\PY{p}{)}\PY{o}{/}\PY{n}{m}
%	\PY{n}{F2} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{interp}\PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
%	\PY{n}{F3} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{interp}\PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
%	\PY{k}{return} \PY{n}{F2}\PY{o}{*}\PY{n}{F3}
%	
%	\PY{k}{def} \PY{n+nf}{integral}\PY{p}{(}\PY{n}{default}\PY{p}{,} \PY{n}{companies}\PY{p}{,} \PY{n}{t}\PY{p}{)}\PY{p}{:}
%	\PY{k}{return} \PY{n}{quad}\PY{p}{(}\PY{n}{func}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{default}\PY{p}{[}\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{n}{t}\PY{p}{]}\PY{p}{,} 
%	
%	\PY{n}{args}\PY{o}{=}\PY{p}{(}\PY{n}{default}\PY{p}{,} \PY{n}{companies}\PY{p}{,} \PY{n}{t}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
%	\PY{k}{for} \PY{n}{companies} \PY{o+ow}{in} \PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{B}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{B}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{B}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{]}\PY{p}{:}
%	\PY{n}{prob} \PY{o}{=} \PY{l+m+mi}{0}
%	\PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}
%	\PY{n}{prob} \PY{o}{=} \PY{n}{integral}\PY{p}{(}\PY{n}{default\PYZus{}rates}\PY{p}{,} \PY{n}{companies}\PY{p}{,} \PY{n}{t}\PY{p}{)}
%	\PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P(1st def) at time (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{) for company }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+si}{\PYZob{}:.5f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{t}\PY{p}{,} 
%	\PY{n}{companies}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{prob}\PY{p}{)}\PY{p}{)}
%	
%	First to default prob at time (1) for company A: 0.02131
%	First to default prob at time (2) for company A: 0.04301
%	First to default prob at time (3) for company A: 0.06460
%	First to default prob at time (4) for company A: 0.08573
%	First to default prob at time (5) for company A: 0.10606
%	First to default prob at time (1) for company B: 0.03080
%	First to default prob at time (2) for company B: 0.06160
%	...
%	\end{Verbatim}
%\end{tcolorbox}

\section{Cholesky Transformation to Correlate Variables}
In Section~\ref{sec:cholesky} it has been shown that a given matrix, $[\Sigma]$ can be factored uniquely into a product $[\Sigma]=[L]^T [L]$, where $[L]$ is a lower triangular matrix with positive diagonal entries. The matrix $[L]$ is the Cholesky (or "square root") matrix.

The Cholesky matrix can be used to create correlations among random variables. Indeed if the original $[\Sigma]$ is a covariance matrix, it transforms uncorrelated variables into variables whose variances and covariances are given by $[\Sigma]$. 

For example, suppose that $X$ is a vector of independent standard normal variables. The matrix $[L]$ can be used to create a new vector $Z$ such that the covariance of $Z$ equal $[\Sigma]$.
Figure~\ref{fig:cholesky_norm} shows the original uncorrelated variable $X$ (left), and the transformed variable $Z$ with correlation $[\Sigma]$ (right).

\begin{ipython}
from scipy.stats import multivariate_normal
import numpy as np

np.random.seed(10)

trials = 1000
m_norm = multivariate_normal(mean=[0, 0], cov=[[1,0],
                                               [0,1]])

X = m_norm.rvs(size=trials)

sigma = np.array([[3, 1], [1, 1]])
L = np.linalg.cholesky(sigma)
print (L)
Z = x.dot(L)
\end{ipython}
\begin{ioutput}
[[1.73205081 0.        ]
 [0.57735027 0.81649658]]
\end{ioutput}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/cholesky_norm}
  \caption{The original uncorrelated variable $X$ (left), the transformed variable $Z$ (right).}
  \label{fig:cholesky_norm}
\end{figure}

Clearly, if you start with correlated variables, you can apply the inverse of the Cholesky transformation to get uncorrelated variables.
If you take the $Z$ points generated above and apply $[L]^{-1}$ transform the original data is obtained again. Figure~\ref{fig:cholesky_inv} shows the resulting distribution.

\begin{ipython}
orig_X = np.dot(Z, np.linalg.inv(L))
\end{ipython}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/cholesky_inv}
  \caption{The transformed variable $Z$ (left), and the original data obtained by the application of the inverse Cholesky transformation (right).}
  \label{fig:cholesky_inv}
\end{figure}


\section{Copula with Unknown Marginals}
In the previous example the marginal distributions of floods and water height were known. Nevertheless the percentile-to-percentile transformation allows to determine the copula function also in more complicated situations an in particular when these are unknown.

Imagine for example a risk manager which owns two non-investment graded bonds (e.g. rated BB+ and BB respectively). The bonds have been issued by two companies and their default probabilities (DP) are as listed below:

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|}
\hline
Time & DP Asset 1 & DP Asset 2 \\
\hline
\hline
1 & 0.065 & 0.238 \\
2 & 0.081 & 0.152 \\
3 & 0.072 & 0.113 \\
4 & 0.064 & 0.092 \\
5 & 0.059 & 0.072 \\
\hline
\end{tabular}
\end{table}

How can a Gaussian copula be constructed to estimate the joint default probability of these two companies in the next year, assuming a Gaussian default correlation of $\rho_M = 0.4$ ?

Applying a procedure similar to that used to derive Table~\ref{tab:transformation} it is possible to map the cumulative default probabilities $Q(t)$ to the standard normal distribution via $N^{-1}[Q(t)]$.
In the code below \texttt{cumsum} is used to determine the cumulative probability from the inputs.

\begin{ipython}
from scipy.stats import norm
import numpy as np

PDF1 = np.array([0.065, 0.081, 0.072, 0.064, 0.059])
PDF2 = np.array([0.238, 0.152, 0.113, 0.092, 0.072])

C1 = PDF1.cumsum()
C2 = PDF2.cumsum()

for c in C1:
    print ("{:.3f} -> {:.4f}".format(c, norm.ppf(c)))
print ("")
for c in C2:
    print ("{:.3f} -> {:.4f}".format(c, norm.ppf(c)))
\end{ipython}
\begin{ioutput}
0.065 -> -1.5141
0.146 -> -1.0537
0.218 -> -0.7790
0.282 -> -0.5769
0.341 -> -0.4097

0.238 -> -0.7128
0.39 -> -0.2793
0.503 -> 0.0075
0.595 -> 0.2404
0.667 -> 0.4316
\end{ioutput}

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Time & DP Asset 1 & $Q_{\textrm{BB+}}(t)$ & $N^{-1}[Q_{\textrm{BB+}}(t)]$ & DP Asset 2 & $Q_{\textrm{BB}}(t)$ & $N^{-1}[Q_{\textrm{BB}}(t)]$\\
\hline
\hline
1 & 0.065 & 0.065 & -1.5141 & 0.238 & 0.238 & -0.7128\\
2 & 0.081 & 0.146 & -1.0537 & 0.152 & 0.39 & -0.2793\\
3 & 0.072 & 0.218 & -0.7790 & 0.113 & 0.503 & 0.0075\\
4 & 0.064 & 0.282 & -0.5769 & 0.092 & 0.595 & 0.2404\\
5 & 0.059 & 0.341 & -0.4097 & 0.072 & 0.667 & 0.4316\\
\hline
\end{tabular}
\end{table}

At this point we can apply the correlation structure $\rho_M$ of the Gaussian multivariate distribution ($M$) to the transformed marginals $N^{-1}[Q_{\textrm{BB+}}(t)]$ and $N^{-1}[Q_{\textrm{BB}}(t)]$.

The joint probability of both companies defaulting within one year can be calculated as

\begin{equation*}
Q(t_{\textrm{BB+}}\leq 1 \cap t_{\textrm{BB+}}\leq 1) = M(X_{\textrm{BB+}}\leq -1.5141 \cap X_{\textrm{BB+}}\leq -0.7128, \rho=0.4)
\end{equation*}

\begin{ipython}
from scipy.stats import multivariate_normal

g = multivariate_normal(mean=[0,0],
                        cov=[[1, 0.4],
                             [0.4, 1]])

print (g.cdf([-1.5141, -0.7128]))
\end{ipython}
\begin{ioutput}
0.03435364791840133
\end{ioutput}
\noindent
which results to be around 3.4\%. Figure~\ref{fig:copula_no_marginals} shows the joint probability distribution, the number we were looking for is given by the fraction of events delimited by the red lines with respect to the total.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\linewidth]{figures/copula_no_marginals}
\caption{Multivariate Gaussian probability distribution with correlation $\rho=0.4$. The red lines delimited the area representing the default of the two companies within year one.}
\label{fig:copula_no_marginals}
\end{figure}

\subsection{Type of Copula}
There are numerous types of copula but there are two main classes: one-factor copula and two-factor copula.

The most popular one-factor copula are the Gaussian copula and the Archimedean copula family. The latter can be split further into Gumbel, Clayton, and Frank copulas.

Two-factor copulas include the student’s t copula, Frechet copula, and Marshal-Olkin copula. We are going to concentrate on the Gaussian copula only though.

\section{One Factor Gaussian Copula Model}
While there are several types of copula function models, the first introduced was the \emph{one-factor Gaussian copula model}. This model is quite simple and has, above all, the advantage that can be solved analytically. In essence the one-factor Gaussian copula, is nothing but a numerical algorithm to generate samples of normally distributed random variables that have a given (Gaussian) pair-wise correlation. 

Consider a portfolio of \(N\) bonds and assume that the marginal default probabilities are known for each issuer. Define:

\begin{itemize}
	\tightlist
	\item
	\(t_i\), the time of default of the \(i^{th}\) company:
	\item
	\(Q_i(t)\), the cumulative probability that company \(i\) will default
	before time \(t\); that is, the probability that \(t_i \le t\).
\end{itemize}

To generate a one-factor model for the default times (\(t_i\)) we define random
variables \(X_i\) \((1\le i \le N)\)
\begin{equation}
X_i = a_i M + \sqrt{1-a_i^2}Z_i,\qquad i = 1, 2,\ldots, N
\label{eq:normalized_var}
\end{equation}
where \(M\) and the \(Z_i\) are independent zero-mean unit-variance  distributions (hence $X_i$ are also distributed with zero-mean and unit standard-deviation) and \(-1 \le a_i \lt 1\).

Eq.~\ref{eq:normalized_var} defines a correlation structure between the \(X_i\) which are dependent on a single common factor \(M\). The $Z_i$ term is usually called the idiosyncratic component of default. The particular functional form of Eq.~\ref{eq:normalized_var} has been chosen to ensure the desired correlation structure among the $X_i$. Indeed the correlation between \(X_i\) and \(X_j\) is

\begin{equation*}
\rho(X_i, X_j) = \cfrac{\mathbb{E}[(X_i-\mu_i)(X_j-\mu_j)]}{\sigma_{i}\sigma_{j}} =\mathbb{E}[X_i X_j] = a_i a_j \mathbb{E}[M^2] = a_i a_j
\end{equation*}
where we just exploit the definition of $X_i$ and its properties.

%Assume that the $i^{th}$ company has defaulted by the time $t_i$ if $X_i$ is below a threshold value $\bar{x}_i(t_i)$.
If $F_i$ is the cumulative distribution function of $X_i$, with a percentile to percentile transformation (see Section~\ref{distribution-transformation}) we can map \(X_i\) to \(t_i\), so that $Q_i(t_i) = P(X_i\le x)=F_i(x)$. Therefore a point \(X_i = x\) is transformed to \(t_i = t\) with \(x = F_i^{-1}[Q_i(t)]\) connecting the marginal distribution of the copula model to the default times.

Let's note that, \textbf{conditional} on $M$, the $N$ default events are \textbf{independent}. So we can write
\begin{equation}
\begin{split}
Q_i^{\textrm{corr}}(t_i|M) = P(X_i\le x|M) &= P(a_i M + \sqrt{1-a_i^2}Z_i\le x) =\\
&= P\left(Z_i\le \cfrac{x-a_i M}{\sqrt{1-a_i^2}}\right)
=H_i\left(\cfrac{F^{-1}[Q(t_i)]-a_i M}{\sqrt{1-a_i^2}}\right)
\end{split}
\label{eq:generic_copula}
\end{equation}
where $H_i$ is the cumulative distribution function of the $Z_i$.

Although in principle any distribution could be used for \(M\) and the \(Z\)'s (provided they have zero mean and unit variance), one common choice is to let them be standard normal distributions (resulting in a Gaussian copula).
So we can rewrite Eq.~\ref{eq:generic_copula} as

\begin{equation}
Q_i^{\textrm{corr}}(t_i|M) = \Phi\left(\cfrac{\Phi^{-1}[Q(t_i)]-a_i M}{\sqrt{1-a_i^2}}\right)
\label{eq:gaussian_one_factor_copula}
\end{equation}
where $\Phi$ denotes the cumulative distribution function of the standard normal distribution.

If we call $\mathcal{C}(t_1,\ldots,t_N)$ the joint distribution of the default times of the $N$ bonds in the portfolio then

\begin{equation}
\mathcal{C}(t_1,\ldots,t_N, \rho)=\Phi_{N}(\Phi^{-1}(Q_1(t_1)),\ldots,\Phi^{-1}(Q_N(t_N)), \boldsymbol{\rho})
\label{eq:gaussian_copula}
\end{equation}
where $\boldsymbol{\rho}$ represents the correlation matrix of the default probabilities. Eq.~\ref{eq:gaussian_copula} is the one factor Gaussian copula model (one factor because there is only a random variable, $M$, which determines the correlation between $X_i$).

Clearly different choices of distributions result in different copula models, and in various natures of the default dependence. For example, copulas where the \(M\) have heavy tails generate models where there is a greater likelihood of a clustering of early defaults for several companies.

\subsection{Standard Market Model}\label{standard-market-model}

Consider the following two assumptions are made:

\begin{itemize}
	\tightlist
	\item
	all the companies have the same default intensity (hazard rates), i.e, \(\lambda_i = \lambda\) (which means they all have the same default probabilities $Q_i = Q$);
	\item
	the pairwise default correlations are the same, i.e \(a_i = a\); in other words the contribution of the market
	component $M$ is the same for all the companies and the correlation between any two companies is constant, \(\rho = a^2\).
\end{itemize}

Under these assumptions, given the market situation \(M = m\), all the companies have the same cumulative default probability \(Q_i^{\textrm{corr}}(t_i|m)=P(X_i < x|m)\). Moreover, for a given value of the market component \(M\), the defaults are mutually independent for all the underlying companies. 

In this case the one factor Gaussian model is also called \emph{Market Standard Model}. Later in this Chapter we will also see how the correlation used in Eq.~\ref{eq:gaussian_one_factor_copula} can be implied from the market quotes of credit derivatives.

Letting \(l(t|m)\) be the total defaults that have occurred by time \(t\) conditional on the market condition \(M = m\), then \(l(t|m)\) follows a binomial distribution (see Appendix~\ref{binomial-distribution}), and the probability of default of $j$ companies can be expressed as

\begin{equation}
DP(l(t|m) = j) = \cfrac{N!}{j!(N-j)!}(Q_i^{\textrm{corr}}(t_i|m))^j(1-Q_i^{\textrm{corr}}(t_i|m))^{N-j},\qquad  j=0, 1, 2,\ldots,N
\end{equation}

In general to evaluate any function $g(DP(l(t))$ regardless the parameter $M$ (e.g. an NPV or a fair value which depend on the default probability, remember~\ref{sec:cds_valuation}) is necessary to average according to all the possible values that $M$ can take. The average is performed with an integral which is the weighted sum of $g$ with the probability distribution ($f_M(m)$) of the random variable $M$  

\begin{equation}
g(DP(l(t) = j)) = \int_{-\infty}^{\infty}{g(DP(l(t|m) = j))\cdot f_M(m)dm}
\label{eq:gaussian_quadrature}
\end{equation}

In the general case where the default probabilities of each company are not the same it is possible to determine $DP(l(t|M)=j)$ through an iterative procedure before proceeding with the integration of Eq.~\ref{eq:gaussian_quadrature}. An example of this iterative technique will be shown in Section~\ref{sec:expected_losses}.

\subsection{Extensions of the One Factor Copula Model}
Many other one-factor model have been tried: t-Student copula, Clayton copula, \ldots In general we can define a new model by simply choosing particular functional forms for $M$ and $Z_i$ in Eq.~\ref{eq:normalized_var} provided they are with zero mean and standard deviation one. 

If instead of the single factor $M$ there are two or more, Eq.~\ref{eq:normalized_var} becomes

\begin{equation}
X_i = a_1 M_1 + a_2 M_2 + \sqrt{1 - a_1^2 - a^2_2}Z_i
\end{equation}
and similarly
\begin{equation}
Q^{\textrm{corr}}(t|M_1, M_2) = \Phi\left(\cfrac{\Phi^{-1}[Q(t)]-a_1 M_1 - a_2 M_2}{\sqrt{1 - a_1^2 - a^2_2}}\right)
\end{equation}
In terms of computation time, this kind of models are proportionally slower with the increase of the number of factors, since the integration of Eq.~\ref{eq:gaussian_quadrature} has to be carried on each factor of the model.

\section{Complex Correlation Structures and the Financial Crisis}
\label{complex-correlation-structures-and-the-financial-crisis}

In the derivation of the Gaussian Copula Model we have used the normal distribution. However, we could have used other and more complex copulas as well. For example we might want to assume the correlation is non-symmetric which is useful in finance when correlations become very strong during market crashes and returns very negative.

In fact, Gaussian copulas are said to have played a key role in the 2008 financial crisis as tail-correlations were severely underestimated. Consider a set of mortgages in a CDO: they are clearly correlated, since if one mortgage fails, the likelihood that another failing is increased. In the early 2000s, the banks only knew how to model the marginals of the default rates. Then an (in)famous paper by Li~\cite{bib:copula_li} suggested to use Gaussian copulas to model the correlations between those marginals. Rating agencies relied on this model so heavily, that severely underestimated the risk and gave false ratings\ldots

If you are interested in the argument read this paper~\cite{bib:copula_and_2008} for an excellent description of Gaussian copulas and the Financial Crisis, which argues that different copula choices would not have made a difference but instead the assumed correlation was way too low.

\section*{Exercises}
\input{modelling_risk_ex_text}

\begin{thebibliography}{9}
\bibitem{bib:copula}P. Wilmott, \emph{Quantitative Finance (2nd edition)}, Credit Derivatives (Ch. 41), John Wiley and Sons, 2006 
\bibitem{bib:copula_nelsen}R. Nelsen, \emph{An Introduction to Copulas}, Springer–Verlag, 1999
\bibitem{bib:copula_cherubini}U. Cherubini, E. Luciano, W. Vecchiato, \emph{Copula Methods in Finance}, John Wiley \& Sons, 2004
\bibitem{bib:t_student}\href{https://en.wikipedia.org/wiki/Student\%27s\_t-distribution}{\emph{t-Student distribution}}, Wikipedia [Online]
\bibitem{bib:gumbel}\href{https://en.wikipedia.org/wiki/Gumbel_distribution}{\emph{Gumbel distribution}}, Wikipedia [Online]
\bibitem{bib:beta}\href{https://en.wikipedia.org/wiki/Beta_distribution}{\emph{Beta distribution}}, Wikipedia [Online]
\bibitem{bib:copula_and_2008} S. Watts, \href{http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf}{\emph{Gaussian Copula and Financial Crisis}} [Online]
\bibitem{bib:copula_li}D. X. Li, \href{http://www.maths.lth.se/matstat/kurser/fmsn15masm23/default.pdf}{\emph{On Default Correlation: A Copula Function Approach}}, RiskMetrics Group, 2000 [Online]
\end{thebibliography}
