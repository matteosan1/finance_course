\chapter{Time Series Forecasting}\label{time-series-forecasting}

Forecasting is the next natural step in the analysis of a time series,
where you want to predict the future values the series is going to take.

Forecasting a time series (like demand and sales) is often of tremendous
commercial value. In most manufacturing companies, it drives the
fundamental business planning, procurement and production activities.
Any errors in the forecasts will ripple down throughout the supply chain
or any business context for that matter. So it's important to get the
forecasts accurate in order to save on costs and is critical to success.

Beyond this example, the techniques and concepts behind time series
forecasting are applicable in any business.

Forecasting a time series can be broadly divided into two types. If you
use only the previous values of the time series to predict its future
values, it is called \emph{Univariate Time Series Forecasting}. If
instead you use predictors other than the series (exogenous
variables) to forecast it is called
\emph{Multi Variate Time Series Forecasting}.

In the first part of this Chapter various classical time series forecasting model are
briefly reviewed. Next forecasting techniques involving machine learning
are described.

\section{Classical Models}
\subsection{Autoregression (AR)}\label{autoregression-ar}

In the previous Chapter we have seen that an autoregression (AR) model
describes the next step in a sequence as a linear function of the
observations at prior time steps.

The correlation statistics can help to choose which lag variables
are useful and which not. Interestingly, if all lag
variables show low or no correlation with the output variable, then it
suggests that the time series problem may not be predictable. This can
be very useful when getting started on a new dataset.

The \texttt{statsmodels.tsa.ar\_model} library provides an
autoregression model (\texttt{AutoReg}
class) where you must specify an appropriate lag value and
trains a linear regression model. The model is 'trained' on our dataset
calling \texttt{fit()}.
Once fit, we can use it to make a prediction by calling the
\texttt{predict()} function for a number of observations in the future.

%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{ar\PYZus{}model} \PY{k}{import} \PY{n}{AutoReg}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}\PY{p}{,} \PY{n}{seed}
%
%\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
%
%\PY{n}{model} \PY{o}{=} \PY{n}{AutoReg}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
%
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%
%[100.55071444] 99.56135786477837
%\end{Verbatim}
%\end{tcolorbox}

\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{ar\PYZus{}model} \PY{k}{import} \PY{n}{AutoReg}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
\PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{sqrt}
\PY{k}{import} \PY{n}{pandas} \PY{k}{as} \PY{n}{pd}

\PY{n}{series} \PY{o}{=} \PY{n}{pd.read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daily\PYZhy{}min\PYZhy{}temperatures.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                                                   \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{squeeze}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{X} \PY{o}{=} \PY{n}{series}\PY{o}{.}\PY{n}{values}
\PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{:}\PY{p}{]}

\PY{n}{model} \PY{o}{=} \PY{n}{AutoReg}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{29}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficients: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{params}\PY{p}{)}

\PY{n}{predictions} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{start}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{test}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} 
                                \PY{n}{dynamic}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{rmse} \PY{o}{=} \PY{n}{sqrt}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test RMSE: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{rmse}\PY{p}{)}

Coefficients: [ 5.57543506e-01  5.88595221e-01 -9.08257090e-02  4.82615092e-02
  4.00650265e-02  3.93020055e-02  2.59463738e-02  4.46675960e-02
  1.27681498e-02  3.74362239e-02 -8.11700276e-04  4.79081949e-03
  1.84731397e-02  2.68908418e-02  5.75906178e-04  2.48096415e-02
  7.40316579e-03  9.91622149e-03  3.41599123e-02 -9.11961877e-03
  2.42127561e-02  1.87870751e-02  1.21841870e-02 -1.85534575e-02
 -1.77162867e-03  1.67319894e-02  1.97615668e-02  9.83245087e-03
  6.22710723e-03 -1.37732255e-03]

Test RMSE: 1.225
\end{Verbatim}
\end{tcolorbox}

\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_46_2.png}
\end{center}
    
%\section{Moving Average (MA)}\label{moving-average-ma}
%
%The moving average (MA) method models the next step in the sequence as a
%linear function of the residual errors from a mean process at prior time
%steps. 
%
%We can use the \texttt{ARIMA} class to create an MA model and setting a
%zeroth-order AR model (we must specify the order of the MA model in the
%order argument). Like before we are going to define a dummy series and
%try to fit a first-order MA model to it.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima}\PY{n+nn}{.}\PY{n+nn}{model} \PY{k}{import} \PY{n}{ARIMA}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
%
%\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
%
%\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
%
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%
%[0.50521534] 0.6881898080477126
%\end{Verbatim}
%\end{tcolorbox}

%\section{Autoregressive Moving Average (ARMA)}
%\label{autoregressive-moving-average-arma}
%
%The Autoregressive Moving Average (ARMA) method models the next step in
%the sequence as a linear function of the observations and residual
%errors at prior time steps. This model combines both autoregression (AR)
%and Moving Average (MA) models.
%
%The notation for the model involves specifying the order for the
%AR(\(p\)) and MA(\(q\)) models as parameters to an ARMA function,
%e.g.~ARMA(\(p\), \(q\)). 
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima}\PY{n+nn}{.}\PY{n+nn}{model} \PY{k}{import} \PY{n}{ARIMA}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
%
%\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
%
%\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
%
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%
%[0.52629796] 0.4950722507160109
%\end{Verbatim}
%\end{tcolorbox}

\section{Autoregressive Integrated Moving Average
(ARIMA)}\label{autoregressive-integrated-moving-average-arima}

The Autoregressive Integrated Moving Average (ARIMA) method models the
next step in the sequence as a linear function of the \emph{differenced}
observations and residual errors at prior time steps.

It combines both autoregression (AR) and Moving Average (MA) models as
well as a differencing pre-processing step of the sequence to make the
sequence stationary, called \emph{integration} (the I in the name).

Any `non-seasonal' time series that exhibits patterns and is not a
random white noise can be modeled with ARIMA models.

An ARIMA model is characterized by 3 terms: \(p\), \(d\), \(q\) where,

\begin{itemize}
\tightlist
\item
  \(p\) is the order of the AR term;
\item
  \(q\) is the order of the MA term;
\item
  \(d\) is the number of differencing required to make the time series
  stationary.
\end{itemize}

%\subsection{Determination of $d$ Term}
%\label{determination-of-d-term}
%
%The first step to build an ARIMA model is to make the time series
%stationary, because, the term `Auto Regressive' in ARIMA means it is a
%linear regression model that uses its own lags as predictors. Linear
%regression models work best when the predictors are not
%correlated and are independent of each other.
%
%The most common approach to make a series stationary is to difference
%it~\ref{sec:diff}. Sometimes, depending on the complexity of the series, 
%more than one differencing may be needed.
%
%The value of \(d\), therefore, is the minimum number of differencing
%needed to make the series stationary. And if the time series is already
%stationary, then \(d = 0\). On the other hand you need to be careful to
%not over-difference the series. Because, an over differenced series may
%still be stationary, which in turn will affect the model parameters.
%
%The right order of differencing is the minimum differencing required to
%get a near-stationary series which roams around a defined mean and the
%ACF plot reaches to zero fairly quick.
%
%If the autocorrelations are positive for many number of lags (10 or
%more), then the series needs further differencing. On the other hand, if
%the lag-1 autocorrelation itself is too negative, then the series is
%probably over-differenced.
%
%In the event, you can't really decide between two orders of
%differencing, then go with the order that gives the least standard
%deviation in the differenced series.
%
%Let's see how to do it with an example.
%
%First, I am going to check if the series is stationary using the
%Augmented Dickey Fuller test because, you need differencing only if the
%series is non-stationary.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{adfuller}
%\PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{log}
%
%\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/wwwusage.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
%                 \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
%
%\PY{n}{result} \PY{o}{=} \PY{n}{adfuller}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ADF Statistic: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{result}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%
%ADF Statistic: -2.464240
%p-value: 0.124419
%\end{Verbatim}
%\end{tcolorbox}
%
%Since p-value is greater than the significance level, let's difference
%the series and see how the autocorrelation plot looks like
%
%%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%%\begin{Verbatim}[commandchars=\\\{\}]
%%\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
%%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}
%%\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
%%\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
%%
%%\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%%
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Order Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%%
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2nd Order Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%%
%%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%%\end{Verbatim}
%%\end{tcolorbox}
%
%\begin{center}
%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_54_0.png}
%\end{center}
%    
%The time series reaches stationarity with two
%orders of differencing. But on looking at the autocorrelation plot for
%the $2^{nd}$ differencing the lag goes into the far negative zone fairly
%quick, which indicates, the series might have been over differenced.
%So, I am going to tentatively fix the order of differencing as 1 even
%though the series is not perfectly stationary (weak stationarity).
%
%\subsection{Determination of $p$ Term}
%\label{determination-of-p-term}
%
%The next step is to identify if the model needs any AR terms. You can
%find out the required number of AR terms by inspecting the Partial
%Autocorrelation plot.
%
%Any autocorrelation in a stationarized series can be rectified by adding
%enough AR terms. So, we initially take the order of AR term to be equal
%to as many lags that crosses the significance limit in the PACF plot.
%
%%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%%\begin{Verbatim}[commandchars=\\\{\}]
%%\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
%%
%%\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{ylim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
%%\PY{n}{plot\PYZus{}pacf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%%\end{Verbatim}
%%\end{tcolorbox}
%
%\begin{center}
%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_57_0.png}
%\end{center}
%    
%You can observe that the PACF lag-1 is quite significant since is well
%above the significance line. Lag-2 turns out to be significant as well,
%slightly managing to cross the significance limit (blue region). But I
%am going to be conservative and tentatively fix the \(p\) as 1.
%
%\subsection{Determination of $q$ Term}
%\label{determination-of-q-term}
%
%Just like how we looked at the PACF plot for the number of AR terms, you
%can look at the ACF plot for the number of MA terms. An MA term is
%technically, the error of the lagged forecast.
%
%The ACF tells how many MA terms are required to remove any
%autocorrelation in the stationarized series.
%
%%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%%\begin{Verbatim}[commandchars=\\\{\}]
%%\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
%%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}
%%\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
%%\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
%%
%%\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{;} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{ylim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{1.2}\PY{p}{)}\PY{p}{)}
%%\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%%
%%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%%\end{Verbatim}
%%\end{tcolorbox}
%
%\begin{center}
%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_59_0.png}
%\end{center}
%    
%Couple of lags are well above the significance line. So, let's
%tentatively fix \(q\) as 2. When in doubt, go with the simpler model
%that sufficiently explains the \(Y\).
%
%%\subsubsection{How to handle if a time series is slightly under or over
%%differenced}\label{how-to-handle-if-a-time-series-is-slightly-under-or-over-differenced}
%%
%%It may so happen that your series is slightly under differenced, that
%%differencing it one more time makes it slightly over-differenced.
%%
%%If your series is slightly under differenced, adding one or more
%%additional AR terms usually makes it up. Likewise, if it is slightly
%%over-differenced, try adding an additional MA term.
%
%\subsubsection{How to build the ARIMA Model}
%\label{how-to-build-the-arima-model}
%
%Now that you've determined the values of \(p\), \(d\) and \(q\), you
%have everything needed to fit the ARIMA model.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{arima\PYZus{}model} \PY{k}{import} \PY{n}{ARIMA}
%
%\PY{c+c1}{\PYZsh{} 1,1,2 ARIMA Model}
%\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
%
%                             ARIMA Model Results
%==============================================================================
%Dep. Variable:                D.value   No. Observations:                   99
%Model:                 ARIMA(1, 1, 2)   Log Likelihood                -253.790
%Method:                       css-mle   S.D. of innovations              3.119
%Date:                Wed, 27 Jan 2021   AIC                            517.579
%Time:                        15:50:35   BIC                            530.555
%Sample:                             1   HQIC                           522.829
%
%================================================================================
%=
%                    coef    std err          z      P>|z|      [0.025
%0.975]
%--------------------------------------------------------------------------------
%-
%const             1.1202      1.290      0.868      0.385      -1.409
%3.649
%ar.L1.D.value     0.6351      0.257      2.469      0.014       0.131
%1.139
%ma.L1.D.value     0.5287      0.355      1.489      0.136      -0.167
%1.224
%ma.L2.D.value    -0.0010      0.321     -0.003      0.998      -0.631
%0.629
%                                    Roots
%=============================================================================
%                  Real          Imaginary           Modulus         Frequency
%-----------------------------------------------------------------------------
%AR.1            1.5746           +0.0000j            1.5746            0.0000
%MA.1           -1.8850           +0.0000j            1.8850            0.5000
%MA.2          544.9014           +0.0000j          544.9014            0.0000
%-----------------------------------------------------------------------------
%\end{Verbatim}
%\end{tcolorbox}
%
%The model summary reveals a lot of information. The table in the middle
%is the coefficients table where the values under `coef' are the weights
%of the respective terms.
%
%Notice here the coefficient of the MA2 term is close to zero and the
%p-Value in `P\textgreater{}\textbar{}z\textbar{}' column is highly
%insignificant. It should ideally be less than 0.05 for the respective
%term to be significant.
%
%So, let's rebuild the model without the MA2 term.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
%
%                             ARIMA Model Results
%==============================================================================
%Dep. Variable:                D.value   No. Observations:                   99
%Model:                 ARIMA(1, 1, 1)   Log Likelihood                -253.790
%Method:                       css-mle   S.D. of innovations              3.119
%Date:                Wed, 27 Jan 2021   AIC                            515.579
%Time:                        15:50:50   BIC                            525.960
%Sample:                             1   HQIC                           519.779
%
%================================================================================
%=
%                    coef    std err          z      P>|z|      [0.025
%0.975]
%--------------------------------------------------------------------------------
%-
%const             1.1205      1.286      0.871      0.384      -1.400
%3.641
%ar.L1.D.value     0.6344      0.087      7.317      0.000       0.464
%0.804
%ma.L1.D.value     0.5297      0.089      5.932      0.000       0.355
%0.705
%                                    Roots
%=============================================================================
%                  Real          Imaginary           Modulus         Frequency
%-----------------------------------------------------------------------------
%AR.1            1.5764           +0.0000j            1.5764            0.0000
%MA.1           -1.8879           +0.0000j            1.8879            0.5000
%-----------------------------------------------------------------------------
%\end{Verbatim}
%\end{tcolorbox}
%
%The model AIC has reduced, which is good. The p-values of the AR1 and
%MA1 terms have improved and are highly significant
%(\textless{}\textless{} 0.05). Let's plot the residuals to ensure there
%are no patterns (that is, look for constant mean and variance).
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{residuals} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{resid}\PY{p}{)}
%\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
%\PY{n}{residuals}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuals}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
%\PY{n}{residuals}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%\begin{center}
%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_65_0.png}
%\end{center}
%    
%The residual errors seem fine with near zero mean and uniform variance.
%Let's plot the actuals against the fitted values using plot\_predict().
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{plot\PYZus{}predict}\PY{p}{(}\PY{n}{dynamic}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%\begin{center}
%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_67_0.png}
%\end{center}
    
\subsubsection{Fitting ARIMA Model}
\label{sec:fitting_arima_models}

In order to define an ARIMA model it is necessary to choose an appropriate set of
parameters ($p$, $q$ and $d$).

This can be done through a grid search on the three parameters checking which 
choice gives least AIC (Akaike Information Criteria) 
(and also looking for a chart tha gives closer actuals
and forecasts). While doing this, the p-values of the AR and MA terms
in the model summary need to be observed paying attention that their values
stay close to zero, ideally, less than 0.05.

Performance is checked creating the training and
testing dataset by splitting the time series into 2 contiguous parts in
approximately 75:25 ratio or a reasonable proportion based on time.

\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
	\begin{Verbatim}[commandchars=\\\{\}]
		\PY{c+c1}{\PYZsh{} Build Model}
		\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}  
		\PY{n}{fitted} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  
		\PY{n+nb}{print}\PY{p}{(}\PY{n}{fitted}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
		
		\PY{c+c1}{\PYZsh{} Forecast}
		\PY{n}{fc}\PY{p}{,} \PY{n}{se}\PY{p}{,} \PY{n}{conf} \PY{o}{=} \PY{n}{fitted}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 95\PYZpc{} conf}
		
		\PY{c+c1}{\PYZsh{} Make as pandas series}
		\PY{n}{fc\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
		\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
		\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
		
		\PY{c+c1}{\PYZsh{} Plot}
		\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fc\PYZus{}series}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{lower\PYZus{}series}\PY{p}{,} \PY{n}{upper\PYZus{}series}\PY{p}{,} 
		\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast vs Actuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
		\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

In Out-of-Time cross-validation, you take few steps back in time and
forecast into the future to as many steps you took back. Then you
compare the forecast against the actuals.


\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k}{import} \PY{n}{acf}

\PY{n}{train} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{85}\PY{p}{]}
\PY{n}{test} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{value}\PY{p}{[}\PY{l+m+mi}{85}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

You can now build the ARIMA model on training dataset, forecast and plot
it.

\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} model = ARIMA(train, order=(3,2,1))  }
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}  
\PY{n}{fitted} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  

\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{fc}\PY{p}{,} \PY{n}{se}\PY{p}{,} \PY{n}{conf} \PY{o}{=} \PY{n}{fitted}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 95\PYZpc{} conf}

\PY{c+c1}{\PYZsh{} Make as pandas series}
\PY{n}{fc\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fc\PYZus{}series}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{lower\PYZus{}series}\PY{p}{,} \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast vs Actuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_71_1.png}
\end{center}
    
From the chart, the ARIMA(1,1,1) model seems to give a directionally
correct forecast. And the actual observed values lie within the 95\%
confidence band. That seems fine.

But each of the predicted forecasts is consistently below the actuals.
That means, by adding a small constant to our forecast, the accuracy
will certainly improve. So, there is definitely scope for improvement.


\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Build Model}
\PY{n}{model} \PY{o}{=} \PY{n}{ARIMA}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}  
\PY{n}{fitted} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  
\PY{n+nb}{print}\PY{p}{(}\PY{n}{fitted}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Forecast}
\PY{n}{fc}\PY{p}{,} \PY{n}{se}\PY{p}{,} \PY{n}{conf} \PY{o}{=} \PY{n}{fitted}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 95\PYZpc{} conf}

\PY{c+c1}{\PYZsh{} Make as pandas series}
\PY{n}{fc\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{conf}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{test}\PY{o}{.}\PY{n}{index}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plot}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fc\PYZus{}series}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{forecast}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{lower\PYZus{}series}\PY{p}{,} \PY{n}{upper\PYZus{}series}\PY{p}{,} 
                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Forecast vs Actuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

                             ARIMA Model Results
==============================================================================
Dep. Variable:               D2.value   No. Observations:                   83
Model:                 ARIMA(3, 2, 1)   Log Likelihood                -214.248
Method:                       css-mle   S.D. of innovations              3.153
Date:                Wed, 27 Jan 2021   AIC                            440.497
Time:                        15:53:48   BIC                            455.010
Sample:                             2   HQIC                           446.327

================================================================================
==
                     coef    std err          z      P>|z|      [0.025
0.975]
--------------------------------------------------------------------------------
--
const              0.0483      0.084      0.577      0.564      -0.116
0.212
ar.L1.D2.value     1.1386      0.109     10.399      0.000       0.924
1.353
ar.L2.D2.value    -0.5923      0.155     -3.827      0.000      -0.896
-0.289
ar.L3.D2.value     0.3079      0.111      2.778      0.005       0.091
0.525
ma.L1.D2.value    -1.0000      0.035    -28.799      0.000      -1.068
-0.932
                                    Roots
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.1557           -0.0000j            1.1557           -0.0000
AR.2            0.3839           -1.6318j            1.6763           -0.2132
AR.3            0.3839           +1.6318j            1.6763            0.2132
MA.1            1.0000           +0.0000j            1.0000            0.0000
-----------------------------------------------------------------------------
\end{Verbatim}
\end{tcolorbox}

\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_73_1.png}
\end{center}
    
The AIC has reduced to 440 from 515. Good. The p-values of the \(Y\)
terms are less the \textless{} 0.05, which is great. So overall it's
much better.

Ideally, you should go back multiple points in time, like, go back 1, 2,
3 and 4 quarters and see how your forecasts are performing at various
points in the year.

\%Here's a great practice exercise: Try to go back 27, 30, 33, 36 data
points and see how the forcasts performs. \%The forecast performance can
be judged using various accuracy metrics discussed next.

\subsection{Accuracy Metrics for Time Series
Forecast}\label{accuracy-metrics-for-time-series-forecast}

The commonly used accuracy metrics to judge forecasts are:

Mean Absolute Percentage Error (MAPE) Mean Error (ME) Mean Absolute
Error (MAE) Mean Percentage Error (MPE) Root Mean Squared Error (RMSE)
Lag-1 Autocorrelation of Error (ACF1) Correlation between the Actual and
the Forecast (corr) Min-Max Error (minmax)

Typically, if you are comparing forecasts of two different series, the
MAPE, Correlation and Min-Max Error can be used. Because only the above
three are percentage errors that vary between 0 and 1. That way, you can
judge how good is the forecast irrespective of the scale of the series.

The other error metrics are quantities. That implies, an RMSE of 100 for
a series whose mean is in 1000's is better than an RMSE of 5 for series
in 10's. So, you can't really use them to compare the forecasts of two
different scaled time series.

\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Accuracy metrics}
\PY{k}{def} \PY{n+nf}{forecast\PYZus{}accuracy}\PY{p}{(}\PY{n}{forecast}\PY{p}{,} \PY{n}{actual}\PY{p}{)}\PY{p}{:}
    \PY{n}{mape} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{actual}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} MAPE}
    \PY{n}{me} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}             \PY{c+c1}{\PYZsh{} ME}
    \PY{n}{mae} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{p}{)}    \PY{c+c1}{\PYZsh{} MAE}
    \PY{n}{mpe} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{/}\PY{n}{actual}\PY{p}{)}   \PY{c+c1}{\PYZsh{} MPE}
    \PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{forecast} \PY{o}{\PYZhy{}} \PY{n}{actual}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{o}{.}\PY{l+m+mi}{5}  \PY{c+c1}{\PYZsh{} RMSE}
    \PY{n}{corr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{forecast}\PY{p}{,} \PY{n}{actual}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}   \PY{c+c1}{\PYZsh{} corr}
    \PY{n}{mins} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{amin}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{forecast}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{,} 
                              \PY{n}{actual}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{maxs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{forecast}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{,} 
                              \PY{n}{actual}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{k+kc}{None}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{minmax} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{mins}\PY{o}{/}\PY{n}{maxs}\PY{p}{)}             \PY{c+c1}{\PYZsh{} minmax}
    \PY{n}{acf1} \PY{o}{=} \PY{n}{acf}\PY{p}{(}\PY{n}{fc}\PY{o}{\PYZhy{}}\PY{n}{test}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}                      \PY{c+c1}{\PYZsh{} ACF1}
    \PY{k}{return}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{mape}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{me}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{me}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mae}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{mae}\PY{p}{,} 
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mpe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{mpe}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{rmse}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{acf1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{acf1}\PY{p}{,} 
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{corr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{corr}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{minmax}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{forecast\PYZus{}accuracy}\PY{p}{(}\PY{n}{fc}\PY{p}{,} \PY{n}{test}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\{'mape': 0.022501338735867873,
 'me': 3.2307975842766514,
 'mae': 4.548327520525081,
 'mpe': 0.016421068924791414,
 'rmse': 6.373248791560426,
 'acf1': 0.5105503577711675,
 'corr': 0.967457637120179,
 'minmax': 0.021631569901959136\}
\end{Verbatim}
\end{tcolorbox}
        
Around 2.2\% MAPE implies the model is about 97.8\% accurate in
predicting the next 15 observations.

\subsection{Seasonal Autoregressive Integrated Moving-Average
(SARIMA)}\label{seasonal-autoregressive-integrated-moving-average-sarima}

The Seasonal Autoregressive Integrated Moving Average (SARIMA) method
models the next step in the sequence as a linear function of the
differenced observations, errors, differenced seasonal observations, and
seasonal errors at prior time steps.

It combines the ARIMA model with the ability to perform the same
autoregression, differencing, and moving average modeling at the
seasonal level.

The notation for the model involves specifying the order for the AR(p),
I(d), and MA(q) models as parameters to an ARIMA function and AR(P),
I(D), MA(Q) and m parameters at the seasonal level, e.g.~SARIMA(p, d,
q)(P, D, Q)m where ``m'' is the number of time steps in each season (the
seasonal period). 

How to automatically build SARIMA model in python The problem with plain
ARIMA model is it does not support seasonality.

If your time series has defined seasonality, then, go for SARIMA which
uses seasonal differencing.

Seasonal differencing is similar to regular differencing, but, instead
of subtracting consecutive terms, you subtract the value from previous
season.

So, the model will be represented as SARIMA(p,d,q)x(P,D,Q), where, P, D
and Q are SAR, order of seasonal differencing and SMA terms respectively
and `x' is the frequency of the time series.

If your model has well defined seasonal patterns, then enforce D=1 for a
given frequency `x'.

Here's some practical advice on building SARIMA model:

As a general rule, set the model parameters such that D never exceeds
one. And the total differencing `d + D' never exceeds 2. Try to keep
only either SAR or SMA terms if your model has seasonal components.

\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} SARIMA example}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{sarimax} \PY{k}{import} \PY{n}{SARIMAX}
\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
\PY{c+c1}{\PYZsh{} contrived dataset}
\PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
\PY{c+c1}{\PYZsh{} fit model}
\PY{n}{model} \PY{o}{=} \PY{n}{SARIMAX}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{seasonal\PYZus{}order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{c+c1}{\PYZsh{} make prediction}
\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

\subsection{Seasonal Autoregressive Integrated Moving-Average with
Exogenous Regressors
(SARIMAX)}\label{seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax}

The Seasonal Autoregressive Integrated Moving-Average with Exogenous
Regressors (SARIMAX) is an extension of the SARIMA model that also
includes the modeling of exogenous variables.

Exogenous variables are also called covariates and can be thought of as
parallel input sequences that have observations at the same time steps
as the original series. The primary series may be referred to as
endogenous data to contrast it from the exogenous sequence(s). The
observations for exogenous variables are included in the model directly
at each time step and are not modeled in the same way as the primary
endogenous sequence (e.g.~as an AR, MA, etc. process).

%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} SARIMAX example}
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{sarimax} \PY{k}{import} \PY{n}{SARIMAX}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
%\PY{c+c1}{\PYZsh{} contrived dataset}
%\PY{n}{data1} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}
%\PY{n}{data2} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}\PY{p}{]}
%\PY{c+c1}{\PYZsh{} fit model}
%\PY{n}{model} \PY{o}{=} \PY{n}{SARIMAX}\PY{p}{(}\PY{n}{data1}\PY{p}{,} \PY{n}{exog}\PY{o}{=}\PY{n}{data2}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{seasonal\PYZus{}order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
%\PY{c+c1}{\PYZsh{} make prediction}
%\PY{n}{exog2} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{200} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}\PY{p}{]}
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data1}\PY{p}{)}\PY{p}{,} \PY{n}{exog}\PY{o}{=}\PY{p}{[}\PY{n}{exog2}\PY{p}{]}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
%
%[100.35471348]
%\end{Verbatim}
%\end{tcolorbox}

\section{Vector Autoregression (VAR)}
\label{vector-autoregression-var}

The Vector Autoregression (VAR) method models the next step in each time
series using an AR model. It is the generalization of AR to multiple
parallel time series, e.g.~multivariate time series.

The notation for the model involves specifying the order for the AR(p)
model as parameters to a VAR function, e.g.~VAR(p).

%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} VAR example}
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{vector\PYZus{}ar}\PY{n+nn}{.}\PY{n+nn}{var\PYZus{}model} \PY{k}{import} \PY{n}{VAR}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
%\PY{c+c1}{\PYZsh{} contrived dataset with dependency}
%\PY{n}{data} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
%\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
%    \PY{n}{v1} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
%    \PY{n}{v2} \PY{o}{=} \PY{n}{v1} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
%    \PY{n}{row} \PY{o}{=} \PY{p}{[}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{]}
%    \PY{n}{data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{)}
%\PY{c+c1}{\PYZsh{} fit model}
%\PY{n}{model} \PY{o}{=} \PY{n}{VAR}\PY{p}{(}\PY{n}{data}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
%\PY{c+c1}{\PYZsh{} make prediction}
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{y}\PY{p}{,} \PY{n}{steps}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

\subsection{Vector Autoregression Moving-Average
(VARMA)}\label{vector-autoregression-moving-average-varma}

The Vector Autoregression Moving-Average (VARMA) method models the next
step in each time series using an ARMA model. It is the generalization
of ARMA to multiple parallel time series, e.g.~multivariate time series.

The notation for the model involves specifying the order for the AR(p)
and MA(q) models as parameters to a VARMA function, e.g.~VARMA(p, q). A
VARMA model can also be used to develop VAR or VMA models.

%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} VARMA example}
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{statespace}\PY{n+nn}{.}\PY{n+nn}{varmax} \PY{k}{import} \PY{n}{VARMAX}
%\PY{k+kn}{from} \PY{n+nn}{random} \PY{k}{import} \PY{n}{random}
%\PY{c+c1}{\PYZsh{} contrived dataset with dependency}
%\PY{n}{data} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
%\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
%    \PY{n}{v1} \PY{o}{=} \PY{n}{random}\PY{p}{(}\PY{p}{)}
%    \PY{n}{v2} \PY{o}{=} \PY{n}{v1} \PY{o}{+} \PY{n}{random}\PY{p}{(}\PY{p}{)}
%    \PY{n}{row} \PY{o}{=} \PY{p}{[}\PY{n}{v1}\PY{p}{,} \PY{n}{v2}\PY{p}{]}
%    \PY{n}{data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{row}\PY{p}{)}
%\PY{c+c1}{\PYZsh{} fit model}
%\PY{n}{model} \PY{o}{=} \PY{n}{VARMAX}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{order}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
%\PY{n}{model\PYZus{}fit} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{disp}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
%\PY{c+c1}{\PYZsh{} make prediction}
%\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}fit}\PY{o}{.}\PY{n}{forecast}\PY{p}{(}\PY{p}{)}
%\PY{n+nb}{print}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
%
%[[0.52387293 1.02383584]]
%\end{Verbatim}
%\end{tcolorbox}    


%Let's build an SARIMA model on `a10' -- the drug sales dataset.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} Import}
%\PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/a10.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} Plot}
%\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} Usual Differencing}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Usual Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Usual Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
%
%
%\PY{c+c1}{\PYZsh{} Seasinal Dei}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Original Series}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{diff}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Seasonal Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Seasonal Differencing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a10 \PYZhy{} Drug Sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%As you can clearly see, the seasonal spikes is intact after applying
%usual differencing (lag 1). Whereas, it is rectified after seasonal
%differencing.
%
%Let's build the SARIMA model using pmdarima`s auto\_arima(). To do that,
%you need to set seasonal=True, set the frequency m=12 for month wise
%series and enforce D=1.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} !pip3 install pyramid\PYZhy{}arima}
%\PY{k+kn}{import} \PY{n+nn}{pmdarima} \PY{k}{as} \PY{n+nn}{pm}
%
%\PY{c+c1}{\PYZsh{} Seasonal \PYZhy{} fit stepwise auto\PYZhy{}ARIMA}
%\PY{n}{smodel} \PY{o}{=} \PY{n}{pm}\PY{o}{.}\PY{n}{auto\PYZus{}arima}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{start\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{start\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
%                         \PY{n}{test}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
%                         \PY{n}{max\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{max\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,}
%                         \PY{n}{start\PYZus{}P}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{seasonal}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
%                         \PY{n}{d}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{D}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{trace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
%                         \PY{n}{error\PYZus{}action}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  
%                         \PY{n}{suppress\PYZus{}warnings}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
%                         \PY{n}{stepwise}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%
%\PY{n}{smodel}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%The model has estimated the AIC and the P values of the coefficients
%look significant. Let's look at the residual diagnostics plot.
%
%The best model SARIMAX(3, 0, 0)x(0, 1, 1, 12) has an AIC of 528.6 and
%the P Values are significant.
%
%Let's forecast for the next 24 months.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} Forecast}
%\PY{n}{n\PYZus{}periods} \PY{o}{=} \PY{l+m+mi}{24}
%\PY{n}{fitted}\PY{p}{,} \PY{n}{confint} \PY{o}{=} \PY{n}{smodel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{n\PYZus{}periods}\PY{o}{=}\PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{return\PYZus{}conf\PYZus{}int}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%\PY{n}{index\PYZus{}of\PYZus{}fc} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{date\PYZus{}range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{periods} \PY{o}{=} \PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{freq}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} make series for plotting purpose}
%\PY{n}{fitted\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fitted}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} Plot}
%\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fitted\PYZus{}series}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} 
%                 \PY{n}{lower\PYZus{}series}\PY{p}{,} 
%                 \PY{n}{upper\PYZus{}series}\PY{p}{,} 
%                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
%
%\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SARIMA \PYZhy{} Final Forecast of a10 \PYZhy{} Drug Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%How to build SARIMAX Model with exogenous variable The SARIMA model we
%built is good. I would stop here typically.
%
%But for the sake of completeness, let's try and force an external
%predictor, also called, `exogenous variable' into the model. This model
%is called the SARIMAX model.
%
%The only requirement to use an exogenous variable is you need to know
%the value of the variable during the forecast period as well.
%
%For the sake of demonstration, I am going to use the seasonal index from
%the classical seasonal decomposition on the latest 36 months of data.
%
%Why the seasonal index? Isn't SARIMA already modeling the seasonality,
%you ask?
%
%You are correct.
%
%But also, I want to see how the model looks if we force the recent
%seasonality pattern into the training and forecast.
%
%Secondly, this is a good variable for demo purpose. So you can use this
%as a template and plug in any of your variables into the code. The
%seasonal index is a good exogenous variable because it repeats every
%frequency cycle, 12 months in this case.
%
%So, you will always know what values the seasonal index will hold for
%the future forecasts.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} Import Data}
%\PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/selva86/datasets/master/a10.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{parse\PYZus{}dates}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%Let's compute the seasonal index so that it can be forced as a
%(exogenous) predictor to the SARIMAX model.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} Compute Seasonal Index}
%\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{tsa}\PY{n+nn}{.}\PY{n+nn}{seasonal} \PY{k}{import} \PY{n}{seasonal\PYZus{}decompose}
%\PY{k+kn}{from} \PY{n+nn}{dateutil}\PY{n+nn}{.}\PY{n+nn}{parser} \PY{k}{import} \PY{n}{parse}
%
%\PY{c+c1}{\PYZsh{} multiplicative seasonal component}
%\PY{n}{result\PYZus{}mul} \PY{o}{=} \PY{n}{seasonal\PYZus{}decompose}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{36}\PY{p}{:}\PY{p}{]}\PY{p}{,}   \PY{c+c1}{\PYZsh{} 3 years}
%                                \PY{n}{model}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{multiplicative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
%                                \PY{n}{extrapolate\PYZus{}trend}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%
%\PY{n}{seasonal\PYZus{}index} \PY{o}{=} \PY{n}{result\PYZus{}mul}\PY{o}{.}\PY{n}{seasonal}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{12}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{p}{)}
%\PY{n}{seasonal\PYZus{}index}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{seasonal\PYZus{}index}\PY{o}{.}\PY{n}{index}\PY{p}{)}\PY{o}{.}\PY{n}{month}
%
%\PY{c+c1}{\PYZsh{} merge with the base data}
%\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{o}{.}\PY{n}{month}
%\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{seasonal\PYZus{}index}\PY{p}{,} \PY{n}{how}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{on}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seasonal\PYZus{}index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
%\PY{n}{df}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{index}  \PY{c+c1}{\PYZsh{} reassign the index.}
%\end{Verbatim}
%\end{tcolorbox}
%
%The exogenous variable (seasonal index) is ready. Let's build the
%SARIMAX model.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k+kn}{import} \PY{n+nn}{pmdarima} \PY{k}{as} \PY{n+nn}{pm}
%
%\PY{c+c1}{\PYZsh{} SARIMAX Model}
%\PY{n}{sxmodel} \PY{o}{=} \PY{n}{pm}\PY{o}{.}\PY{n}{auto\PYZus{}arima}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{exogenous}\PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seasonal\PYZus{}index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,}
%                           \PY{n}{start\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{start\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
%                           \PY{n}{test}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
%                           \PY{n}{max\PYZus{}p}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{max\PYZus{}q}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,}
%                           \PY{n}{start\PYZus{}P}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{seasonal}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
%                           \PY{n}{d}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{D}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{trace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
%                           \PY{n}{error\PYZus{}action}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  
%                           \PY{n}{suppress\PYZus{}warnings}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
%                           \PY{n}{stepwise}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%
%\PY{n}{sxmodel}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
%So, we have the model with the exogenous term. But the coefficient is
%very small for x1, so the contribution from that variable will be
%negligible. Let's forecast it anyway.
%
%We have effectively forced the latest seasonal effect of the latest 3
%years into the model instead of the entire history.
%
%Alright let's forecast into the next 24 months. For this, you need the
%value of the seasonal index for the next 24 months.
%
%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{c+c1}{\PYZsh{} Forecast}
%\PY{n}{n\PYZus{}periods} \PY{o}{=} \PY{l+m+mi}{24}
%\PY{n}{fitted}\PY{p}{,} \PY{n}{confint} \PY{o}{=} \PY{n}{sxmodel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{n\PYZus{}periods}\PY{o}{=}\PY{n}{n\PYZus{}periods}\PY{p}{,} 
%                                  \PY{n}{exogenous}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{tile}\PY{p}{(}\PY{n}{seasonal\PYZus{}index}\PY{o}{.}\PY{n}{value}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} 
%                                  \PY{n}{return\PYZus{}conf\PYZus{}int}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
%
%\PY{n}{index\PYZus{}of\PYZus{}fc} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{date\PYZus{}range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{periods} \PY{o}{=} \PY{n}{n\PYZus{}periods}\PY{p}{,} \PY{n}{freq}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} make series for plotting purpose}
%\PY{n}{fitted\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{fitted}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%\PY{n}{lower\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%\PY{n}{upper\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{confint}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{index\PYZus{}of\PYZus{}fc}\PY{p}{)}
%
%\PY{c+c1}{\PYZsh{} Plot}
%\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fitted\PYZus{}series}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{lower\PYZus{}series}\PY{o}{.}\PY{n}{index}\PY{p}{,} 
%                 \PY{n}{lower\PYZus{}series}\PY{p}{,} 
%                 \PY{n}{upper\PYZus{}series}\PY{p}{,} 
%                 \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{15}\PY{p}{)}
%
%\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SARIMAX Forecast of a10 \PYZhy{} Drug Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
%
% 
%Practice Exercises In the AirPassengers dataset, go back 12 months in
%time and build the SARIMA forecast for the next 12 months.
%
%Is the series stationary? If not what sort of differencing is required?
%What is the order of your best model? What is the AIC of your model?
%What is the MAPE achieved in OOT cross-validation? What is the order of
%the best model predicted by auto\_arima() method?
%
%%\subsubsection{Persistence Model}\label{persistence-model}
%%
%%Let's say that we want to develop a model to predict the last 7 days of
%%minimum temperatures in the dataset given all prior observations.
%%
%%The simplest model that we could use to make predictions would be to
%%persist the last observation. We can call this a persistence model and
%%it provides a baseline of performance for the problem that we can use
%%for comparison with an autoregression model.
%%
%%We can develop a test harness for the problem by splitting the
%%observations into training and test sets, with only the last 7
%%observations in the dataset assigned to the test set as ``unseen'' data
%%that we wish to predict.
%%
%%The predictions are made using a walk-forward validation model so that
%%we can persist the most recent observations for the next day. This means
%%that we are not making a 7-day forecast, but 7 1-day forecasts.
%%
%%\begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%%\begin{Verbatim}[commandchars=\\\{\}]
%%\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{read\PYZus{}csv}
%%\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{DataFrame}
%%\PY{k+kn}{from} \PY{n+nn}{pandas} \PY{k}{import} \PY{n}{concat}
%%\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot}
%%\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
%%\PY{n}{series} \PY{o}{=} \PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{daily\PYZhy{}min\PYZhy{}temperatures.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
%%\PY{c+c1}{\PYZsh{} create lagged dataset}
%%\PY{n}{values} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{series}\PY{o}{.}\PY{n}{values}\PY{p}{)}
%%\PY{n}{dataframe} \PY{o}{=} \PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{values}\PY{o}{.}\PY{n}{shift}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{values}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
%%\PY{n}{dataframe}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t\PYZhy{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t+1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
%%\PY{c+c1}{\PYZsh{} split into train and test sets}
%%\PY{n}{X} \PY{o}{=} \PY{n}{dataframe}\PY{o}{.}\PY{n}{values}
%%\PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{7}\PY{p}{:}\PY{p}{]}
%%\PY{n}{train\PYZus{}X}\PY{p}{,} \PY{n}{train\PYZus{}y} \PY{o}{=} \PY{n}{train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{train}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
%%\PY{n}{test\PYZus{}X}\PY{p}{,} \PY{n}{test\PYZus{}y} \PY{o}{=} \PY{n}{test}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
%%
%%\PY{c+c1}{\PYZsh{} persistence model}
%%\PY{k}{def} \PY{n+nf}{model\PYZus{}persistence}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
%%	\PY{k}{return} \PY{n}{x}
%%
%%\PY{c+c1}{\PYZsh{} walk\PYZhy{}forward validation}
%%\PY{n}{predictions} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
%%\PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{test\PYZus{}X}\PY{p}{:}
%%	\PY{n}{yhat} \PY{o}{=} \PY{n}{model\PYZus{}persistence}\PY{p}{(}\PY{n}{x}\PY{p}{)}
%%	\PY{n}{predictions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{yhat}\PY{p}{)}
%%\PY{n}{test\PYZus{}score} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
%%\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test MSE: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}score}\PY{p}{)}
%%\PY{c+c1}{\PYZsh{} plot predictions vs expected}
%%\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}y}\PY{p}{)}
%%\PY{n}{pyplot}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
%%\PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
%%
%%Test MSE: 3.423
%%\end{Verbatim}
%%
%%\begin{center}
%%\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{time_series_files/time_series_106_1.png}
%%\end{center}
%%    
%%Running the example prints the mean squared error (MSE). The value
%%provides a baseline performance for the problem. The expected values for
%%the next 7 days are plotted (blue) compared to the predictions from the
%%model (red).
