\chapter{Machine Learning}\label{ex-lesson-15}

\begin{Exercise}
In order to see how different parameter choices affect the training
repeat the exercise of the function approximation but

\begin{itemize}
\tightlist
\item
  change the number of points used in the training (move the step
  to 0.01 and to 0.0001 in \(\tt{np.arange(-2, 2, 0.001)}\);
\item
  change the number of nodes per layer from 15-5 to 5-2 and 30-15.
\end{itemize}
\end{Exercise}

\begin{Answer}
In this first test we are going to reduce the number of sample in the training test by changing the step to define the $x$ values to 0.01.

\begin{codebox}[size=fbox, boxrule=1pt, colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{]}\PY{p}{)}
	
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{o}{+}\PY{l+m+mi}{2} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{]}\PY{p}{)}
	
\PY{n}{trainer} \PY{o}{=} \PY{n}{FinNN}\PY{p}{(}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{setData}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{addInputLayer}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{neurons}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{addHiddenLayer}\PY{p}{(}\PY{n}{neurons}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{addOutputLayer}\PY{p}{(}\PY{n}{outputs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{compileModel}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{opt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{codebox}

\begin{codebox}[size=fbox, boxrule=1pt, colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/1000
320/320 [==============================] - 0s 298us/step - loss: 0.2785
Epoch 2/1000
320/320 [==============================] - 0s 33us/step - loss: 0.2782
...
Epoch 999/1000
320/320 [==============================] - 0s 40us/step - loss: 3.6015e-04
Epoch 1000/1000
320/320 [==============================] - 0s 41us/step - loss: 3.5167e-04

320/320 [==============================] - 0s 80us/step
Training: 0.0003514565250952728
80/80 [==============================] - 0s 94us/step
Test: 0.0005268571898341179
\end{Verbatim}
\end{codebox}

The accuracy is not as quite as good as in the original example when the step
was 0.001.

So now let's try to reduce the step to 0.0001.
\begin{codebox}[size=fbox, boxrule=1pt, colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/1000
32000/32000 [==============================] - 0s 14us/step - loss: 0.0545
Epoch 2/1000
32000/32000 [==============================] - 0s 12us/step - loss: 0.0053
...
Epoch 999/1000
32000/32000 [==============================] - 0s 10us/step - loss: 4.8033e-07
Epoch 1000/1000
32000/32000 [==============================] - 0s 10us/step - loss: 4.5722e-07

32000/32000 [==============================] - 0s 14us/step
Training: 6.162963977018876e-08
8000/8000 [==============================] - 0s 15us/step
Test: 6.096156344881365e-08
None
\end{Verbatim}
\end{codebox}

In this case first of all the training is much longer, results are much
better though since the neural network can learn from more points of the
target function leading to a better approximation (old MSE was 2.5e-6).

Now let's try to change the number of neurons in each hidden layer to 5 and 2.
\begin{codebox}[size=fbox, boxrule=1pt,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{)}\PY{p}{]}\PY{p}{)}
	
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{o}{+}\PY{l+m+mi}{2} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{x}\PY{p}{]}\PY{p}{)}
	
\PY{n}{trainer} \PY{o}{=} \PY{n}{FinNN}\PY{p}{(}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{setData}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{addInputLayer}\PY{p}{(}\PY{n}{inputs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{neurons}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{addHiddenLayer}\PY{p}{(}\PY{n}{neurons}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{addOutputLayer}\PY{p}{(}\PY{n}{outputs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{trainer}\PY{o}{.}\PY{n}{compileModel}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{opt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
	
\PY{n}{trainer}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{codebox}

\begin{codebox}[size=fbox, boxrule=1pt,colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/1000
3200/3200 [==============================] - 0s 40us/step - loss: 0.0158
Epoch 2/1000
3200/3200 [==============================] - 0s 13us/step - loss: 0.0136
...
Epoch 999/1000
3200/3200 [==============================] - 0s 21us/step - loss: 5.2428e-06
Epoch 1000/1000
3200/3200 [==============================] - 0s 20us/step - loss: 4.7666e-06

3200/3200 [==============================] - 0s 40us/step
Training: 4.866414481057291e-06
800/800 [==============================] - 0s 46us/step
Test: 4.6103044132905776e-06
\end{Verbatim}
\end{codebox}

In this case there is a slight degradation of performance.
Changing the number of neurons to 30-15 instead...

\begin{codebox}[size=fbox, boxrule=1pt, colback=cellbackground, colframe=cellborder]
\begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/1000
3200/3200 [==============================] - 0s 39us/step - loss: 0.0477
Epoch 2/1000
3200/3200 [==============================] - 0s 12us/step - loss: 0.0141
...
Epoch 999/1000
3200/3200 [==============================] - 0s 13us/step - loss: 5.2197e-06
Epoch 1000/1000
3200/3200 [==============================] - 0s 16us/step - loss: 2.2086e-06

3200/3200 [==============================] - 0s 19us/step
Training: 1.6095951883698946e-06
800/800 [==============================] - 0s 16us/step
Test: 1.4640712151958723e-06
\end{Verbatim}
\end{codebox}

In this case there is some improvement. In general more complex NN are
capable of catching more features from the training sample. The
developer has to choose the needed trade off between complexity and gain
in accuracy.
\end{Answer}

\begin{Exercise}
Check how the CNN used to classify handwritten digits perform with your own calligraphy.

\begin{itemize}
\tightlist
\item
  Open \texttt{paint} (or any other drawing program) and create a 280x280 white square;
\item
  change brush type and set it the maximum size (larger sign of the "pencil");
\item
  with the mouse draw a digit;
\item
  finally save the file (e.g.~five.png).
\end{itemize}

Before passing the image to the NN it has to be resized and this is done
with an ad-hoc function (\texttt{transform\_image}) which is in the
\href{https://drive.google.com/file/d/1FMYvOJDDOdIv7kDb2VIGhAkNNmReiOb_/view?usp=sharing}{\texttt{digit\_converter.py}} module.
\end{Exercise}
\begin{Answer}
See the lecture notes.
\end{Answer}

\begin{Exercise}
Taking as example the pricing NN trained on call, try to price put
options.
\end{Exercise}
\begin{Answer}
\end{Answer}
