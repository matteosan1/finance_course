\chapter{The Theory of Machine Learning}
\label{ch:neural-network}

In this Chapter we will see how Machine Learning (ML) techniques can be successfully applied to solve financial problems. We first do a quick tour on the theory behind neural networks and then see few examples and practical applications regarding regression and classification issues.

Beware that this Chapter just scratches the surface of machine learning theory which has seen a huge development in the latest years leading to thousands of applications in many different fields.

\section{Artificial Intelligence, Machine and Deep Learning}


\emph{Artificial Intelligence} (AI) is a science devoted to making machines think and act like humans. 

This may sound simple, but no existing computer begins to match the complexities of human intelligence. Computers excel at applying rules and executing tasks, but sometimes a relatively straightforward ‘action’ for a person might be extremely complex for a computer.

For example, carrying a tray of drinks through a crowded bar and serving them to the correct customer is something servers do every day, but it is a complex exercise in decision making and based on a high volume of data being transmitted between neurons in the human brain.

Computers aren’t there yet, but machine learning and deep learning are steps towards a key element of this goal: analyzing large volumes of data and making decisions/predictions based on it with as little human intervention as possible.

\emph{Machine Learning} is a subset of artificial intelligence focusing on a specific goal: setting computers up to be able to perform tasks without the need for explicit programming.

Computers are fed structured data (in most cases) and "learn" to become better at evaluating and acting on that data over time. 

Think of structured data as data inputs you can put in columns and rows. You might create a category column in Excel called "food", and have row entries such as "fruit" or "meat". This form of structured data is very easy for computers to work with, and the benefits are obvious. 

Once programmed, a computer can take in new data indefinitely, acting on it without the need for further human intervention. 

Over time, the computer may be able to recognize that "fruit" is a type of "food" even if you stop labeling your data. This \emph{self-reliance} is so fundamental to machine learning that the field breaks down into subsets based on how much ongoing human help is involved (e.g. supervised and unsupervised learning).
    
The computer ability to perform some complex tasks, like gathering data from an image or video, for example, still falls far short of what humans are capable of~\cite{bib:nn_over_human}.

\emph{Deep learning models} introduce an extremely sophisticated approach to machine learning and are set to tackle these challenges because they've been specifically modeled after the human brain. Complex, multi-layered \emph{neural networks} are built to allow data to be passed between nodes (like neurons) in highly connected ways. The result is a non-linear transformation of the data that is increasingly abstract.

While it takes tremendous volumes of data to feed and build such a system, it can begin to generate immediate results, and there is relatively little need for human intervention once the programs are in place.
    
\section{Neural Networks}\label{neural-networks}

Artificial Neural Networks (ANN or simply NN) are information processing models that are developed by inspiring from the working principles of the human brain. Their most essential property is \emph{the ability of learning from sample sets}.

\subsection{Neurons and Activation Functions}
The basic unit of an ANN is the neuron.
It consists of weights (\(w_i\)) and takes in input real numbers (\(x_i\)). After the injection into a neuron all these inputs are individually weighted, added together (sometimes it is added also a bias \(w_0\)) and passed into the activation function which produces the final neuron output

\[ \textrm{Inputs} = \sum_{i=1}^{N} x_i w_i + w_0 \rightarrow f(\textrm{Inputs}) = \textrm{Outputs}\]
Figure~\ref{fig:neuron} shows an example.

There are many different types of activation functions, examples are
\begin{itemize}
\item \emph{step function}: which returns just 0 or 1 according to the input value;
\item \emph{sigmoid}: which can be thought of as the continuous version of the step function, see Fig.~\ref{fig:act_func};
\item \emph{Rectified Linear Unit} (ReLU);
\item \emph{hyperbolic tangent} (tanh).
\end{itemize}

For a deeper discussion on activation functions see~\cite{bib:activation_function}.

\begin{figure}[htb]
\centering
\subfloat[Model of an artificial neuron.\label{fig:neuron}]{%
	\includegraphics[width=0.6\textwidth]{figures/neuron}
}
\subfloat[Examples of sigmoidal and step activation functions.\label{fig:act_func}]{%
	\includegraphics[height=0.30\textwidth]{figures/sigmoid}
}
\caption{Basic components of a neuron.}
\label{fig:sigmoid}
\end{figure}

\subsection{Training of a Neuron}
\label{training-of-a-neuron}

When teaching children how to recognize a bus, we just tell them, showing an example: "This is a bus. That is not a bus." until they learn the concept of what a bus is. Afterwards, if a child sees new objects that she hasn't seen before, we could expect her to recognize correctly whether the new object is a bus or not.

This is exactly the idea behind neuron training. Inputs from a \emph{training} set are presented to the neuron,  one after the other, together with the correct output. During the process neuron weights are modified so that its response best matches the true output.

When an entire pass through all of the inputs is completed (an \emph{epoch}) the neuron has learned. Usually to make it learn even better the same training data is processed multiple times.

After the training is completed, when an input vector \(\mathbf{x}\), contained in the training set, is presented to the neuron, it will output the correct value. If \(\mathbf{x}\) is not in the training set, the neuron will respond with an output close to those corresponding to other training vectors similar to \(\mathbf{x}\).

This kind of training is called \emph{supervised} because the input dataset is accompanied by the known targets (the true outputs), and we want our model to learn to predict the target from those input variables.

Unfortunately using just a neuron is not too useful since with such a simple architecture is not possible to solve the interesting problems we would like to face. 
The next step is to put together more neurons in \emph{layers}.

\subsection{Multilayered Neural Networks}
\label{multi-layered-neural-networks}

In a multilayered configuration each neuron from the \emph{input layer} is fed up to each node in the next \emph{hidden layer}. This kind of connection is repeated for each node down to the \emph{output layer}. 
We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer. The only two architectural constraints are on the neuron number in the input layer, that has to match the number of inputs, and on the nodes in the output layer which depends on the kind of target. Figure~\ref{fig:multilayered_nn} shows an example of such an architecture.

\begin{figure}[htb]
\centering
\includegraphics[width=0.6\textwidth]{figures/multilayer.jpeg}
\caption{A multilayered neural network.}
\label{fig:multilayered_nn}
\end{figure}

\subsection{Training a Multilayered Neural Network}
\label{training-a-multilayered-neural-network}

The training of a multilayered NN follows these steps:

\begin{itemize}
\tightlist
\item present an input sample to the neural network whose neurons are initialized with random weights;
\item compute the network output obtained by calculating the activation functions of each layer;
\item calculate the \emph{error (loss)} as the difference between NN predicted and target output;
\item re-adjust the network weights such that the error decreases;
\item continue the process for the entire sample (and also for several epochs), until the error is not changing too much (i.e. the process converged).
\end{itemize}
In Figure~\ref{fig:training} an example of training is shown.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{figures/training_nn}
\caption{Training example of a multilayered neural network.}
\label{fig:training}
\end{figure}

\subsubsection{Loss Function}
The NN error is computed by the \emph{loss function}. Different loss functions will give different error estimates for the same prediction, and thus they have a considerable effect on the performance of any model. Two are the main choices

\begin{itemize}
\tightlist
\item Mean Absolute Error (MAE): the average of the absolute value of the differences between the predictions and true values. It shows how far off we are on average from the correct value. 
\item Root Mean Squared Error (MSE): the square root of the average of the squared differences between the predictions and true values. It instead penalizes larger errors more heavily and is commonly used in regression tasks. 
\end{itemize}

Either metrics may be appropriate depending on the situation and you can use both for comparison. 
More information about loss function can be found in~\cite{bib:loss_function}.

\subsubsection{Back-propagation}
The loss is a function of the internal parameters of the model (i.e neuron weights). For an accurate predictions, one needs to minimize the calculated error and in a neural network, this is done using
\emph{back-propagation}~\cite{bib:backpropagation}.

In this algorithm the current error is "propagated" backwards to previous layers, where it is used to modify the weights in such a way that the loss tends to be minimized.
The weights are modified with a function called \emph{optimization function} (we will use \emph{Adam} in the following but there are more).

\subsection{Regression and Classification}
\label{regression-and-classification}

The two main categories of problems that can be solved with neural networks are \emph{classification} and \emph{regression}. Let's see their characteristics and differences.

\subsubsection{Classification}
\label{classification}

Classification is the process of finding a function which helps in dividing the input dataset into classes based on its characteristics. 
The goal is to find the mapping function between the input (\(x\)) and the \textbf{discrete} output (\(y\)) or in other words to find the decision boundary, which can divide the dataset into the different classes.

A typical classification problem is the \emph{email spam detection}. The model is trained on different parameters (e.g. subject, email text, recipients) using millions of emails, and whenever it receives a new email, it checks whether is spam or not. Classification algorithms can also be used in speech recognition, car plates identification, \ldots

\subsubsection{Regression}
\label{regression}

Regression is the process of finding hidden correlations between dependent variables. It helps in predicting market trends, house prices, \ldots

The goal is to find the mapping function between the input variables (\(x\)) and the \textbf{continuous} output variable (\(y\)), that is to find the best fit which can predict the output.

As an example suppose we want to do weather forecasting. The model is trained on past data (e.g. temperature, geographic location, pressure), and on the basis of today's inputs can predict the weather for future days. In general whenever we are dealing with function approximations this kind of algorithms can be applied.

\begin{attention}
\subsubsection{Technical Note}
\label{technical-note}

Neural network training and testing is performed using two modules: \texttt{keras}~\cite{bib:keras} (which in turn is based on a Google open source library called \texttt{tensorflow}~\cite{bib:tensorflow}) and \texttt{scikit-learn}~\cite{bib:scikit} which provide many useful utilities.
\end{attention}

\section{Function approximation}
\label{function-approximation}

Let's design an ANN which is capable of learning the functional form underlying a set of data. 

The training sample is generated as a two columns array: \(x\) (input), \(f(x)\) (target output) where \(f(x) = x^3 +2\). 
Unlike this simple case, it usually takes some time to generate the data sample. So it is always advisable to store it in a file since it may be needed many times during the NN development. 

\begin{ipython}
import numpy as np
X = np.array([(x, x**3+2) for x in np.arange(-2, 2.001, 0.001)])
print("Distribution of original data ", X[:, 0].min(), X[:, 0].max(), 
X[:, 1].min(), X[:, 1].max())
\end{ipython}
\begin{ioutput}
Distribution of original data  -2.0 1.999999999995595 -6.0 9.99999999994714
\end{ioutput}

Before embarking into the actual training the sample undergoes to a transformation in order to have all the inputs (and outputs) with the same scale. This sample transformation is called \emph{scaling}, and is done to provide the NN with "equalized" inputs since it could be fooled by very large (or very small) numbers giving unstable results.

There are various methods available for data scaling in \texttt{scikit-learn}, the most important being:
\begin{itemize}
\tightlist
\item \emph{StandardScaler}: scales features such that the distribution is centered around 0, with a standard deviation of 1. It is not quite good if data is not normally distributed (i.e. no Gaussian distribution);
\item \emph{MinMaxScaler}: shrinks the range such that it becomes between 0 and 1 (or -1 to 1 if there are negative values in the first place). It is influenced heavily by outliers (i.e. extreme values);
\item \emph{RobustScaler}: similar to the previous one but it instead uses quantile ranges, so that it is robust against outliers. It doesn't take the median of the original sample into account and only focuses on the regions where the bulk data is. Consider that \textbf{robust} doesn't mean immune or invulnerable and that the purpouse of scaling is not to remove outliers.
\end{itemize}

Which one to use depends on the particular case under study. 

Figure~\ref{fig:scalers} reports the different transformations on the distribution of the target outputs. In the actual example we are going to use the \texttt{MinMaxScaler}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{figures/scalers}
\caption{Effect of different kind of scalers in the target output distribution.}
\label{fig:scalers}
\end{figure}

\begin{ipython}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

scale = MinMaxScaler(feature_range=(-1,1))
X_scaled = scale.fit_transform(X)

print("The same data after the normalization ", X_scaled[:, 0].min()
                                              , X_scaled[:, 0].max()
                                              , X_scaled[:, 1].min()
                                              , X_scaled[:, 1].max())

X_train, X_test = train_test_split(X_scaled, test_size=0.2)
\end{ipython}
\begin{ioutput}
The same data after the normalization  -1.0 1.0 -1.0 1.0
\end{ioutput}

Notice that the sample has been split into two parts: training and testing. The reason will be explained in Sec.~\ref{sec:overtraining}.

\subsection{Neural Network Design}
\label{neural-network-design}

There is no rule to guide developers into the design of a neural network in terms of number of layers and neurons (the so called \emph{hyper-parameters}). 

One popular method for hyper-parameter optimization is the \emph{grid search}. 
A list of possible values for each hyper-parameter is defined, then the model is run multiple times, each time with a different combination of the hyper-parameter values. In the end the set giving the best result is taken. This is the most thorough way of selecting the neural network architecture but it is also the most computationally intense.
In the end a \emph{trial and error} strategy is what is implemented anyway, and the solution giving the best accuracy is selected although without an exhaustive check of all the possibilities.

In general a larger number of nodes is better to catch highly structured data with a lot of feature although it may require larger training sample to work correctly.

As a rule of thumb a NN with just one hidden layer with a number of neurons averaging the inputs and outputs is sufficient in most cases.

In the following we will use complex networks just for illustration, but no attempt in optimizing the layout has been done at all.

Here two layers with 15 and 5 neurons respectively and a \texttt{tanh} activation function are used, see Fig,~\ref{fig:ann_1}. The \texttt{inputs} parameter has to be set to 1 since we have just one single input, the \texttt{x} value.

\begin{ipython}
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(15, input_dim=1, activation='tanh'))
model.add(Dense(5, activation='tanh'))
model.add(Dense(1, activation='tanh'))

model.compile(loss='mse', optimizer='adam')
model.fit(X_train[:, 0], X_train[:, 1], epochs=2000, verbose=1)
\end{ipython}
\begin{ioutput}
Epoch 1/2000
100/100 [==============================] - 1s 2ms/step - loss: 0.0362
Epoch 2/2000
100/100 [==============================] - 0s 2ms/step - loss: 0.0293
...
Epoch 1999/2000
100/100 [==============================] - 0s 2ms/step - loss: 5.6730e-06
Epoch 2000/2000
100/100 [==============================] - 0s 3ms/step - loss: 6.1324e-06
<keras.callbacks.History at 0x7f9c22abd450>
\end{ioutput}

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{figures/ann_1.png}
\caption{Graphical representation of the ANN used to approximate the function $f(x) = x^3 + 2$.}
\label{fig:ann_1}
\end{figure}

The output shows a counter with the current epoch number and the computed loss which is evaluated using the method chosen in the code.

As for the input dataset it is convenient to save in files both the training model and the actual scalers used to transform data to use them for later predictions. Indeed the NN has to be fed with data as much "similar" as possible to that used during the training to produce sensible results.

\begin{ipython}
import joblib

job_name = 'func_approx'
model.save(job_name)
joblib.dump(scale_X, job_name + "_x_scaler.save")
joblib.dump(scale_Y, job_name + "_y_scaler.save")
\end{ipython}

\subsection{Model Prediction}

When the training is completed we can evaluate how good it is. \textbf{Usually performance is measured using the loss function value at the end of the training.}
A \emph{perfect} prediction would lead to \(\textrm{loss}=0\) so the lower this number the better the training. 

One first thing that can be notice is that model performance improves with the number of epochs in the training. Indeed the NN has more chances to learn the input features. In Fig.~\ref{fig:training_vs_epochs} are shown the example $f(x)$ we wanted to approximate and different predictions obtained in trainings with the same NN architecture but with various numbers of epochs (i.e. 5, 100, 800, 5000).

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{figures/training_vs_epoch}
\caption{Prediction of the target $f(x)$ using different numbers of epochs in the training (5, 100, 800 and 5000) respectively.}
\label{fig:training_vs_epochs}
\end{figure}

\subsection{Overtraining}
\label{sec:overtraining}
A common issue to avoid during the training is \emph{overtraining} or \emph{overfitting} a neural network. 

This happens when a model is "trained too well" on the dataset, which means that it mostly memorized data, and its performance accounts for a large accuracy within the training set but poor performance in similar but independent datasets. 

To check for overfitting it is required to split the available sample in at least two parts: training and testing (e.g.~80\% and 20\%) and to use the former to perform the training and the latter to cross-check the performance.

%Actually you should split the sample in three parts: training, testing and validation. What you would do is take the model with the highest validation accuracy and then test it with the testing set.
%This makes sure that you don’t overfit the model. Using the validation set to choose the best model is a form of data leakage (or “cheating”) to get to pick the result that produced the best test score out of hundreds of them. Data leakage happens when information outside the training data set is used in the model.
%In this case, our testing and validation set are the same, since we have a smaller sample size.

Calling twice the \texttt{evaluate()} method the losses on the training and testing samples can be compared. If the two numbers are comparable the neural network is ok, otherwise if the loss on the training set is much smaller than the testing we had overfitting.
In our example since the two numbers are in good agreement we can be confident that there hasn't been overtraining.

\begin{ipython}
eval1 = model.evaluate(X_train[:, 0], X_train[:, 1])
print('Training: {}'.format(eval1))

eval2 = model.evaluate(X_test[:, 0], X_test[:, 1])
print('Test: {}'.format(eval2))
\end{ipython}
\begin{ioutput}
100/100 [==============================] - 0s 1ms/step - loss: 5.2089e-06
Training: 5.208917627896881e-06
26/26 [==============================] - 0s 2ms/step - loss: 5.2780e-06
Test: 5.277975560602499e-06
\end{ioutput}

In case one would need more accuracy and had already incremented too much the number of epochs could either increase the training sample size or change the NN architecture.

\subsection{Why Am I Getting Different Results ?}

You shouldn't be surprised if you are trying to reproduce the above and you are getting different numbers. This is to be expected and might even be a feature of the algorithm, not a bug.

%In applied machine learning, we run a machine learning “algorithm” on a dataset to get a machine learning “model.” The model can then be evaluated on data.
%We cannot write code to predict outputs given inputs because it is too hard, so we use machine learning algorithms to learn how to predict outputs from inputs given historical examples.
%Unlike the programming that we may be used to, the programs may not be entirely deterministic.

The resulting machine learning models may be different each time they are trained. In turn, the models may make different predictions, and when evaluated, may have a different level of error or accuracy. Let's see the possible causes.

\begin{itemize}
\item \textbf{Differences in training data}: it is highly probable that you will get different results when you run the same algorithm on different inputs;

\item \textbf{stochastic learning algorithm}: you can get different results when you run the same algorithm on the same data due to the nature of the learning algorithm. 
While some ML algorithms are deterministic, that is when the algorithm is given the same dataset, it learns the same model every time (e.g. logistic regression algorithm) others are not. This means that their behavior incorporates elements of randomness.
Beware that \emph{stochastic} machine learning algorithms are not learning a random model although its specific small decisions made during the learning process can vary randomly.
The impact is that each time it is run on the same data, it learns a slightly different model. In turn, the model may make slightly different predictions, and when evaluated, may have a slightly different performance.
\emph{Neural networks} are a stochastic algorithm. Their randomness comes from: the random initial weights, which allow the model to try learning from a different starting point in the search space each time, the random shuffle of examples during training, which ensures that each gradient estimate and weight update is slightly different.
Nevertheless the learning algorithms randomness can be controlled, for example by setting the seed used by the pseudorandom number generator to ensure that each time the algorithm is run, it gets the same.
This is not a good approach in practice (apart from tutorials or lectures). Since there is no best seed for any algorithm you need to summarize the performance by fitting multiple times a model on your dataset and averaging its predictions;

\item \textbf{evaluation procedure}: the sample splitting in training and testing is random so you can get different results (evaluations) when running the same algorithm with the same data;

\item \textbf{differences caused by platform}: even if you fix the random number seed to address the stochastic nature of the learning algorithm you can get different results when running the same algorithm on the same data on different computers.
The cause in this case is the platform or development environment used to run for example:
\begin{itemize}
\tightlist
\item system architecture, e.g. CPU or GPU;
\item operating system, e.g. MacOS or Linux;
\item underlying math libraries, e.g. LAPACK or BLAS;
\item Python of library version, e.g. 3.6 or 3.7, scikit-learn 0.22 or 0.23.
\end{itemize}
Honestly, the effect is usually very small in practice (at least in my experience) as long as major software versions are a good or close enough match.
\end{itemize}

\section{Image Recognition Neural Network}
\label{neural-net-to-recognize-handwritten-digits}

The difficulties of visual pattern recognition becomes immediately apparent if you attempt to write a computer program to recognize digits like those in Fig.~\ref{fig:mnist}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.4\textwidth]{figures/mnist_100_digits}
\caption{MNIST sample of handwritten digits.}
\label{fig:mnist}
\end{figure}

Simple intuition about how we discern shapes (e.g. a 9 has a loop at the top, and a vertical stroke in the bottom right) turns out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions, caveats, and special cases so that it seems hopeless.

Neural networks approach the problem in a different way. They take a large number of handwritten digits and develop a system which can learn from those.

By increasing the number of training examples, the network can learn more and more about handwriting, and so improve its accuracy. A better handwriting recognizer could be certainly built by using thousands or millions training examples (as we will see in the next Chapter \textbf{neural networks are not capable of extrapolating results}, hence in general it won't recognize a digit written in some strange way not included in the training sample).

Here the digit sample provided with \texttt{mnist} module and partially shown in Fig.~\ref{fig:mnist} will be used. 
The algorithm is based on a different kind of neural network, specifically designed for image and pattern recognition, the Convolutional Neural Network (CNN). It works by using different kind of layers (\emph{convolutional layers}) which apply various filters on top of the image. Each filter works as edge detectors and add useful information to classify the image according to its features.

Stacking convolutional layers prove to be very effective, and allows to learn both low (e.g. lines) and high level features (e.g. complex shapes or even specific objects), see an example in Fig.~\ref{fig:conv_filters}.

\begin{figure}[!hbt]
\centering
\includegraphics[width=0.75\textwidth]{figures/edges.jpg}
\caption{Edge detected by different layers of a convolutional neural network.}
\label{fig:conv_filters}
\end{figure}

This is the first example of \emph{classification}, indeed the CNN output won't be a single number but rather a list containing the probabilities that an image belongs to each class. Next we define the CNN architecture, see Fig.~\ref{fig:cnn2d}.

\begin{ipython}
import numpy as np, mnist
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.utils import to_categorical

train_images = mnist.train_images() 
train_labels = mnist.train_labels() 

model = Sequential()
model.add(Conv2D(8, kernel_size=3, input_shape=(28, 28, 1)))
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(10, activation="softmax"))

model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(train_images, to_categorical(train_labels), epochs=5, verbose=1)
\end{ipython}
\begin{ioutput}
Epoch 1/5
1875/1875 [==============================] - 19s 10ms/step - loss: 2.0868
Epoch 2/5
1875/1875 [==============================] - 18s 10ms/step - loss: 0.3320
...
Epoch 4/5
1875/1875 [==============================] - 18s 10ms/step - loss: 0.1891
Epoch 5/5
1875/1875 [==============================] - 18s 10ms/step - loss: 0.1799
<keras.callbacks.History at 0x7f07b05d0350>
\end{ioutput}

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{figures/cnn_2d}
\caption{Graphical representation of the CNN developed to recognize handwritten digits.}
\label{fig:cnn2d}
\end{figure}

\begin{attention}
\subsubsection{Anatomy of the CNN}

The code is quite different than the previous example, let's highlight the major differences.
In input we are providing $28\times 28$ matrices whose values represents the color of each image pixel. 
The \texttt{Conv2D} layer performs the \emph{convolution} between a set of eight (in our case) filters with the image. 

In the context of a CNN, a convolution is an operation involving the multiplication of a set of weights with the input, much like a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between the input matrix data and a 2D array of weights, called filter or kernel ($3\times 3$ in the example). Since usually kernels are smaller than the original image the result is a \emph{feature map} (i.e. a smaller trasformed version of the input image).
\newline

\begin{center}
\includegraphics[width=0.7\textwidth]{figures/convolution}
\captionof{figure}{Convolution application to an image.}
\end{center}

A limitation of such maps is that they record the precise position of features in the input. This means that small movements in its position will result in a different feature map. 

Imagine a program that looks for car plates in pictures taken by a speed radar: cars won't be in the same position in the frame so there may be differences in the classification of similar (but not equal) pictures.

A common approach to address this problem is called \emph{down sampling}, which consists in reducing the resolution of the image. In such a way it still contains the large or important structural elements, without the finer detail that may not be as useful to the task.

Down sampling can be achieved using a \emph{pooling layer}. This layer typically works on $2\times 2$ pixels and can performs two kind of operations:
\begin{itemize}
\tightlist
\item average pooling: calculate the average value for each $2\times 2$ patch on the feature map;
\item maximum pooling: calculate the maximum value for each patch.
\end{itemize}

Finally there is the \texttt{Flatten} layer which merges the stack of feature maps created by the convolutional layer.

Other differences are the activation function \emph{softmax} which is a particular kind of sigmoid and the loss function, \emph{crossentropy} which is particularly suited for classification problems.
\end{attention}

\subsection{CNN Performance}
Let's see how well our NN predicts \texttt{mnist} testing digits.

\begin{ipython}
test_images = mnist.test_images()
test_labels = mnist.test_labels()

predictions = model.predict(test_images[:10])

print ("Tesing on MNIST digits...")
print("Predicted: ", np.argmax(predictions, axis=1)) 
print("Truth:     ", test_labels[:10])
\end{ipython}
\begin{ioutput}
Tesing on MNIST digits...
Predicted:  [7 2 1 0 4 1 4 4 6 9]
Truth: [7 2 1 0 4 1 4 9 5 9]
\end{ioutput}

Since the last but one digit has lower probability let's check the returned list to see which other categories have non-zero probability.

\begin{ipython}
for i, p in enumerate(predictions[-2])])
    print("9th digit:", ["dig {}: {:.3f}".format(i, p)
\end{ipython}
\begin{ioutput}
9th digit: ['dig 0: 0.0000', 'dig 1: 0.0000', 'dig 2: 0.0000', 'dig 3: 0.0000', 
            'dig 4: 0.0000', 'dig 5: 0.0001', 'dig 6: 0.9999', 'dig 7: 0.0000', 
            'dig 8: 0.0000', 'dig 9: 0.0000']
\end{ioutput}

%So the second ranked digit is a 6 (which can be confused with a five if the lower loop is almost closed).

\subsection{Further Tests}
The NN performance can be checked using your own calligraphy. 

Open your favourite image editor (e.g. Paint, GIMP,\ldots) and create a black-white 200x200 pixels canvas. With the help of the mouse draw a digit and save it as a png file.
Before passing the image to the NN it has to be resized and this is done with an ad-hoc function, \texttt{transform\_image}, which can be imported from \href{https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/digit_converter.py}{\texttt{digit\_converter.py}}.

\begin{ipython}
from digit_converter import transform_image

filenames = ['four.png', 'five.png']
for f in filenames:
    test_images = np.array(transform_image(f))
test_images = np.expand_dims(test_images, axis=3)
predict = model.predict(test_images)

print ("Tesing on custom digits...")
print ("Predicted: ", np.argmax(predict, axis=1))
print("%:", ["{:.3f}".format(p[np.argmax(p)]) for p in predict])
print(["{:.2f}".format(p) for p in predict[0]])
\end{ipython}
\begin{ioutput}
Tesing on custom digits...
Predicted:  [4]
%: ['0.802']
['0.00', '0.00', '0.00', '0.00', '0.80', '0.00', '0.00', '0.20', '0.00', 
'0.00']

Testing on custom digits...
Predicted:  [5]
%: ['0.981']
['0.00', '0.00', '0.00', '0.01', '0.00', '0.98', '0.00', '0.01', '0.00', 
'0.00']
\end{ioutput}
The handwritten images used in this test are shown in Fig.~\ref{fig:test_images}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.2\textwidth]{figures/four.png}
\includegraphics[width=0.2\textwidth]{figures/five.png}
\caption{Test handwritten digits used in the example.}
\label{fig:test_images}
\end{figure}

\section{Remarks on Artificial Intelligence}

A neural network is a \emph{black box} in the sense that while it can approximate any function, studying its structure won't give you any insights on the functional form of the function being approximated.

As an example, one common use of neural networks on the banking business is to classify loaners on "good" and "bad" payers. You have a matrix of input characteristics $C$ (sex, age, income, \ldots) and a vector of results $R$ (defaulted, not defaulted, \ldots). When you model this using a neural network, you are supposing that there is a function $f(C)=R$. This function $f$ can be arbitrarily complex, and might change according to the evolution of the business, so you can't derive it by hand.

Then you use the neural network to build an approximation of $f$ that has an error rate that is acceptable to your application. This works, and the precision can be arbitrarily small, you can expand the network, fine tune its training parameters and get more data until the precision hits your goals.

The black box issue is: the approximation given by the neural network will not give you any insight on the form of $f$. There is no simple link between the weights and the function being approximated. 

Plus, from a traditional statistics viewpoint, a neural network is a non-identifiable model: given a dataset and network architecture, there can be two neural networks with different weights but exactly the same result. This makes the analysis very hard.
Needless to say, debugging NN results is even harder given this lack of interpretability. So use with caution machine learning models since they can be quite misleading.
 
\begin{thebibliography}{9}
\bibitem{bib:deep_learning} I. Goodfellow, Y. Bengio and A. Courville, \href{http://www.deeplearningbook.org}{\emph{Deep Learning}}, MIT Press, 2016
\bibitem{bib:nn_over_human} K. Grace, J. Salvatier, A. Dafoe, B. Zhang and O. Evans, \emph{When Will AI Exceed Human Performance? Evidence from AI Experts}, arXiv:1705.08807, 2005
\bibitem{bib:activation_function} J. Lederer, \emph{Activation Functions in Artificial Neural Networks: A Systematic Overview}, arxiv: 2101.09957, 2001
\bibitem{bib:backpropagation}\href{https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd}{\emph{Understanding back-propagation algorithm}}, Towards Data Science [Online]
\bibitem{bib:loss_function}\href{https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d}{\emph{Mae and rmse which metric is better ?}}, Medium [Online]
\bibitem{bib:keras}\href{https://keras.io/}{\emph{Keras Documentation}}, [Online]  
\bibitem{bib:tensorflow}\href{https://www.tensorflow.org/}{\emph{Tensorflow Documentation}}, [Online] 
\bibitem{bib:scikit}\href{https://scikit-learn.org/stable/}{\emph{scikit-learn Documentation}}, [Online]
\end{thebibliography}