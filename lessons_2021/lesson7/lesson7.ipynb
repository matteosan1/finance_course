{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lesson7.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPe4TKCtSd9o+htkZBuzG0g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyN38MPZW54F","executionInfo":{"status":"ok","timestamp":1637052095200,"user_tz":-60,"elapsed":22529,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"518079da-b8c0-4587-9c23-cf0f571ea178"},"source":["import sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","sys.path.append('/content/drive/MyDrive/finance_course/2021/lesson6')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"FF2PpJk99CKL"},"source":["## Value at Risk\n","\n","* The Value at Risk (VaR) of a portfolio is used to know to a certain confidence level how much will be the maximum loss in the next $N$ days. \n","  * It is a function of two parameters: the time horizon (i.e. N days) and the confidence level. \n","  * It can be interpreted as the loss level over a certain time horizon that has a probability of only (100 − X)% of being exceeded.\n","  * Usually N is 1 day and X is 95.\n","* The VaR is the **loss corresponding to the (100 − X)th percentile of the portfolio change in value distribution over the next N days**.\n","\n","![](https://drive.google.com/uc?id=1TNNlBn_Xl-F5BLYxJR7uEDSdzouGOafU)\n","\n","* VaR is useful to summarize all the information about the risk of a portfolio in one single number\n","  * this can also be considered its main limitation as it implies too much simplification of such a complex task.\n","\n","## How to Estimate VaR\n","* In the following historical series of Apple and Netflix are used ([historical_data.csv](https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/historical_data.csv)):\n","  * it has been assumed a portfolio made of 60% of AAPL and 40% NFLX."]},{"cell_type":"code","metadata":{"id":"U-9Qu8q-UF8z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647506075462,"user_tz":-60,"elapsed":356,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"77dd60fe-92a4-4272-b570-6c23c5c931fb"},"source":["# read historical series and define weights\n","import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv(\"https://raw.githubusercontent.com/matteosan1/finance_course/develop/libro/input_files/historical_data.csv\")\n","print (df.head())\n","\n","w = np.array([0.6, 0.4])"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["         Date       aapl       nflx\n","0  2014-01-02  17.598297  51.831429\n","1  2014-01-03  17.211735  51.871429\n","2  2014-01-06  17.305593  51.367142\n","3  2014-01-07  17.181829  48.500000\n","4  2014-01-08  17.290642  48.712856\n"]}]},{"cell_type":"markdown","metadata":{"id":"EZV69j7UAVUA"},"source":["* In the following we are going to add a new column with the daily return to the dataframe.\n","  * **Arithmetic return**:\n","$$r_{arithm} = \\frac{FV}{PV} - 1 = \\frac{FV - PV}{PV}\\quad(\\texttt{.pct_change()})$$\n","    * not symmetric: if a position appreciates 15% and then depreciates 15%, the total change is -2.25%.\n","$$FV = PV(1 + r_{arithm})\\\\[5pt]$$\n","$$FV = PV(1+.15)(1 - .15) = PV(0.9775)\\\\[5pt]$$\n","$$\\frac{FV}{PV} - 1 = 0.9775 - 1 = -2.25 \\%$$\n","  * **Logaritmic return**:\n","$$r_{log} = \\log\\left(\\frac{FV}{PV}\\right)$$\n","    * symmetric:\n","$$FV = PV e^{r_{log}}\\\\[5pt]$$\n","$$FV = PV e^{0.1398} e^{-0.1398} = PV\\\\[10pt]$$\n","\n","$$r_{arithm} + 1 = \\frac{FV}{PV} \\implies \\log(r_{arithm} + 1) = \\log\\left(\\frac{FV}{PV}\\right) \\implies r_{log} = \\log(1 + r_{arithm})$$ \n","\n","* For small variations $r_{arithm}$ and $r_{log}$ are very similar."]},{"cell_type":"code","metadata":{"id":"TmWvpV97UslI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647506100783,"user_tz":-60,"elapsed":456,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"009cf6cf-68c0-42d3-fb6d-b4814bdd1609"},"source":["# add daily log-returns\n","df['aapl_rets'] = np.log1p(df['aapl'].pct_change())\n","df['nflx_rets'] = np.log1p(df['nflx'].pct_change())\n","print (df.head())"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["         Date       aapl       nflx  aapl_rets  nflx_rets\n","0  2014-01-02  17.598297  51.831429        NaN        NaN\n","1  2014-01-03  17.211735  51.871429  -0.022211   0.000771\n","2  2014-01-06  17.305593  51.367142   0.005438  -0.009769\n","3  2014-01-07  17.181829  48.500000  -0.007177  -0.057435\n","4  2014-01-08  17.290642  48.712856   0.006313   0.004379\n"]}]},{"cell_type":"markdown","metadata":{"id":"DuJWHI5vUHL0"},"source":["* Portfolio value $\\Pi$ is determined by the *scalar product* between the invested amount (the weights $w_i$) and the asset values ($v_i$), i.e. $\\Pi = \\sum_i w_i \\cdot v_i$. \n","  * In $\\texttt{python}$, when using $\\texttt{numpy.array}$ for weights and values, it is indicated with the method $\\texttt{.dot}$. \n","  * For some practical calculations it may be useful to multiply two vectors compents by components without summing up at the end all the results and this is done with the operator $\\texttt{*}$.\n","\n","$$\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4 \\\\\n","\\end{bmatrix}*[10, 20] = \n","\\begin{bmatrix}\n","1*10 & 2*20 \\\\\n","3*10 & 4*20 \\\\\n","\\end{bmatrix} = \n","\\begin{bmatrix}\n","10 & 40 \\\\\n","30 & 80 \\\\\n","\\end{bmatrix}\\\\[10pt]\n","$$\n","\n","$$[10, 20]\\cdot[5, 6] = 10*5 + 20*6 = 170$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hlTL9YNUM-f","executionInfo":{"status":"ok","timestamp":1637006600602,"user_tz":-60,"elapsed":10,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"a1fda317-b58e-485b-fc98-d9bb33aa3458"},"source":["import numpy as np\n","a = np.array([[1, 2], [3, 4]])\n","b = np.array([10, 20])\n","c = np.array([5, 6])\n","\n","print (a*b)\n","print (b.dot(c))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[10 40]\n"," [30 80]]\n","170\n"]}]},{"cell_type":"markdown","metadata":{"id":"2NAXzXy4AUq0"},"source":["* **To determine VaR there are essentially two methods: historical or Monte Carlo simulation.**\n","\n","### Historical Simulation\n","\n","* The historical sequence of the asset daily price variations will provide different scenarios to be applied to today’s asset values. \n","\n","* The daily evolution of the portfolio can be simulated by rescaling each asset value according to its variation between two consecutive days $i$ and $i − 1$\n","\n","$$\\Delta\\Pi = \\left(\\cfrac{v_1(t_i)}{v_1(t_{i−1})} - 1\\right)v_1(t_n)w_1 + \\left(\\cfrac{v_2(t_i)}{v_2(t_{i−1})} - 1\\right)v_2(t_n)w_2$$\n","\n","* Draw the portfolio variation distribution, and the VaR estimate will be its (100-X)-percentile.\n","  * Such estimates relies on the assumption that past behaviors are indicative of what might happen in the future, \n","  * it is important that historical series was as large as possible.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cm6WMloXUrAq","executionInfo":{"status":"ok","timestamp":1647506110458,"user_tz":-60,"elapsed":575,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"19c05ab2-d8d0-432a-c0c8-eef34615b060"},"source":["# historical VaR with for-loops\n","\n","dP = []\n","for i in range(1, len(df)-1):\n","  dP.append(w[0]*df.iloc[i]['aapl_rets']*df.iloc[-1]['aapl'] + \n","            w[1]*df.iloc[i]['nflx_rets']*df.iloc[-1]['nflx'])\n","  #print (dP[-1])\n","hist_var = np.percentile(dP, 5)\n","print('Historical VAR is {:.3f}'.format(hist_var))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Historical VAR is -4.298\n"]}]},{"cell_type":"markdown","metadata":{"id":"mqzqugs8aso-"},"source":["<img src=\"https://drive.google.com/uc?id=15Tkz8_iOy7NVft_8Ia6vDpP4ShuSAqlb\">"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbjYPGYoUZ-_","executionInfo":{"status":"ok","timestamp":1647506132185,"user_tz":-60,"elapsed":569,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"953835a1-458c-4732-b5f2-22b4ba86f62a"},"source":["from scipy.stats import norm, t\n","\n","t_params = t.fit(dP)\n","g_params = norm.fit(dP)\n","\n","print (\"VaR from t-student fit\", round(t(*t_params).ppf(0.05), 3))\n","print (\"VaR from Gaussian fit \",round(norm(*g_params).ppf(0.05), 3))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["VaR from t-student fit -4.493\n","VaR from Gaussian fit  -5.197\n"]}]},{"cell_type":"markdown","metadata":{"id":"StluS1JTVlNA"},"source":["* By fitting the distribution of the changes in portfolio value it is apparent how it is a t-student rather than a Gaussian."]},{"cell_type":"markdown","metadata":{"id":"uIn14Pp4GHx7"},"source":["### Monte Carlo Simulation\n","* A very useful alternative to the historical approach is to use Monte Carlo simulation to generate the distribution of $\\Delta\\Pi$.\n","\n","* The simulation can be done in two alternative ways:\n","  * generating random returns from a distribution with mean and standard deviation obtained from the historical data of each asset;\n","  * evolving each asset price using a geometric Brownian motion.\n","\n","#### First Option\n","* Compute mean and standard deviation from the historical dataset, then sample various simulated returns from a multivariate Gaussian with such mean and variance. \n","  * One useful aspect of this method is that other distributions than Gaussian could be used.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTgedEQAZ13N","executionInfo":{"status":"ok","timestamp":1647506519952,"user_tz":-60,"elapsed":352,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"1bd09832-0140-44b8-ef58-81b1f7252dbf"},"source":["# MC VaR with distribution\n","from scipy.stats import multivariate_normal\n","\n","mean = df[['aapl_rets', 'nflx_rets']].iloc[1:].mean()\n","print (mean)\n","cov = df[['aapl_rets', 'nflx_rets']].iloc[1:].cov()\n","print (cov)\n","mv = multivariate_normal(mean=mean, cov=cov)\n","\n","np.random.seed(1)\n","x = mv.rvs(size=100000)\n","dP = (x*df[['aapl', 'nflx']].iloc[-1].values).dot(w)\n","mc_var = np.percentile(dP, 5)\n","\n","print('Simulated VAR is {:.3f}'.format(mc_var))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["aapl_rets    0.000785\n","nflx_rets    0.001651\n","dtype: float64\n","           aapl_rets  nflx_rets\n","aapl_rets   0.000207   0.000104\n","nflx_rets   0.000104   0.000705\n","Simulated VAR is -5.199\n"]}]},{"cell_type":"markdown","metadata":{"id":"wYCOu4l6bqCO"},"source":["<img src=\"https://drive.google.com/uc?id=1TGgzTD1czT0YJUV-BgDyc1eZ7OE39WHv\">"]},{"cell_type":"markdown","metadata":{"id":"Xgf1TJwYO1Ft"},"source":["* This result can be compared to the VaR estimate determined by evolving the asset price. \n","  * We will use the geometric Brownian motion where $\\mu$ and $\\sigma$ are the mean and variance estimated from the historical series. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2U3CMj-DaCCU","executionInfo":{"status":"ok","timestamp":1647506630403,"user_tz":-60,"elapsed":54892,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"9f01aad1-c22a-47ce-804a-81f2ee42cafe"},"source":["# MC VaR with evolution\n","from numpy.random import normal, seed\n","\n","portfolio_value = df[['aapl', 'nflx']].iloc[-1].dot(w)\n","T = 1\n","dP = []\n","seed(1)\n","for _ in range(100000):\n","  P1 = df.iloc[-1]['aapl']*np.exp((mean[0]-0.5*cov.iloc[0, 0])*T + np.sqrt(cov.iloc[0, 0]*T)*normal())\n","  P2 = df.iloc[-1]['nflx']*np.exp((mean[1]-0.5*cov.iloc[1, 1])*T + np.sqrt(cov.iloc[1, 1]*T)*normal())\n","  dP.append(w[0]*P1 + w[1]*P2 - portfolio_value)\n","\n","mc_var2 = np.percentile(dP, 5)\n","print('Simulated VAR is {:.3f}'.format(mc_var2))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Simulated VAR is -4.958\n"]}]},{"cell_type":"markdown","metadata":{"id":"lZ-tbEw1bz6h"},"source":["<img src=\"https://drive.google.com/uc?id=1Dd_Z2lAgfR9TRA0C_xaELLt4zDXMHOpw\">"]},{"cell_type":"markdown","metadata":{"id":"SGjRXql1PtNW"},"source":["### Stress-test and Backtesting\n","\n","* It is generally useful to check how VaR would behave under the most extreme market moves seen in the last years.\n","  * This kind of test is called **stress-test**;\n","  * it is done by extracting from the historical series particular days with exceptionally large variation of the market variables, to take into account extreme events that can happen more frequently in reality than in a simulation (where usually Gaussian tails are assumed). \n","  * For example a 5-standard deviation move is expected to happen once every 7000 years but in practice can be observed twice over 10 years.\n","\n","* **Backtesting** consists of assessing how well the VaR estimate would have performed in the past. \n","  * Basically it has to be tested how often the daily loss exceeded the daily X% VaR just computed. \n","    * If it happens on about (100- X)% of the times we can be confident that our estimate is correct. \n","    * Clearly back-testing makes sense only if VaR has been estimated on an independent historical sample with respect to that used in the test."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7wvDZpq1nRZ","executionInfo":{"status":"ok","timestamp":1647507701340,"user_tz":-60,"elapsed":681,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"87a6dc86-de15-4ec4-9e24-3fdb2ea5d314"},"source":["# backtesting\n","\n","df['P'] = df[['aapl', 'nflx']].iloc[:-1].dot(w)\n","df['dP'] = np.log1p(df['P'].pct_change())*df['P']\n","\n","print ((df['dP'] < mc_var).sum()/len(df))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0028142589118198874\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4P6bnsoQWPM"},"source":["## Credit VaR\n","\n","* Credit VaR is defined as a percentile of the **credit loss** distribution. \n","  * In this case we are concerned with the default risk associated to counter-parties instead of the market risk.\n","* The **exposure** $EE(\\tau)$ defined as the sum of the discounted cash flows at the default date $\\tau$. \n","* The corresponding **loss** is then given by $L =(1−R)·EE(\\tau)$\n","  * where $L$ is non-zero only in scenarios of early counter-party default.\n","* Credit VaR can be expressed as the X-quantile of the distribution of $L$. \n","  * Time horizon is usually set to one year and the percentile to the 99.9th, so it returns the loss that is exceeded only in 1 case out of 1000.\n","\n","### Credit VaR and MC Simulation\n","* Credit VaR can be calculated through a simulation of the evolution of a portfolio up to the risk horizon, including possible defaults of the counter-parties.\n","* In each experiment the portfolio is priced obtaining a number of scenarios to draw the loss distribution.\n","\n","#### Example\n","* Consider a portfolio of 20 zero coupon bonds each one with a default probability of 8% and the same face value (100 EUR). The recovery rate in case of default is 40% and the risk free rate is 1%."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQStBMspoOz6","executionInfo":{"status":"ok","timestamp":1647508129477,"user_tz":-60,"elapsed":825,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"2cc5ae90-8fce-4d70-acfc-093356a41d69"},"source":["# compute credit var of 20 ZCB\n","import numpy as np\n","from datetime import date\n","from scipy.stats import uniform\n","\n","bonds = 20\n","DP = 0.08\n","FV = 100\n","R = 0.4\n","r = 0.01\n","df = np.exp(-r)\n","\n","L = []\n","sims = 10000\n","for _ in range(sims):\n","  EE = FV*df\n","  u = uniform.rvs(size=bonds)\n","  n_defaults = (u<DP).sum()\n","  L.append((1-R)*EE*n_defaults)\n","\n","print (np.percentile(L, 99.9))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["356.4179401497005\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRr8jZqxUqOA","executionInfo":{"status":"ok","timestamp":1647508133493,"user_tz":-60,"elapsed":342,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"1278eb27-b865-4d60-d185-73c61a569129"},"source":["from scipy.stats import binom\n","\n","bi = binom(bonds, DP)\n","q = bi.ppf(0.999)\n","\n","print ((1 - R)*FV*df*q)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["356.4179401497005\n"]}]},{"cell_type":"markdown","metadata":{"id":"6UErWH-HYKA0"},"source":["* Actually, since we are dealing with independent and equiprobable default events, the distribution of the losses could have been estimated simply with the **binomial distribution**.\n","  * Loss distribution is a scaled version of the distribution of defaults, since LGD and FV are the same for each ZCB.\n","\n","\n","<table>\n","<tr>\n","<td><img src=\"https://drive.google.com/uc?id=1fstX-PUqjhFr1Xujoeam9yxw3_01jhOS\"></td>\n","<td><img src=\"https://drive.google.com/uc?id=1L0P2889QJJSGfqq3zXtzCO5LXnPgaE_Y\"></td>\n","</tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"FtATujWma8HY"},"source":["## CVA and DVA\n","\n","* Suppose you have a portfolio of derivatives. \n","* If a counter-party defaults and the present value of the portfolio at default is positive to the surviving party (you), then the actual gain is only given by the recovery fraction of the value. \n","* If however the present value is negative to you, you have to pay it in full to the liquidators of the defaulted entity.\n","* This behaviour creates an asymmetry which can be corrected by changing the definition of the deal value as the value without counter-party risk minus a positive adjustment, called Credit Valuation Adjustment (CVA).\n","\n","$$\\textrm{CVA} = (1-R)\\int_0^{T} D(t)\\cdot EE(t) dQ(t)$$\n","\n","* where $T$ is the latest maturity in the portfolio, $D$ is the discount factor, $EE$ is the expected exposure or $\\mathbb{E}[\\textrm{max}(0, \\mathrm{NPV_{portfolio}})]$, and $dQ$ is the probability of default between $t$ and $t + dt$.\n","\n","* Or it's discrete version:\n","\n","$$\\textrm{CVA} = (1-R)\\sum_i D(t_i)\\cdot EE(t_i)\\cdot  Q(t_{i-1}, t_i))$$\n","\n","* **Credit VaR measures the risk of losses faced due to the default of some counter-party, while CVA measures the price adjustment of a contract due to this risk**.\n","\n","### DVA\n","* The adjustment seen from the point of view of our counter-party is positive, and is called Debit Valuation Adjustment, DVA. \n","* It is positive because the early default of the client itself would imply a discount on its payment obligations, and this means a gain. \n","\n","* When both parties have a non-null probability of default, they consistently include both CVA and DVA into the valuation. So they will mark **a positive CVA to be subtracted** and **a positive DVA to be added** to the default-risk-free price of the deal. \n","  * The CVA of one party will be the DVA of the other one and vice versa.\n","\n","$$\\textrm{price = default risk free price + DVA - CVA}$$\n","\n","## CVA Computation\n","\n","* CVA can be computed with Monte Carlo simulation. \n","  1. Compute the portfolio value at each time point for each MC scenario.    \n","  2. Calculate the CVA using one of the equation above. \n","  3. Average the CVA of all the scenarios to get its estimate.\n","\n","#### Example\n","* Imagine a 3-years zero coupon bond with a face value FV = 100. \n","* The bond issuer has the following default probabilities 10%, 20% and 30% for 1, 2 and 3 years respectively and the recovery rate is 40%. \n","* The risk free rate is instead 3% flat.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exljwHqtYvU2","executionInfo":{"status":"ok","timestamp":1637052130025,"user_tz":-60,"elapsed":4,"user":{"displayName":"Matteo Sani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GineBtaMxaxjhqcwcrPYHu9akgYzh5nR28bIHY8M0k=s64","userId":"10639058540037747059"}},"outputId":"2408a570-2ca4-4118-c7f6-de46fc27fa88"},"source":["# CVA of a ZCB\n","from dateutil.relativedelta import relativedelta\n","from finmarkets import DiscountCurve, CreditCurve\n","import math\n","\n","FV = 100\n","T = 3\n","r = 0.03\n","R = 0.4\n","pricing_date = date.today()\n","pillar_dates = [pricing_date + relativedelta(years=i) for i in range(T+1)]\n","dfs = [np.exp(-r*t) for t in range(T+1)]\n","S = [1, 0.9, 0.8, 0.7]\n","dc = DiscountCurve(pillar_dates, dfs)\n","cc = CreditCurve(pillar_dates, S)\n","\n","cva = 0\n","d = pricing_date\n","while d <= pillar_dates[-1]:\n","  cva += (1-R)*dc.df(d)*max(0, FV)*(cc.ndp(d)-cc.ndp(d+relativedelta(days=1)))\n","  d += relativedelta(days=1)\n","\n","PV = FV*dc.df(pillar_dates[-1])\n","print (\"CVA: {:.2f}\".format(cva))\n","print (\"Adjusted Price: {:.2f}\".format(PV - cva))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CVA: 17.21\n","Adjusted Price: 74.18\n"]}]},{"cell_type":"markdown","metadata":{"id":"tyo2F6cKbEB2"},"source":["* This is a simplified situation in which one scenario is enough: \n","  * no simulation of ZCB paramaters is needed (eg. interest rate)."]}]}