{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Transformation\n",
    "\n",
    "* Distribution transformation is used to transform a random variable PDF to a uniform distribution and vice versa (also called *probability integral transform* or *percentile-to-percentile transform*). \n",
    "    * This method involves computing the CDF or the quantile function of the original distribution.\n",
    "    \n",
    "$$\\textrm{uniform_sample} \\rightarrow \\tt{distribution.ppf(uniform\\_sample)} \\rightarrow \\textrm{distribution_sample}$$\n",
    "$$\\textrm{distribution_sample} \\rightarrow \\tt{distribution.cdf(distribution\\_sample)} \\rightarrow \\textrm{uniform_sample}$$\n",
    "    \n",
    "#### Example\n",
    "\n",
    "* $P(X)$ is the uniform distribution to convert it to standard normal distribution with zero mean and unit standard deviation $\\mathcal{N}(0,1)$ just apply the inverse of normal CDF:\n",
    "    * remember that the uniform samples can be interpreted as cumulative probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000 -> -0.5244\n",
      "0.50000 -> 0.0000\n",
      "0.90000 -> 1.2816\n",
      "0.99999 -> 4.2649\n"
     ]
    }
   ],
   "source": [
    "# make table uniform to gauss\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_unif = [0.3, 0.5, 0.9, 0.99999]\n",
    "\n",
    "for x in x_unif:\n",
    "    print (\"{:.05f} -> {:.4f}\".format(x, norm.ppf(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\tt{python}$ provides an easier way to apply such transformations: indeed it can be applied directly on the entire sample.\n",
    "    * Given a sitribution object:\n",
    "        * $\\tt{rvs}$ method samples $\\tt{size}$ times from it.\n",
    "        * each other method (like $\\tt{cdf}$ or $\\tt{ppf}$) works on $\\tt{numpy.array}$, a particular kind of list, allowing to avoid loop-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0.16546429 0.83629022 0.5656176  0.81587113 0.96457022 0.35379494\n",
      " 0.24245933 0.71108937 0.97897189 0.60132219]\n",
      "[-0.9722452   0.97932471  0.16522766  0.89974166  1.80637638 -0.37509491\n",
      " -0.69841346  0.55656998  2.03296346  0.25677092]\n"
     ]
    }
   ],
   "source": [
    "# sample from uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "x_unif = uniform.rvs(size=10)\n",
    "print (type(x_unif))\n",
    "x_transf = norm.ppf(x_unif)\n",
    "print (x_unif)\n",
    "print (x_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uniform_gauss.png\">\n",
    "\n",
    "* with a 2D plot we can get a sense of what is going on when using the inverse CDF transformation:\n",
    "\n",
    "<img src=\"uniform_to_gauss_2d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It stretches the outer regions of the uniform to yield a normal distribution. \n",
    "\n",
    "\n",
    "* This technique can be used with any arbitrary (univariate) probability distributions, like for example t-Student or Gumbel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with t-student\n",
    "from scipy.stats import t\n",
    "\n",
    "x_trans2 = t(4).ppf(x_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uniform_tstudent_2d.png\">\n",
    "\n",
    "* To go from an arbitray distribution to uniform, just apply the inverse of the inverse CDF, which is the CDF itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make uniform to gauss to uniform\n",
    "x_trans3 = norm.ppf(x_unif)\n",
    "x_unif2 = norm.cdf(x_trans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"full_chain.png\">\n",
    "\n",
    "## Copula\n",
    "\n",
    "* In probability theory a *copula* $\\mathcal{C}(F_1, F_2, \\ldots, F_n)$ is a multivariate cumulative distribution function whose marginal probability distributions (the probability distribution of each dimension) are uniform. \n",
    "\n",
    "\n",
    "* Copulas are used to describe dependencies between random variables.\n",
    "    * widely used in quantitative finance to model risk. \n",
    "    \n",
    "    \n",
    "* Very popular since allow to easily model and estimate the distribution of random vectors by representing marginals and their correlation separately. \n",
    "    * A complicated problem can be split into many simpler components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem Case\n",
    "\n",
    "* Imagine to measure two correlated variables: look at various rivers and for each one look at its maximum water level, and also count how many months each river caused flooding. \n",
    "    * For the probability distribution of the maximum level of the river we know that are *Gumbel* distributed;\n",
    "    * while the number of floods can be modelled according to a *Beta* distribution.\n",
    "\n",
    "\n",
    "* It's reasonable to assume that the two are correlated, however we don't know how we could model that correlated probability distribution. \n",
    "    * Above it was only specified the distributions of the individual variables, irrespective of the other one (i.e. the marginals), in reality we would like to study the joint distribution of both of these together. \n",
    "\n",
    "\n",
    "* Copulas allow to decompose a joint probability distribution into their marginals (which by definition have no correlation) and a function which couples (hence the name) them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Correlation with Gaussian Copulas\n",
    "\n",
    "* We know how to convert every PDF from and to a uniform distribution. \n",
    "    * So we can generate uniformly distributed data with the correlation we want (for simplicity we ara going to assume Gaussian correlation);\n",
    "    * then transform the uniform marginals each into the desired distributions. \n",
    "\n",
    "\n",
    "* Let's sample from a multivariate normal (2D) with a 0.5 correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98096774 -1.09392014]\n",
      " [ 0.90299709 -0.38523323]\n",
      " [-1.00509964 -2.3151635 ]\n",
      " [-0.73534541 -0.99197424]\n",
      " [ 0.00415729 -0.07833814]]\n"
     ]
    }
   ],
   "source": [
    "# sample from multi-normal with corr\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0, 0], cov=[[1, 0.5],\n",
    "                                               [0.5 , 1]])\n",
    "\n",
    "x = mvnorm.rvs(size=10000)\n",
    "\n",
    "print (x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This particular $\\tt{python}$ notation is called *slicing* of an array\n",
    "    \n",
    "\n",
    "* Imagine to have a 2D matrix: to access an element of the matrix it is necessary to specify two indices (the row and the column of the element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4],[5,6,7,8], [9,10,11,12]])\n",
    "print (a)\n",
    "print(\"\")\n",
    "print (a[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To access a range of values it is possible to specify the interval of indices inside square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "print (a[1][1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally to get an entire row (or column) it is enough to specify $\\tt{:}$ (colon) as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 7 8]\n",
      "\n",
      "[5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "print (a[1][:]) \n",
    "print (\"\")\n",
    "print (a[1][0:4]) # which is equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multivariate_2d.png\">\n",
    "\n",
    "* Now tranform the marginals to uniform using the normal $\\tt{cdf}$ function (again the method is smart enough to automatically transform each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16330432 0.136995  ]\n",
      " [0.81673628 0.35003231]\n",
      " [0.15742444 0.01030199]\n",
      " [0.23106458 0.16060505]\n",
      " [0.50165851 0.46877954]]\n"
     ]
    }
   ],
   "source": [
    "# convert each component\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "print (x_unif[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"copula_2d.png\" width=300></td>\n",
    "        <td><img src=\"copula_3d.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* These plots are usually how copulas are visualized. \n",
    "\n",
    "\n",
    "* **Since we used a multivariate stadard normal to model correlation this is also called a Gaussian Copula.**\n",
    "\n",
    "\n",
    "* Finally we can just transform the marginals from uniform to what we want (i.e. Gumbel and Beta in our river example): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert again to gumbel e beta\n",
    "from scipy.stats import gumbel_l, beta\n",
    "\n",
    "gumbel = gumbel_l()\n",
    "b = beta(a=4, b=10)\n",
    "\n",
    "x1 = gumbel.ppf(x_unif[:][0])\n",
    "x2 = b.ppf(x_unif[:][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just compare the scatter plot with correlation to the joint distribution of the same marginals without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from Gumbel\n",
    "# sample from Beta\n",
    "m1 = gumbel.rvs(size=10000)\n",
    "m2 = b.rvs(size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"gumbel_beta_corr.png\" width=300></td>\n",
    "        <td><img src=\"gumbel_beta_uncorr.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Using the uniform distribution as a common base for our transformations we can easily introduce correlations and flexibly construct complex probability distributions. \n",
    "\n",
    "\n",
    "* Clearly this is directly extendeable to higher dimensional distributions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Correlation between Risks\n",
    "\n",
    "* The estimate of default probabilities and their correlations is the most important issue in credit derivative valuation and credit risk management. \n",
    "\n",
    "\n",
    "* Default correlation measures the tendency of two companies to default at about the same time. \n",
    "    * For this there are two ways: using historical default data or using mathematical models, like copulas. \n",
    "\n",
    "\n",
    "* *Historical default data* played an important role in the estimation of default probabilities, but: \n",
    "    * default events are rare so there is very limited default data available;\n",
    "    * historical data reflects the those default pattern only and it may not be a proper indicator of the future;\n",
    "        * default probabilities estimate from historical data is difficult and inexact; \n",
    "        * to estimate default correlations with same data is even more difficult and more inexact. \n",
    "\n",
    "\n",
    "* *Mathematical models* don't rely on historical default data: is the way that is now used to estimate/apply correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generate Correlated Distributions (Numerical Example)\n",
    "\n",
    "* Generate a random vector $\\mathbf{x}=(x_1, x_2,\\ldots)$ from a multivariate distribution with the desired correlation;\n",
    "* determine every $U_i(x_i)$ by applying $\\tt{cdf}$ to each $x_i$;\n",
    "* transform again each $U_i(x_i)$ to the desired marginal distributions using $\\tt{ppf}$.\n",
    "\n",
    "\n",
    "* Each component of $\\mathbf{x}$ is now transformed as it was sampled from the desired marginals with the appropriate correlation.\n",
    "\n",
    "#### Example\n",
    "\n",
    "* Imagine three companies (A, B and C) with cumulative 2 years default probability of 10%.\n",
    "    * Let’s compute the probabilities to have the three of them all defaulting within the next two years in the two cases: with independent and correlated default probabilities.\n",
    "\n",
    "\n",
    "* **Independent probabilities**: the odds to get three defaults within two years is the product of the single probabilities, hence:\n",
    "\n",
    "$$\\mathbb{P}_{\\mathrm{uncorr}}= 10\\%\\cdot 10\\%\\cdot 10\\% = 0.1\\%$$\n",
    "\n",
    "\n",
    "* The verification in $\\tt{python}$ can be done with the algorithm outlined above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00098\n"
     ]
    }
   ],
   "source": [
    "# test default probabilities of three companies (no corr)\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0,0,0], cov=[[1, 0, 0],\n",
    "                                                [0, 1, 0],\n",
    "                                                [0, 0, 1]])\n",
    "samples = 100000\n",
    "success = 0.\n",
    "x = mvnorm.rvs(size=samples)\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "for v in x_unif:\n",
    "    if max(v) < 0.1:\n",
    "        success += 1\n",
    "        \n",
    "print (success/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeat the same Monte Carlo experiment with perfectly correlated default probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09827\n"
     ]
    }
   ],
   "source": [
    "# test default probabilities of three companies (perfect corr)\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0, 0, 0], cov=[[1, 0.9999, 0.9999],\n",
    "                                                  [0.9999, 1, 0.9999],\n",
    "                                                  [0.9999, 0.9999, 1]])\n",
    "samples = 100000\n",
    "success = 0.\n",
    "x = mvnorm.rvs(size=samples)\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "for v in x_unif:\n",
    "    if max(v) < 0.1:\n",
    "        success += 1\n",
    "        \n",
    "print (success/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case the result is close to 10%.\n",
    "    * Indeed being the three companies perfectly correlated either there is no default or three *simultaneous* defaults with 10% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Copula Model (Analytical Solution)\n",
    "\n",
    "* The first copulla model introduced was the *one-factor Gaussian*.\n",
    "    * This model has, above all, the advantage that can be solved analytically.\n",
    "\n",
    "\n",
    "* Consider a portfolio of $N$ bonds with known marginal issuer probabilities of default\n",
    "    * $t_i$, the time of default of the $i^{th}$ company;\n",
    "    * $Q_i(t)$, the cumulative probability that company $i$ will default before time $t$; that is, the probability that $t_i \\le t$.\n",
    "\n",
    "\n",
    "* To generate a one-factor model for the $t_i$ we define random variables \n",
    "\n",
    "$$X_i = a_i M + \\sqrt{1-a_i^2}Z_i,\\qquad i = 1, 2,\\ldots, N$$\n",
    "\n",
    "* where $M$ and the $Z_i$ are independent zero-mean unit-variance distributions (hence $X_i$ are also distributed with zero-mean and unit standard-deviation) and $-1 \\le a_i \\lt 1$.\n",
    "    * Previous equation defines a correlation structure between the $X_i$ which depend on a single common factor $M$. \n",
    "    * The $Z_i$ term is usually called the idiosyncratic component of default\n",
    "\n",
    "\n",
    "* The correlation between $X_i$ and $X_j$ is\n",
    "\n",
    "$$\\mathrm{Corr}(X_i, X_j) = \\cfrac{\\mathbb{E}[(X_i-\\mu_i)(X_j-\\mu_j)]}{\\sigma_{i}\\sigma_{j}} =\\mathbb{E}[X_i X_j] = a_i a_j \\mathbb{E}[M^2] = a_i a_j\n",
    "$$\n",
    "\n",
    "* where we just exploit the definition of $X_i$ and its properties.\n",
    "\n",
    "\n",
    "* If $F_i$ is the CDF of $X_i$, with a percentile to percentile transformation we can map the $X_i$ to the $t_i$, so that $Q_i(t_i) = \\mathbb{P}(X_i\\le x)=F_i(x)$.\n",
    "\n",
    "\n",
    "* Therefore the point $X_i = x$ is transformed to $t_i = t$ where $x = F_i^{-1}[Q_i(t)]$:\n",
    "   * *conditional* on $M$, the $N$ default events are independent. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "Q_i(t_i|M) = \\mathbb{P}(X_i\\le x|M) &= \\mathbb{P}(a_i M + \\sqrt{1-a_i^2}Z_i\\le x|M) =\\\\\n",
    "&= \\mathbb{P}\\left(Z_i\\le \\cfrac{x-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "=H_i\\left(\\cfrac{F^{-1}[Q_i(t_i)]-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "* where $H_i$ is the CDF of the $Z_i$.\n",
    "\n",
    "\n",
    "* Since we are describing the Gaussian copula model we can specialize the previous equation as\n",
    "\n",
    "$$\n",
    "Q_i(t_i|M) = \\Phi\\left(\\cfrac{\\Phi^{-1}[Q_i(t_i)]-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "$$\n",
    "\n",
    "* where $\\Phi$ denotes the standard normal CDF. \n",
    "\n",
    "\n",
    "* If $\\mathcal{C}(t_1,\\ldots,t_N)$ is the joint distribution of the default times of the $N$ bonds in the portfolio then\n",
    "\n",
    "$$\n",
    "\\mathcal{C}(t_1,\\ldots,t_N)=\\Phi_{A}(\\Phi^{-1}(Q_1(t_1)),\\ldots,\\Phi^{-1}(Q_N(t_N)))\n",
    "$$\n",
    "\n",
    "* where $A$ is the correlation matrix of the default probabilities, is the one factor Gaussian copula\n",
    "    * one factor because there is only a random variable, $M$, which determines the correlation between $X_i$.\n",
    "\n",
    "\n",
    "* Different distributions result in different copula models, and in different natures of the default dependence. \n",
    "    * For example, copulas where the \\(M\\) have heavy tails generate models where there is a\n",
    "greater likelihood of a clustering of early defaults for several companies.\n",
    "\n",
    "#### Standard Market Model\n",
    "\n",
    "* Assume the following two assumptions are made:\n",
    "    * all the companies have the same default intensity (hazard rates), i.e, $\\lambda_i = \\lambda$ (which means they all have the same default probabilities);\n",
    "    * the pairwise default correlations are the same, i.e $a_i = a$; in other words the contribution of the market \tcomponent $M$ is the same for all the companies and the correlation between any two companies is constant, $\\rho = a^2$.\n",
    "\n",
    "\n",
    "* Under these assumptions, given the market situation $M = m$, all the companies have the same cumulative default probability $Q_i(t_i|m)=\\mathbb{P}(X_i \\le x|m)$.\n",
    "\n",
    "* With these assumptions the one factor model is also called *Market Standard Model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Distribution\n",
    "\n",
    "* The binomial distribution represents the discrete probability of independent events and each event has exactly two possible outcomes (e.g. toss of a coin).\n",
    "\n",
    "\n",
    "* The binomial distribution answers to the question: what is the probability of exactly $k$ successes on $n$ repeated trials given that:\n",
    "    * the number of observations or trials is fixed;\n",
    "    * each observation or trial is independent (e.g. like in the coin toss, the previous toss doesn’t affect the following);\n",
    "    * the probability of success is exactly the same from one trial to another (e.g. each coin toss has 50% chances to fall in heads or tails).\n",
    "\n",
    "$$\\mathrm{PMF_{binomial}} = \\binom{n}{k} p^{k}(1-p)^{n-k} = \\cfrac{n!}{k!(n−k)!} p^{k}(1-p)^{n-k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24609375000000003\n"
     ]
    }
   ],
   "source": [
    "# probability of tossing a coin \n",
    "from scipy.stats import binom\n",
    "print (binom(10, 0.5).pmf(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = range(20)\n",
    "plt.bar(x, binom(10, 0.5).pmf(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6230468750000006\n"
     ]
    }
   ],
   "source": [
    "# probability of having more then 5 heads\n",
    "b = binom(10, 0.5)\n",
    "\n",
    "s = 0\n",
    "for i in range(5, 11):\n",
    "    s += b.pmf(i)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6230468749999999\n",
      "0.6230468749999999\n"
     ]
    }
   ],
   "source": [
    "# compute as difference of cdfs or 1-cdf\n",
    "print (b.cdf(10)-b.cdf(4))\n",
    "print (1 - b.cdf(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Gaussian Copula Model\n",
    "\n",
    "* Let $l_{t|m}$ be the total defaults that have occurred by time $t$ conditional on the market condition $M = m$, then $l_{t|m}$ follows a binomial distribution (each default has the same probability and events are independent to each other once $M$ is fixed)\n",
    "\n",
    "$$Q(l_{t|m} = j) = \\cfrac{N!}{j!(N-j)!}Q^j_{t|m}(1-Q_{t|m})^{N-j},\\qquad  j=0, 1, 2,\\ldots,N$$\n",
    "\n",
    "\n",
    "* To evaluate any function of $Q$, $g(Q(l_t))$, regardless the value of $M$ is necessary to average \n",
    "\n",
    "$$\n",
    "g(Q(l_{t} = j)) = \\int_{-\\infty}^{\\infty}{g(Q(l_{t|m} = j))\\cdot f_M(m)dm}\n",
    "$$\n",
    "\n",
    "* where $f_M(m)$ is the PDF of $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basket Default Swaps\n",
    "* A basket default swap is a credit derivative on a portfolio of reference entities. \n",
    "    * The simplest basket default swaps are first-to-default, second-to-default...nth-to-default swaps. \n",
    "\n",
    "\n",
    "* Very similar to normal CDS except for the protection they offer:\n",
    "    * a first-to-default swap provides insurance for only the first default happening;\n",
    "    * a second-to-default swap provides insurance for only the second default...\n",
    "    \n",
    "\n",
    "* For example, in a nth-to-default swap, the seller does not make any payment to the protection buyer for the first $n-1$ defaulted entities, and makes it only for the\n",
    "$n^{th}$ default. Once there has been this payment the swap terminates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n$^{th}$-to-default Basket Valuation \n",
    "\n",
    "* Assume the principals and expected recovery rates are the same for all underlying reference assets.\n",
    "\n",
    "\n",
    "* The valuation procedure is similar to that for a regular CDS:\n",
    "    * in CDS valuation based on the probability that a default occured between times $t_1$ and $t_2$. \n",
    "    * in basket based on the probability that the $n^{th}$ default was between times $t_1$ and $t_2$.\n",
    "\n",
    "\n",
    "* The buyer of protection makes quarterly payments at a specified rate until the $n^{th}$ default occurs or the end of the life of the contract is reached. \n",
    "\n",
    "\n",
    "* In the event of the $n^{th}$ default occurring, the seller pays $F\\cdot(1-R)$.\n",
    "\n",
    "\n",
    "* The contract can be valued by calculating the expected present value of payments and the expected present value of payoffs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\tt{args}$ and $\\tt{kwargs}$\n",
    "\n",
    "* $\\tt{args}$ and $\\tt{kwargs}$ represent two useful ways of passing parameters to a function that is called by another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "22\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "def runner(f, x, args):\n",
    "    return f(x, *args)*2\n",
    "\n",
    "def func(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "for v in range(3):\n",
    "    print (runner(func, 2, args=(v, 1, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\tt{kwargs}$ works like $\\tt{args}$ except that you have to pass a dictionary with as keys the names of the parameters. Also to expand it you have to use the double-star operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "22\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "def runner(f, x, kwargs):\n",
    "    return f(x, **kwargs)*2\n",
    "\n",
    "for v in range(3):\n",
    "    print (runner(func, 2, kwargs={\"a\":v, \"c\":5, \"b\":1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compute the correlated default probability of each name according to \n",
    "\n",
    "$$Q(t|M) = \\Phi\\left(\\cfrac{\\Phi^{-1}[Q(t)]-\\sqrt{\\rho} M}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "* compute the probability to have at least j defaults using the binomial distribution\n",
    "\n",
    "$$Q(l_{t|m} \\ge j) = \\sum_{k=j}^{N}\\left[\\cfrac{N!}{k!(N-k)!}Q(t|M)^k(1-Q(t|M))^{N-k}\\right]$$\n",
    "\n",
    "* create a **credit curve**, $CC(Q)$,  with those probabilities;\n",
    "* re-use the $\\tt{CreditDefaultSwap}$ class methods to compute NPV and breakeven rate (which depend on a discount curve and a previous credit curce), through integration of\n",
    "\n",
    "$$ \\mathrm{NPV}(DC, CC(Q)) = \\int_{-\\infty}^{\\infty}{\\mathrm{NPV}(DC, CC(Q)) f_M(m)dm} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration in Python\n",
    "\n",
    "* To integrate a function $f$ in $\\tt{python}$ we can use the $\\tt{scipy.integrate.quad}$ which takes as input:\n",
    "    * the function to integrate; \n",
    "    * the integration limits;\n",
    "    * optional arguments of the integrand.\n",
    "\n",
    "$$f(x) = \\int_{-2}^{4}(ax^{3} + b)~dx$$\n",
    "\n",
    "* for $a=3$ and $b=5$. \n",
    "\n",
    "$$\\int_{-2}^{4}(ax^{3} + b)~dx = a\\cfrac{x^4}{4} + bx |_{-2}^{4} = 210$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210.0, 2.475694446663456e-12)\n"
     ]
    }
   ],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a*x**3 + b\n",
    "\n",
    "s = quad(func, -2, 4, args=(3, 5))\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finmarkets import CreditCurve, CreditDefaultSwap\n",
    "from scipy.stats import norm, binom\n",
    "from numpy import sqrt, exp\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "class BasketDefaultSwaps:\n",
    "    def __init__(self, notional, names, rho, start_date, spread,\n",
    "                 maturity, tenor=3, recovery=0.4):\n",
    "        self.names = names\n",
    "        self.rho = rho\n",
    "        self.cds = CreditDefaultSwap(notional, start_date, spread, maturity,\n",
    "                                     tenor, recovery)\n",
    "        \n",
    "    def npv(self, Q_dates, Q, dc, n_defaults):\n",
    "        v = quad(self.one_factor_model, -np.inf, np.inf,\n",
    "                 args=(self.cds.npv, Q_dates, Q, dc, n_defaults))\n",
    "        return v[0]\n",
    "    \n",
    "    def one_factor_model(self, M, f, Q_dates, Q, dc, j):\n",
    "        DP = norm.cdf((norm.ppf(Q)-sqrt(self.rho)*M)/(sqrt(1-rho)))\n",
    "        b = binom(self.names, DP)\n",
    "        S = 1 - (1-b.cdf(j-1))\n",
    "        cc = CreditCurve(Q_dates, S)\n",
    "        return f(dc, cc)*norm.pdf(M)\n",
    "    \n",
    "    def breakeven(self, Q_dates, Q, dc, n_defaults):\n",
    "        v = quad(self.one_factor_model, -np.inf, np.inf,\n",
    "                 args=(self.cds.breakevenRate, Q_dates, Q, dc, n_defaults))\n",
    "        return v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider a 5-year 3rd-to-default CDS on a basket of 10 reference entities with copula correlation of 0.3 and the expected recovery rate, $R$, is $40\\%$. \n",
    "\n",
    "\n",
    "* The term structure of interest rates is assumed to be flat at 5%. \n",
    "\n",
    "\n",
    "* The default probabilities for the 10 entities are generated by Poisson processes with constant default intensities, $\\lambda_i = 0.01$, so that \n",
    "\n",
    "$$Q(t) = 1 - e^{-\\lambda t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017753900471554232\n"
     ]
    }
   ],
   "source": [
    "from finmarkets import DiscountCurve\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "\n",
    "n_cds = 10\n",
    "rho = 0.3\n",
    "l = 0.01\n",
    "observation_date = date.today()\n",
    "\n",
    "pillar_dates = [observation_date + relativedelta(years=i) for i in range(6)]\n",
    "dfs = [1/(1+0.05)**i for i in range(6)]\n",
    "dc = DiscountCurve(pillar_dates, dfs)\n",
    "Q = [1-np.exp(-(l*t)) for t in range(6)]\n",
    "\n",
    "ndefaults = 3\n",
    "basket = BasketDefaultSwaps(1, n_cds, rho, observation_date, 0.01, 5)\n",
    "print(basket.breakeven(pillar_dates, Q, dc, ndefaults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collateralized Debt Obligation\n",
    "\n",
    "A Collateralized Debt Obligation (CDO) is a credit derivative where the issuer, typically investment banks, gather risky assets and repackage them into discrete classes (*tranches*) based on the level of credit risk assumed by the investor. These tranches of securities become the final investment product.\n",
    "\n",
    "Tranches are named to reflect their risk profile: senior, mezzanine and subordinated/equity and are delimited by the attachment ($L$) and detachment points ($U$), which represent the percentages of the total principal defining their boundaries. \n",
    "For example, a 5-10% tranche has an attachment point of 5% and a detachment point of 10%. \n",
    "\n",
    "<!----When the accumulated loss of the reference pool is no more than 5% of the total initial notional of the pool, the tranche will not be affected. However, when the loss has exceeded 5%, any further loss will be deducted from the tranche's notional until the detachment point, 10%, is reached.-->\n",
    "\n",
    "Each of these tranches has a different level of seniority relative to the others in the sense that a senior tranche has coupon\n",
    "and principal payment priority over a mezzanine tranche, while a mezzanine tranche has\n",
    "coupon and principal payment priority over an equity tranche. \n",
    "\n",
    "Indeed they receive returns using a set of rules known as *waterfall*. Incomes of the portfolio are first used to provide returns to the most senior tranche, then to the next and so on.\n",
    "So the senior tranches are generally safest because they have the first claim on the collateral, although they'll offer lower coupon rates.\n",
    "\n",
    "It is important to note\n",
    "that a CDO only redistributes the total risk associated with the underlying pool of assets\n",
    "to the priority ordered tranches. It neither reduces nor increases the total risk associated\n",
    "with the pool.\n",
    "\n",
    "There are various kind of CDOs:\n",
    "\n",
    "* in a **Cash CDO** the reference portfolio consists of corporate bonds owned by the CDO issuer. Cash flows from collateral are used to pay principal and interest to investors. If such cash flows prove inadequate, principal and interest is paid to tranches according to their seniority. \n",
    "\n",
    "<img src=\"cdo_structure.png\">\n",
    "\n",
    "* in a **Synthetic CDO** the underlying reference portfolio is no longer a physical portfolio of bonds or loans, instead it is a *fictitious* portfolio consisting of a number of names each with an associated notional amount. The value of a synthetic CDO usually comes from insurance premiums of credit default swaps paid for by investors. The seller assumes the underlying assets will perform while the investor, on the other hand, assumes the underlying assets will default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
