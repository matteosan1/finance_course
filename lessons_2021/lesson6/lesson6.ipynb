{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Correlation between Risks\n",
    "\n",
    "* The estimate of default probabilities and their correlations is the most important issue in credit derivative valuation and credit risk management. \n",
    "\n",
    "\n",
    "* Default correlation measures the tendency of two companies to default at about the same time. \n",
    "    * For this there are two ways: using historical default data or using mathematical models, like copulas. \n",
    "\n",
    "\n",
    "* *Historical default data* played an important role in the estimation of default probabilities, but: \n",
    "    * default events are rare so there is very limited default data available;\n",
    "    * historical data reflects the those default pattern only and it may not be a proper indicator of the future;\n",
    "        * default probabilities estimate from historical data is difficult and inexact; \n",
    "        * to estimate default correlations with same data is even more difficult and more inexact. \n",
    "\n",
    "\n",
    "* *Mathematical models* don't rely on historical default data: is the way that is now used to estimate/apply correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Transformation\n",
    "\n",
    "* Distribution transformation is used to transform a random variable PDF to a uniform distribution and vice versa (also called *probability integral transform* or *percentile-to-percentile transform*). \n",
    "    * This method involves computing the CDF or the quantile function of the original distribution.\n",
    "    \n",
    "$$\\textrm{uniform_sample} \\rightarrow \\tt{distribution.ppf(uniform\\_sample)} \\rightarrow \\textrm{distribution_sample}$$\n",
    "$$\\textrm{distribution_sample} \\rightarrow \\tt{distribution.cdf(distribution\\_sample)} \\rightarrow \\textrm{uniform_sample}$$\n",
    "    \n",
    "#### Example\n",
    "\n",
    "* $P(X)$ is the uniform distribution to convert it to standard normal distribution with zero mean and unit standard deviation $\\mathcal{N}(0,1)$ just apply the inverse of normal CDF:\n",
    "    * remember that the uniform samples can be interpreted as cumulative probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000 -> -0.5244\n",
      "0.50000 -> 0.0000\n",
      "0.90000 -> 1.2816\n",
      "0.99999 -> 4.2649\n"
     ]
    }
   ],
   "source": [
    "# make table uniform to gauss\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_unif = [0.3, 0.5, 0.9, 0.99999]\n",
    "\n",
    "for x in x_unif:\n",
    "    print (\"{:.05f} -> {:.4f}\".format(x, norm.ppf(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\tt{python}$ provides an easier way to apply such transformations: indeed it can be applied directly on the entire sample.\n",
    "    * Given a sitribution object:\n",
    "        * $\\tt{rvs}$ method samples $\\tt{size}$ times from it.\n",
    "        * each other method (like $\\tt{cdf}$ or $\\tt{ppf}$) works on $\\tt{numpy.array}$, a particular kind of list, allowing to avoid loop-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0.16546429 0.83629022 0.5656176  0.81587113 0.96457022 0.35379494\n",
      " 0.24245933 0.71108937 0.97897189 0.60132219]\n",
      "[-0.9722452   0.97932471  0.16522766  0.89974166  1.80637638 -0.37509491\n",
      " -0.69841346  0.55656998  2.03296346  0.25677092]\n"
     ]
    }
   ],
   "source": [
    "# sample from uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "x_unif = uniform.rvs(size=10)\n",
    "print (type(x_unif))\n",
    "x_transf = norm.ppf(x_unif)\n",
    "print (x_unif)\n",
    "print (x_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uniform_gauss.png\">\n",
    "\n",
    "* with a 2D plot we can get a sense of what is going on when using the inverse CDF transformation:\n",
    "\n",
    "<img src=\"uniform_to_gauss_2d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It stretches the outer regions of the uniform to yield a normal distribution. \n",
    "\n",
    "\n",
    "* This technique can be used with any arbitrary (univariate) probability distributions, like for example t-Student or Gumbel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with t-student\n",
    "from scipy.stats import t\n",
    "\n",
    "x_trans2 = t(4).ppf(x_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uniform_tstudent_2d.png\">\n",
    "\n",
    "* To go from an arbitray distribution to uniform, just apply the inverse of the inverse CDF, which is the CDF itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make uniform to gauss to uniform\n",
    "x_trans3 = norm.ppf(x_unif)\n",
    "x_unif2 = norm.cdf(x_trans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"full_chain.png\">\n",
    "\n",
    "## Copula\n",
    "\n",
    "* In probability theory a *copula* $\\mathcal{C}(F_1, F_2, \\ldots, F_n)$ is a multivariate cumulative distribution function whose marginal probability distributions (the probability distribution of each dimension) are uniform. \n",
    "\n",
    "\n",
    "* Copulas are used to describe dependencies between random variables.\n",
    "    * widely used in quantitative finance to model risk. \n",
    "    \n",
    "    \n",
    "* Very popular since allow to easily model and estimate the distribution of random vectors by representing marginals and their correlation separately. \n",
    "    * A complicated problem can be split into many simpler components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem Case\n",
    "\n",
    "* Imagine to measure two correlated variables: look at various rivers and for each one look at its maximum water level, and also count how many months each river caused flooding. \n",
    "    * For the probability distribution of the maximum level of the river we know that are *Gumbel* distributed;\n",
    "    * while the number of floods can be modelled according to a *Beta* distribution.\n",
    "\n",
    "\n",
    "* It's reasonable to assume that the two are correlated, however we don't know how we could model that correlated probability distribution. \n",
    "    * Above it was only specified the distributions of the individual variables, irrespective of the other one (i.e. the marginals), in reality we would like to study the joint distribution of both of these together. \n",
    "\n",
    "\n",
    "* Copulas allow to decompose a joint probability distribution into their marginals (which by definition have no correlation) and a function which couples (hence the name) them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Correlation with Gaussian Copulas\n",
    "\n",
    "* We know how to convert every PDF from and to a uniform distribution. \n",
    "    * So we can generate uniformly distributed data with the correlation we want (for simplicity we ara going to assume Gaussian correlation);\n",
    "    * then transform the uniform marginals each into the desired distributions. \n",
    "\n",
    "\n",
    "* Let's sample from a multivariate normal (2D) with a 0.5 correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98096774 -1.09392014]\n",
      " [ 0.90299709 -0.38523323]\n",
      " [-1.00509964 -2.3151635 ]\n",
      " [-0.73534541 -0.99197424]\n",
      " [ 0.00415729 -0.07833814]]\n"
     ]
    }
   ],
   "source": [
    "# sample from multi-normal with corr\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0, 0], cov=[[1, 0.5],\n",
    "                                               [0.5 , 1]])\n",
    "\n",
    "x = mvnorm.rvs(size=10000)\n",
    "\n",
    "print (x[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This particular $\\tt{python}$ notation is called *slicing* of an array\n",
    "    \n",
    "\n",
    "* Imagine to have a 2D matrix: to access an element of the matrix it is necessary to specify two indices (the row and the column of the element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4],[5,6,7,8], [9,10,11,12]])\n",
    "print (a)\n",
    "print(\"\")\n",
    "print (a[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To access a range of values it is possible to specify the interval of indices inside square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "print (a[1][1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally to get an entire row (or column) it is enough to specify $\\tt{:}$ (colon) as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 7 8]\n",
      "\n",
      "[5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "print (a[1][:]) \n",
    "print (\"\")\n",
    "print (a[1][0:4]) # which is equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multivariate_2d.png\">\n",
    "\n",
    "* Now tranform the marginals to uniform using the normal $\\tt{cdf}$ function (again the method is smart enough to automatically transform each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16330432 0.136995  ]\n",
      " [0.81673628 0.35003231]\n",
      " [0.15742444 0.01030199]\n",
      " [0.23106458 0.16060505]\n",
      " [0.50165851 0.46877954]]\n"
     ]
    }
   ],
   "source": [
    "# convert each component\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "print (x_unif[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"copula_2d.png\" width=300></td>\n",
    "        <td><img src=\"copula_3d.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* These plots are usually how copulas are visualized. \n",
    "\n",
    "\n",
    "* **Since we used a multivariate stadard normal to model correlation this is also called a Gaussian Copula.**\n",
    "\n",
    "\n",
    "* Finally we can just transform the marginals from uniform to what we want (i.e. Gumbel and Beta in our river example): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert again to gumbel e beta\n",
    "from scipy.stats import gumbel_l, beta\n",
    "\n",
    "gumbel = gumbel_l()\n",
    "b = beta(a=4, b=10)\n",
    "\n",
    "x1 = gumbel.ppf(x_unif[:][0])\n",
    "x2 = b.ppf(x_unif[:][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just compare the scatter plot with correlation to the joint distribution of the same marginals without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from Gumbel\n",
    "# sample from Beta\n",
    "m1 = gumbel.rvs(size=10000)\n",
    "m2 = b.rvs(size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"gumbel_beta_corr.png\" width=300></td>\n",
    "        <td><img src=\"gumbel_beta_uncorr.png\" width=300></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Using the uniform distribution as a common base for our transformations we can easily introduce correlations and flexibly construct complex probability distributions. \n",
    "\n",
    "\n",
    "* Clearly this is directly extendeable to higher dimensional distributions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generate Correlated Distributions (Numerical Example)\n",
    "\n",
    "* Generate a random vector $\\mathbf{x}=(x_1, x_2,\\ldots)$ from a multivariate distribution with the desired correlation;\n",
    "* determine every $U_i(x_i)$ by applying $\\tt{cdf}$ to each $x_i$;\n",
    "* transform again each $U_i(x_i)$ to the desired marginal distributions using $\\tt{ppf}$.\n",
    "\n",
    "\n",
    "* Each component of $\\mathbf{x}$ is now transformed as it was sampled from the desired marginals with the appropriate correlation.\n",
    "\n",
    "#### Example\n",
    "\n",
    "* Imagine three companies (A, B and C) with cumulative 2 years default probability of 10%.\n",
    "    * Let’s compute the probabilities to have the three of them all defaulting within the next two years in the two cases: with independent and correlated default probabilities.\n",
    "\n",
    "\n",
    "* **Independent probabilities**: the odds to get three defaults within two years is the product of the single probabilities, hence:\n",
    "\n",
    "$$\\mathbb{P}_{\\mathrm{uncorr}}= 10\\%\\cdot 10\\%\\cdot 10\\% = 0.1\\%$$\n",
    "\n",
    "\n",
    "* The verification in $\\tt{python}$ can be done with the algorithm outlined above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00098\n"
     ]
    }
   ],
   "source": [
    "# test default probabilities of three companies (no corr)\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0,0,0], cov=[[1, 0, 0],\n",
    "                                                [0, 1, 0],\n",
    "                                                [0, 0, 1]])\n",
    "samples = 100000\n",
    "success = 0.\n",
    "x = mvnorm.rvs(size=samples)\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "for v in x_unif:\n",
    "    if max(v) < 0.1:\n",
    "        success += 1\n",
    "        \n",
    "print (success/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeat the same Monte Carlo experiment with perfectly correlated default probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09827\n"
     ]
    }
   ],
   "source": [
    "# test default probabilities of three companies (perfect corr)\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "mvnorm = multivariate_normal(mean=[0, 0, 0], cov=[[1, 0.9999, 0.9999],\n",
    "                                                  [0.9999, 1, 0.9999],\n",
    "                                                  [0.9999, 0.9999, 1]])\n",
    "samples = 100000\n",
    "success = 0.\n",
    "x = mvnorm.rvs(size=samples)\n",
    "x_unif = norm.cdf(x)\n",
    "\n",
    "for v in x_unif:\n",
    "    if max(v) < 0.1:\n",
    "        success += 1\n",
    "        \n",
    "print (success/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case the result is close to 10%.\n",
    "    * Indeed being the three companies perfectly correlated either there is no default or three *simultaneous* defaults with 10% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Copula Model (Analytical Solution)\n",
    "\n",
    "* The first copulla model introduced was the *one-factor Gaussian*.\n",
    "    * This model has, above all, the advantage that can be solved analytically.\n",
    "\n",
    "\n",
    "* Consider a portfolio of $N$ bonds with known marginal issuer probabilities of default\n",
    "    * $t_i$, the time of default of the $i^{th}$ company;\n",
    "    * $Q_i(t)$, the cumulative probability that company $i$ will default before time $t$; that is, the probability that $t_i \\le t$.\n",
    "\n",
    "\n",
    "* To generate a one-factor model for the $t_i$ we define random variables \n",
    "\n",
    "$$X_i = a_i M + \\sqrt{1-a_i^2}Z_i,\\qquad i = 1, 2,\\ldots, N$$\n",
    "\n",
    "* where $M$ and the $Z_i$ are independent zero-mean unit-variance distributions (hence $X_i$ are also distributed with zero-mean and unit standard-deviation) and $-1 \\le a_i \\lt 1$.\n",
    "    * Previous equation defines a correlation structure between the $X_i$ which depend on a single common factor $M$. \n",
    "    * The $Z_i$ term is usually called the idiosyncratic component of default\n",
    "\n",
    "\n",
    "* The correlation between $X_i$ and $X_j$ is\n",
    "\n",
    "$$\\mathrm{Corr}(X_i, X_j) = \\cfrac{\\mathbb{E}[(X_i-\\mu_i)(X_j-\\mu_j)]}{\\sigma_{i}\\sigma_{j}} =\\mathbb{E}[X_i X_j] = a_i a_j \\mathbb{E}[M^2] = a_i a_j\n",
    "$$\n",
    "\n",
    "* where we just exploit the definition of $X_i$ and its properties.\n",
    "\n",
    "\n",
    "* If $F_i$ is the CDF of $X_i$, with a percentile to percentile transformation we can map the $X_i$ to the $t_i$, so that $Q_i(t_i) = P(X_i\\le x)=F_i(x)$.\n",
    "\n",
    "\n",
    "* Therefore the point $X_i = x$ is transformed to $t_i = t$ where $x = F_i^{-1}[Q_i(t)]$:\n",
    "   * *conditional* on $M$, the $N$ default events are independent. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "Q_i(t_i|M) = P(X_i\\le x|M) &= P(a_i M + \\sqrt{1-a_i^2}Z_i\\le x|M) =\\\\\n",
    "&= P\\left(Z_i\\le \\cfrac{x-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "=H_i\\left(\\cfrac{F^{-1}[Q_i(t_i)]-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "* where $H_i$ is the CDF of the $Z_i$.\n",
    "\n",
    "\n",
    "* Since we are describing the Gaussian copula model we can specialize the previous equation as\n",
    "\n",
    "$$\n",
    "Q_i(t_i|M) = \\Phi\\left(\\cfrac{\\Phi^{-1}[Q_i(t_i)]-a_i M}{\\sqrt{1-a_i^2}}\\right)\n",
    "$$\n",
    "\n",
    "* where $\\Phi$ denotes the standard normal CDF. \n",
    "\n",
    "\n",
    "* If $\\mathcal{C}(t_1,\\ldots,t_N)$ is the joint distribution of the default times of the $N$ bonds in the portfolio then\n",
    "\n",
    "$$\n",
    "\\mathcal{C}(t_1,\\ldots,t_N)=\\Phi_{A}(\\Phi^{-1}(Q_1(t_1)),\\ldots,\\Phi^{-1}(Q_N(t_N)))\n",
    "$$\n",
    "\n",
    "* where $A$ is the correlation matrix of the default probabilities, is the one factor Gaussian copula\n",
    "    * one factor because there is only a random variable, $M$, which determines the correlation between $X_i$.\n",
    "\n",
    "\n",
    "* Different distributions result in different copula models, and in different natures of the default dependence. \n",
    "    * For example, copulas where the \\(M\\) have heavy tails generate models where there is a\n",
    "greater likelihood of a clustering of early defaults for several companies.\n",
    "\n",
    "#### Standard Market Model\n",
    "\n",
    "* Assume the following two assumptions are made:\n",
    "    * all the companies have the same default intensity (hazard rates), i.e, $\\lambda_i = \\lambda$ (which means they all have the same default probabilities);\n",
    "    * the pairwise default correlations are the same, i.e $a_i = a$; in other words the contribution of the market \tcomponent $M$ is the same for all the companies and the correlation between any two companies is constant, $\\rho = a^2$.\n",
    "\n",
    "\n",
    "* Under these assumptions, given the market situation $M = m$, all the companies have the same cumulative default probability $Q_i(t_i|m)=\\mathbb{P}(X_i \\le x|m)$.\n",
    "\n",
    "* With these assumptions the one factor model is also called *Market Standard Model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Distribution\n",
    "\n",
    "* The binomial distribution represents the discrete probability of independent events and each event has exactly two possible outcomes (e.g. toss of a coin).\n",
    "\n",
    "\n",
    "* The binomial distribution answers to the question: what is the probability of exactly $k$ successes on $n$ repeated trials given that:\n",
    "    * the number of observations or trials is fixed;\n",
    "    * each observation or trial is independent (e.g. like in the coin toss, the previous toss doesn’t affect the following);\n",
    "    * the probability of success is exactly the same from one trial to another (e.g. each coin toss has 50% chances to fall in heads or tails).\n",
    "\n",
    "$$\\mathrm{PMF_{binomial}} = \\binom{n}{k} p^{k}(1-p)^{n-k} = \\cfrac{n!}{k!(n−k)!} p^{k}(1-p)^{n-k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24609375000000003\n"
     ]
    }
   ],
   "source": [
    "# probability of tossing a coin \n",
    "from scipy.stats import binom\n",
    "print (binom(10, 0.5).pmf(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = range(20)\n",
    "plt.bar(x, binom(10, 0.5).pmf(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6230468750000006\n"
     ]
    }
   ],
   "source": [
    "# probability of having more then 5 heads\n",
    "b = binom(10, 0.5)\n",
    "\n",
    "s = 0\n",
    "for i in range(5, 11):\n",
    "    s += b.pmf(i)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6230468749999999\n",
      "0.6230468749999999\n"
     ]
    }
   ],
   "source": [
    "# compute as difference of cdfs or 1-cdf\n",
    "print (b.cdf(10)-b.cdf(4))\n",
    "print (1 - b.cdf(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Gaussian Copula Model\n",
    "\n",
    "* Let $l_{t|m}$ be the total defaults that have occurred by time $t$ conditional on the market condition $M = m$, then $l_{t|m}$ follows a binomial distribution (each default has the same probability and events are independent to each other once $M$ is fixed)\n",
    "\n",
    "$$Q(l_{t|m} = j) = \\cfrac{N!}{j!(N-j)!}Q^j_{t|m}(1-Q_{t|m})^{N-j},\\qquad  j=0, 1, 2,\\ldots,N$$\n",
    "\n",
    "\n",
    "* To evaluate any function of $Q$, $g(Q(l_t))$, regardless the value of $M$ is necessary to average \n",
    "\n",
    "$$\n",
    "g(Q(l_{t} = j)) = \\int_{-\\infty}^{\\infty}{g(Q(l_{t|m} = j))\\cdot f_M(m)dm}\n",
    "$$\n",
    "\n",
    "* where $f_M(m)$ is the PDF of $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basket Default Swaps\n",
    "* A basket default swap is a credit derivative on a portfolio of reference entities. \n",
    "    * The simplest basket default swaps are first-to-default, second-to-default...nth-to-default swaps. \n",
    "\n",
    "\n",
    "* Very similar to normal CDS except for the protection they offer:\n",
    "    * a first-to-default swap provides insurance for only the first default happening;\n",
    "    * a second-to-default swap provides insurance for only the second default...\n",
    "    \n",
    "\n",
    "* For example, in a nth-to-default swap, the seller does not make any payment to the protection buyer for the first $n-1$ defaulted entities, and makes it only for the\n",
    "$n^{th}$ default. Once there has been this payment the swap terminates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n$^{th}$-to-default Basket Valuation \n",
    "\n",
    "* Assume the principals and expected recovery rates are the same for all underlying reference assets.\n",
    "\n",
    "\n",
    "* The valuation procedure is similar to that for a regular CDS:\n",
    "    * in CDS valuation based on the probability that a default occured between times $t_1$ and $t_2$. \n",
    "    * in basket based on the probability that the $n^{th}$ default was between times $t_1$ and $t_2$.\n",
    "\n",
    "\n",
    "* The buyer of protection makes quarterly payments at a specified rate until the $n^{th}$ default occurs or the end of the life of the contract is reached. \n",
    "\n",
    "\n",
    "* In the event of the $n^{th}$ default occurring, the seller pays $F\\cdot(1-R)$.\n",
    "\n",
    "\n",
    "* The contract can be valued by calculating the expected present value of payments and the expected present value of payoffs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\tt{args}$ and $\\tt{kwargs}$\n",
    "\n",
    "* $\\tt{args}$ and $\\tt{kwargs}$ represent two useful ways of passing parameters to a function that is called by another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "22\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "def runner(f, x, args):\n",
    "    return f(x, *args)*2\n",
    "\n",
    "def func(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "for v in range(3):\n",
    "    print (runner(func, 2, args=(v, 1, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\tt{kwargs}$ works like $\\tt{args}$ except that you have to pass a dictionary with as keys the names of the parameters. Also to expand it you have to use the double-star operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "22\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "def runner(f, x, kwargs):\n",
    "    return f(x, **kwargs)*2\n",
    "\n",
    "for v in range(3):\n",
    "    print (runner(func, 2, kwargs={\"a\":v, \"c\":5, \"b\":1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compute the correlated default probability of each name according to \n",
    "\n",
    "$$Q(t|M) = \\Phi\\left(\\cfrac{\\Phi^{-1}[Q(t)]-\\sqrt{\\rho} M}{\\sqrt{1-\\rho}}\\right)$$\n",
    "\n",
    "* compute the probability to have at least j defaults using the binomial distribution\n",
    "\n",
    "$$Q(l_{t|m} \\ge j) = \\sum_{k=j}^{N}\\left[\\cfrac{N!}{k!(N-k)!}Q(t|M)^k(1-Q(t|M))^{N-k}\\right]$$\n",
    "\n",
    "* create a **credit curve**, $CC(Q)$,  with those probabilities;\n",
    "* re-use the $\\tt{CreditDefaultSwap}$ class methods to compute NPV and breakeven rate (which depend on a discount curve and a previous credit curce), through integration of\n",
    "\n",
    "$$ \\mathrm{NPV}(DC, CC(Q)) = \\int_{-\\infty}^{\\infty}{\\mathrm{NPV}(DC, CC(Q)) f_M(m)dm} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration in Python\n",
    "\n",
    "* To integrate a function $f$ in $\\tt{python}$ we can use the $\\tt{scipy.integrate.quad}$ which takes as input:\n",
    "    * the function to integrate; \n",
    "    * the integration limits;\n",
    "    * optional arguments of the integrand.\n",
    "\n",
    "$$f(x) = \\int_{-2}^{4}(ax^{3} + b)~dx$$\n",
    "\n",
    "* for $a=3$ and $b=5$. \n",
    "\n",
    "$$\\int_{-2}^{4}(ax^{3} + b)~dx = a\\cfrac{x^4}{4} + bx |_{-2}^{4} = 210$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210.0, 2.475694446663456e-12)\n"
     ]
    }
   ],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def func(x, a, b):\n",
    "    return a*x**3 + b\n",
    "\n",
    "s = quad(func, -2, 4, args=(3, 5))\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finmarkets import CreditCurve, CreditDefaultSwap\n",
    "from scipy.stats import norm, binom\n",
    "from numpy import sqrt, exp\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "class BasketDefaultSwaps:\n",
    "    def __init__(self, notional, names, rho, start_date, spread,\n",
    "                 maturity, tenor=3, recovery=0.4):\n",
    "        self.names = names\n",
    "        self.rho = rho\n",
    "        self.cds = CreditDefaultSwap(notional, start_date, spread, maturity,\n",
    "                                     tenor, recovery)\n",
    "        \n",
    "    def npv(self, Q_dates, Q, dc, n_defaults):\n",
    "        v = quad(self.one_factor_model, -np.inf, np.inf,\n",
    "                 args=(self.cds.npv, Q_dates, Q, dc, n_defaults))\n",
    "        return v[0]\n",
    "    \n",
    "    def one_factor_model(self, M, f, Q_dates, Q, dc, j):\n",
    "        DP = norm.cdf((norm.ppf(Q)-sqrt(self.rho)*M)/(sqrt(1-rho)))\n",
    "        b = binom(self.names, DP)\n",
    "        S = 1 - (1-b.cdf(j-1))\n",
    "        cc = CreditCurve(Q_dates, S)\n",
    "        return f(dc, cc)*norm.pdf(M)\n",
    "    \n",
    "    def breakeven(self, Q_dates, Q, dc, n_defaults):\n",
    "        v = quad(self.one_factor_model, -np.inf, np.inf,\n",
    "                 args=(self.cds.breakevenRate, Q_dates, Q, dc, n_defaults))\n",
    "        return v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider a 5-year 3rd-to-default CDS on a basket of 10 reference entities with copula correlation of 0.3 and the expected recovery rate, $R$, is $40\\%$. \n",
    "\n",
    "\n",
    "* The term structure of interest rates is assumed to be flat at 5%. \n",
    "\n",
    "\n",
    "* The default probabilities for the 10 entities are generated by Poisson processes with constant default intensities, $\\lambda_i = 0.01$, so that \n",
    "\n",
    "$$Q(t) = 1 - e^{-\\lambda t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017753900471554232\n"
     ]
    }
   ],
   "source": [
    "from finmarkets import DiscountCurve\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "\n",
    "n_cds = 10\n",
    "rho = 0.3\n",
    "l = 0.01\n",
    "observation_date = date.today()\n",
    "\n",
    "pillar_dates = [observation_date + relativedelta(years=i) for i in range(6)]\n",
    "dfs = [1/(1+0.05)**i for i in range(6)]\n",
    "dc = DiscountCurve(pillar_dates, dfs)\n",
    "Q = [1-np.exp(-(l*t)) for t in range(6)]\n",
    "\n",
    "ndefaults = 3\n",
    "basket = BasketDefaultSwaps(1, n_cds, rho, observation_date, 0.01, 5)\n",
    "print(basket.breakeven(pillar_dates, Q, dc, ndefaults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collateralized Debt Obligation\n",
    "\n",
    "A Collateralized Debt Obligation (CDO) is a credit derivative where the issuer, typically investment banks, gather risky assets and repackage them into discrete classes (*tranches*) based on the level of credit risk assumed by the investor. These tranches of securities become the final investment product.\n",
    "\n",
    "Tranches are named to reflect their risk profile: senior, mezzanine and subordinated/equity and are delimited by the attachment ($L$) and detachment points ($U$), which represent the percentages of the total principal defining their boundaries. \n",
    "For example, a 5-10% tranche has an attachment point of 5% and a detachment point of 10%. Which means that when the accumulated loss of the reference pool is no more than 5% of the total initial notional of the pool, the tranche will not be affected. However, when the loss has exceeded 5%, any further loss will be deducted from the tranche's notional until the detachment point, 10%, is reached.\n",
    "\n",
    "Each of these tranches has a different level of seniority relative to the others in the sense that a senior tranche has coupon\n",
    "and principal payment priority over a mezzanine tranche, while a mezzanine tranche has\n",
    "coupon and principal payment priority over an equity tranche. \n",
    "Indeed they receive returns using a set of rules known as *waterfall*. Incomes of the portfolio are first used to provide returns to the most senior tranche, then to the next and so on.\n",
    "So the senior tranches are generally safest because they have the first claim on the collateral, although they'll offer lower coupon rates.\n",
    "\n",
    "It is important to note\n",
    "that a CDO only redistributes the total risk associated with the underlying pool of assets\n",
    "to the priority ordered tranches. It neither reduces nor increases the total risk associated\n",
    "with the pool.\n",
    "\n",
    "There are various kind of CDOs:\n",
    "\n",
    "* in a **Cash CDO** the reference portfolio consists of corporate bonds owned by the CDO issuer. Cash flows from collateral are used to pay principal and interest to investors. If such cash flows prove inadequate, principal and interest is paid to tranches according to seniority.\n",
    "\n",
    "<img src=\"cdo_structure.png\">\n",
    "\n",
    "* in a **Synthetic CDO** the underlying reference portfolio is no longer a physical portfolio of bonds or loans, instead it is a *fictitious* portfolio consisting of a number of names each with an associated notional amount. The value of a synthetic CDO usually comes from insurance premiums of credit default swaps paid for by investors. The seller assumes the underlying assets will perform. The investor, on the other hand, assumes the underlying assets will default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cash CDO Expected Losses\n",
    "\n",
    "Consider a Cash CDO with a maturity of 1 year, made of 125 bonds. Each bond pays a coupon of one unit after 1 year and it has not yet defaulted (the recovery rate $R$ is assumed 0). We are interested in the following three tranches: equity ([0, 3] defaults), mezzanine ([4, 6] defaults) and senior ([7, 9] defaults), (note that now tranches are identified through the number of defaults and not percentages of the principal). \n",
    "\n",
    "<img src=\"ex_cdo_1.png\">\n",
    "\n",
    "We also assume that the probability of default within 1 year are identical for each bond ($Q$) and that the correlation between each pair is also identical and equal to $\\rho$.\n",
    "\n",
    "Under these assumptions we are in the position to use the Gaussian Copula Model and the derivation of the expected losses results quite simple.\n",
    "\n",
    "The probability of having $l$ defaults, conditional to the market parameter $M$ will follow a binomial distribution given by\n",
    "\n",
    "$$p(l|M) = \\binom{N}{l}Q_M^l (1-Q_M)^{N-l}$$\n",
    "\n",
    "where $N$ is the number of bonds in the portfolio and \n",
    "\n",
    "$$Q_M = \\Phi\\left(\\cfrac{\\Phi^{-1}(Q)-\\sqrt{\\rho}M}{\\sqrt{1-\\rho}}\\right)$$\n",
    "where $\\Phi$ is the standard normal CDF and $Q$ the probability of default within 1 year of a single name.\n",
    "\n",
    "From the definition of each tranche we have that the expected losses are\n",
    "\n",
    "* $\\mathbb{E}(\\textrm{equity loss})=3\\cdot p(l\\ge 3|M) + \\sum_{k=1}^{2}{k\\cdot p(l=k|M)}$\n",
    "* $\\mathbb{E}(\\textrm{mezzanine loss})=3\\cdot p(l\\ge 6|M) + \\sum_{k=1}^{2}{k\\cdot p(l=k+3|M)}$\n",
    "* $\\mathbb{E}(\\textrm{senior loss})=3\\cdot p(l\\ge 9|M) + \\sum_{k=1}^{2}{k\\cdot p(l=k+6|M)}$\n",
    "\n",
    "Each probability $\\mathbb{P}$ can be calculated by integrating the above with respect to $M$.\n",
    "\n",
    "Let's see the corresponding $\\tt{python}$ implementation.\n",
    "First we import the necessary modules and define the needed constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the needed constants\n",
    "\n",
    "N = 125\n",
    "C = 1\n",
    "R = 0\n",
    "q = 0.02\n",
    "tranches = [[1,3],[4, 6],[7,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we define a function $\\tt{p}$ which implements the expected losses for each tranche.\n",
    "The function depends on the parameter $\\tt{M}$, and takes as inputs the correlation $\\tt{rho}$ and the tranche attach-detach limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define p whcih implements the one factor model\n",
    "from scipy.stats import binom, norm \n",
    "import numpy as np\n",
    "\n",
    "def p(M, q, rho, tranche, N):\n",
    "    qM = norm.cdf((norm.ppf(q)-np.sqrt(rho)*M)/(np.sqrt(1-rho)))\n",
    "    b = binom(N, qM)\n",
    "    loss = 3 * (1 - b.cdf(tranche[1]-1))     \n",
    "    for l in tranche:\n",
    "        loss += (l - tranche[0] + 1)*b.pmf(l)\n",
    "    return loss * norm.pdf(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we loop over a range of possible values for the correlation on each tranche to draw the plot of the expected losses vs the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tranches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10394/194721348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tranches' is not defined"
     ]
    }
   ],
   "source": [
    "# loop over tho\n",
    "from scipy.integrate import quad \n",
    "\n",
    "res = [[],[],[]]\n",
    "\n",
    "for i, t in enumerate(tranches):\n",
    "    for rho in np.arange(0, 1.05, 0.05):\n",
    "        if rho == 1.0:\n",
    "            rho = 0.99999\n",
    "        v = quad(p, -np.inf, np.inf,\n",
    "                 args=(q, rho, t, N))\n",
    "        res[i].append(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"losses_vs_rho.png\">\n",
    "\n",
    "Some considerations can be done from these results. First of all, as expected, the equity tranche is the riskier, producing the highest level of loss. The \n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\mathrm{equity})\\ge \\mathbb{E}(\\mathrm{mezzanine}) \\ge \\mathbb{E}(\\mathrm{senior})\n",
    "$$ \n",
    "\n",
    "relation holds only if each tranche has the same notional exposure (in our example 3).\n",
    "\n",
    "Then we can notice that in the equity tranche losses are decreasing in $\\rho$. When the correlation is low indeed the probability to have few defaults is higher than that of many. As the correlation increases, there will be more and more \"simultaneous\" defaults so also other tranches start to suffer losses. In the extreme case of correlation equal to 1 all the tranches are the same (indeed the expected losses curves join together). \n",
    "\n",
    "When considering all the tranches covering the entire number of names, the last tranche (the one with detachment point of 100\\%) is always increasing in $\\rho$. Again this can be explained with the correlated defaults. \n",
    "Also, the total expected losses on the three tranches is independent of $\\rho$. This is not an accident but it is due to the fact that every default scenario is now categorized in one of the plotted tranches while before this was not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic CDO Valuation\n",
    "\n",
    "Imagine a CDO made of $N$ names in the reference portfolio. Each name has the same notional amount and $F$ is the total notional of the CDO.\n",
    "When the $i^{th}$ name defaults, then the portfolio incurs in a loss of $F/N(1-R)$ (the recovery rate is assumed to be fixed for all entities of the portfolio).\n",
    "\n",
    "The tranche loss function $TL^{L,U}(l)$ for a given time $t$ is a function of the number of defaults $l$ occurred up to that time and is given by\n",
    "\n",
    "$$TL_{t}^{L,U}=\\mathrm{max}(\\mathrm{min}(l/N\\cdot F(1-R), U)-L, 0)$$\n",
    "\n",
    "where $l/N\\cdot F(1-R)$ is the total portfolio loss, if it is greater than $U$ then the tranche loss is $U$. Conversely if it is lower than $L$ there is no loss. Now $U$ and $L$ are in percentage of total notional.\n",
    "\n",
    "So for example suppose $L=3\\%$ and $U=7\\%$ and suppose also that the portfolio loss is $l/N\\cdot F(1-R)=5\\%$. Then the tranche loss is 2\\% of the total portfolio notional (or 50\\% of the tranche notional $=7\\%-3\\%=4\\%$).\n",
    "\n",
    "When an investor *sells protection* on a tranche she is guaranteeing to reimburse any realized losses on the tranche to the *protection buyer* (to better understand this concept it is useful to think of the protection as an *insurance*). \n",
    "\n",
    "In return, the protection seller receives a premium at regular intervals (typically every three months) from the protection buyer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premium Leg\n",
    "As seen above the premium leg represents the payments that are done periodically by the protection buyer to the protection seller.\n",
    "\n",
    "These payments are made at the end of each time interval and are proportional to the **remaining notional** in the tranche (this is an important difference with respect to CDS, where the contract ends as soon as a default occurs).\n",
    "\n",
    "We can then write the NPV of the premium leg as\n",
    "\n",
    "$$\\mathrm{NPV}_{\\mathrm{premium}}^{L,U}=S\\sum^{n}_{i=1}D(d_i)\\cfrac{(d_i - d_{i-1})}{360}\\left(F(U-L)-\\mathbb{E}[TL_{d_{i-1}}^{L,U}]\\right)$$\n",
    "where $n$ is the number of payment dates, $D(d_i)$ is the discount factor, $S$ is the annualized premium. The expected value represents the expected notional remaining in the tranche at time \n",
    "$d_{i-1}$.\n",
    "Note that for simplicity we are ignoring that the default may take place at any time between each payment date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Leg\n",
    "The default leg represents the cash flows paid to the protection buyer upon losses occurring in the considered tranche. \n",
    "\n",
    "The NPV of the leg can be expressed as\n",
    "$$\\mathrm{NPV}_{\\mathrm{default}}^{L,U}=\\sum_{i=1}^{n}D(d_i)\\left(\\mathbb{E}[TL_{d_i}^{L,U}]-\\mathbb{E}[TL_{d_{i-1}}^{L,U}]\\right)$$\n",
    "where the argument in parenthesis is the expected losses between time $d_{i-1}$ up to $d_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the key ingredient for the valuation of a CDO is the calculation of $\\mathbb{E}[TL_{d_i}^{L,U}]$ which appears in both legs.\n",
    "Using the Gaussian copula it is relatively easy to compute it. \n",
    "Indeed we know that \n",
    "\n",
    "$$TL_{t}^{L,U}=\\mathrm{max}(\\mathrm{min}(l/NF(1-R), U)-L, 0)$$\n",
    "\n",
    "where the only random variable is the number of defaults $l$. We also know that \n",
    "\n",
    "$$\\mathbb{E}[TL_{t}^{L,U}] = \\sum_{l=0}^{N}TL_{t}^{L,U}\\cdot \\int_{-\\infty}^{\\infty} DP(l_{t|M}) \\phi(M)dM$$\n",
    "\n",
    "And has we have already seen this calculation can be carried on without too much effort.\n",
    "The large popularity of the Gaussian copula just resides in this, it allows to valuate very quickly very complicated contracts like CDOs which usually involve a large number of correlated names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDO Fair Value\n",
    "The *fair value* of a CDO tranche is that value of the premium $S^*$ for which the expected value of the premium leg equals the expected value of the default leg and for what we have seen depends on the expected value of the tranche loss function.\n",
    "\n",
    "$$ S^* = \\cfrac{\\mathrm{NPV_{default}}^{L,U}}{\\sum^{n}_{i=1}D(d_i)\\cfrac{(d_i - d_{i-1})}{360}\\left((U-L)-\\mathbb{E}[TL_{d-1}^{L,U}]\\right)}$$\n",
    "\n",
    "This equation defines the CDO fair value, but can also be used to calibrate the implied correlation parameter from the market.\n",
    "This can be obtained by plugging into the equation the market premium value and solve for the correlation parameter $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finmarkets_tot import DiscountCurve, CreditCurve, generate_swap_dates\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm, binom\n",
    "import numpy as np\n",
    "from numpy import exp, sqrt\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "class CollDebtObligation:\n",
    "    def __init__(self, notional, names, tranches, rho, cc,\n",
    "                 start_date, spreads,\n",
    "                 maturity, tenor=3, recovery=0.4):\n",
    "        self.notional = notional\n",
    "        self.names = names\n",
    "        self.tranches = tranches\n",
    "        self.payment_dates = generate_swap_dates(start_date, maturity * 12, tenor)\n",
    "        self.spreads = spreads\n",
    "        self.rho = rho\n",
    "        self.recovery = recovery\n",
    "        self.cc = cc\n",
    "\n",
    "    def npv(self, tranche, dc):\n",
    "        return self.npv_default(tranche, dc) - self.npv_premium(tranche, dc)\n",
    "\n",
    "    def npv_premium(self, tranche, dc):\n",
    "        L = self.tranches[tranche][0] * self.notional\n",
    "        U = self.tranches[tranche][1] * self.notional\n",
    "        v = 0\n",
    "        for i in range(1, len(self.payment_dates)):\n",
    "            ds = self.payment_dates[i - 1]\n",
    "            de = self.payment_dates[i]\n",
    "            D = dc.df(de)\n",
    "            ETL = self.expected_tranche_loss(ds, L, U)\n",
    "            v += D * (de - ds).days / 360 * ((U - L) - ETL)\n",
    "        return v * self.spreads[tranche]\n",
    "\n",
    "    def npv_default(self, tranche, dc):\n",
    "        U = self.tranches[tranche][1] * self.notional\n",
    "        L = self.tranches[tranche][0] * self.notional\n",
    "        v = 0\n",
    "        for i in range(1, len(self.payment_dates)):\n",
    "            ds = self.payment_dates[i - 1]\n",
    "            de = self.payment_dates[i]\n",
    "            ETL1 = self.expected_tranche_loss(ds, L, U)\n",
    "            ETL2 = self.expected_tranche_loss(de, L, U)\n",
    "            v += dc.df(de) * (ETL2 - ETL1)\n",
    "        return v\n",
    "\n",
    "    def expected_tranche_loss(self, d, L, U):\n",
    "        Q = 1 - self.cc.ndp(d)\n",
    "        v = 0\n",
    "        for l in range(self.names+1):\n",
    "            v += quad(self.one_factor_model, -np.inf, np.inf, \n",
    "                     args=(Q, l, L, U))[0]\n",
    "        return v\n",
    "\n",
    "    def one_factor_model(self, M, Q, l, L, U):\n",
    "        P = norm.cdf((norm.ppf(Q) - sqrt(self.rho) * M) / (sqrt(1 - self.rho)))\n",
    "        b = binom(self.names, P)\n",
    "        return b.pmf(l) * norm.pdf(M) * max(min(l/self.names * \n",
    "                                                self.notional * \n",
    "                                                (1 - self.recovery), U) - L, 0)\n",
    "    \n",
    "    \n",
    "    def fair_value(self, tranche, dc):\n",
    "        num = self.npv_default(tranche, dc)\n",
    "        den = self.npv_premium(tranche, dc) / self.spreads[tranche]\n",
    "        return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to test the class on a 1-years synthetic CDO of 125 names. The default probabilities are 1, 3, 5, and 7% for each year (the correlation is set to 0.3) and tranches are defined as\n",
    "\n",
    "$$[0.0, 0.03], [0.03, 0.06], [0.06, 0.09], [0.09, 1.0]$$\n",
    "\n",
    "with the following premium spreads\n",
    "\n",
    "$$[0.15, 0.07, 0.03, 0.01]$$\n",
    "\n",
    "To spead up the calculation I have set to 12 months the tenor of the premium leg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranche 0 ([0.0, 0.03]): 0.15942\n",
      "Tranche 1 ([0.03, 0.06]): 0.02505\n",
      "Tranche 2 ([0.06, 0.09]): 0.00773\n",
      "Tranche 3 ([0.09, 1.0]): 0.00017\n"
     ]
    }
   ],
   "source": [
    "observation_date = date.today()\n",
    "pillar_dates = [observation_date + relativedelta(years=i) for i in range(5)]\n",
    "dfs = [1 / (1 + 0.05) ** i for i in range(5)]\n",
    "dc = DiscountCurve(observation_date, pillar_dates, dfs)\n",
    "\n",
    "cc = CreditCurve([observation_date + relativedelta(years=i) for i in range(5)],\n",
    "                 [1, 0.99, 0.97, 0.95, 0.93])\n",
    "\n",
    "tranches = [[0.0, 0.03], [0.03, 0.06], [0.06, 0.09], [0.09, 1.0]]\n",
    "spreads = [0.15, 0.07, 0.03, 0.01]\n",
    "\n",
    "cdo = CollDebtObligation(100e6, 125, tranches, 0.3, cc,\n",
    "                         observation_date, spreads, 1, 12)\n",
    "for i in range(len(tranches)):\n",
    "    print (\"Tranche {} ({}): {:.5f}\".format(i, tranches[i], cdo.fair_value(i, dc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
