{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Practical Lesson 8\n",
    "\n",
    "## Overview\n",
    "In this lesson we will see how machine learning techniques can be successfully applied to solve financial problems. We will first do a quick tour on the theory behind neural networks and then we will see an example and two practical applications regarding regression and classification issues.\n",
    "\n",
    "## Neural networks\n",
    "\n",
    "### Definition\n",
    "Artificial Neural Networks (ANN) are information processing models that are developed by inspiring from the working principles of human brain. Their most essential property is the ability of learning from sample sets. The basic process units of ANN architecture are neurons which are internally in connection with other neurons. \n",
    "\n",
    "![Model of an artificial neuron.](neuron.jpeg)\n",
    "\n",
    "A neuron consists of weights ($w_i$) and real ($x_i$) numbers. All inputs injected into neurons are individually weighted, added together and passed into the activation function. There are many different types of activation function but one of the simplest would be step function (another is the sigmoid). \n",
    "\n",
    "![Step function.](step_function.png)\n",
    "![Sigmoid function.](sigmoid.png)\n",
    "\n",
    "The activation function is then responsible to provide the neuron output.\n",
    "\n",
    "### Training of a neuron\n",
    "\n",
    "When teaching children how to recognize a bus, we just tell them, showing an example: “This is a bus. That is not a bus.” until they learn the concept of what a bus is. \n",
    "Furthermore, if the child sees new objects that she hasn’t seen before, we could expect her to recognize correctly whether the new object is a bus or not.\n",
    "This is exactly the idea behind the neurons.\n",
    "Similarly, inputs from a *training* set are presented to the neuron one after the other and weights are modified according to the expected output.\n",
    "\n",
    "When an entire pass through all of the input training vectors is completed the neuron has learnt ! At this time, if an input vector $\\vec{P}$ (already in the training set) is given to the neuron, it will output the correct value. If $\\vec{P}$ is not in the training set, the network will respond with an output similar to other training vectors close to $\\vec{P}$.\n",
    "\n",
    "Unfortunately using just a neuron is not too useful since it is not possible to solve\n",
    "the interesting problems we would like to face with just that simple architecture. The next step is then to put together more neurons in *layers*.\n",
    "\n",
    "### Multi-layered neural networks\n",
    "\n",
    "![A multi-layered neural network.](multilayer.jpeg)\n",
    "\n",
    "Each input from the *input layer* is fed up to each node in the hidden layer, and from there to each node on the output layer. We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer.\n",
    "But to train this network we need a learning algorithm which should be able to tune not only the weights between the output layer and the hidden layer but also the weights between the hidden layer and the input layer. \n",
    "\n",
    "### Back propagation\n",
    "\n",
    "First of all, we need to understand what do we lack. To tune the weights between the hidden layer and the input layer, we need to know the error at the hidden layer, but we know the error only at the output layer (we know the correct output from the training sample and we also know the output predicted by the network.)\n",
    "So, the method that was suggested was to take the errors at the output layer and proportionally propagate them backwards to the hidden layer.\n",
    "\n",
    "So, what we are doing is:\n",
    "\n",
    "* we present a training sample to the neural network (initialized with random weights);\n",
    "* compute the output received by calculating activations of each layer and thus calculate the error;\n",
    "* having calculated the error, we readjust the weights such that the error decreases;\n",
    "* we continue the process for all training samples several times until the weights are not changing too much.\n",
    "\n",
    "The error is computed by the *loss function* and an *optimization function* is then \n",
    "used to choose the appropriate weight values which reduce the loss function (we will use *Adam* as optimizator in the following but there are more).\n",
    "\n",
    "### Neural Network Design\n",
    "\n",
    "There is no rule to guide us into the design of a neural network in terms of number of layers and neuron per layer. The most common strategy is a trail and error one where you finally pick up the solution giving the best accuracy.\n",
    "\n",
    "A common mistake to avoid is to start with a too complex (with many layers and neurons) network which usually leads to *overtraining*. Overtraining is what happens when the NN learns too well the training sample but its performance degrade substantially in an independent testing sample.\n",
    "\n",
    "A NN with just one hidden layer with a number of neurons averaging the inputs and outputs is sufficient in most cases. In the following we will use more complex networks just for illustration, no attempt in optimizing the layout has been done.\n",
    "\n",
    "\n",
    "## Neural net to recognize handwritten digits\n",
    "\n",
    "We don't usually appreciate how tough a problem our visual system solve (consider that it involves 5 visual cortices containing 140 million neurons each), but the difficulties of visual pattern recognition become apparent if you attempt to write a computer program to recognize digits like those below. \n",
    "\n",
    "![The so-called MNIST training sample](mnist_100_digits.png)\n",
    "\n",
    "Simple intuition about how we recognize shapes - \"a 9 has a loop at the top, and a vertical stroke in the bottom right\" - turn out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions and caveats and special cases so that it seems hopeless.\n",
    "\n",
    "Neural networks approach the problem in a different way. The idea is to take a large number of handwritten digits and then develop a system which can learn from those training examples. By increasing the number of training examples, the network can learn more about handwriting, and so improve its accuracy. So while I've shown just 100 training digits above, we could certainly build a better handwriting recognizer by using thousands or even millions or billions of training examples (**remember that neural nets are not so capable of extrapolating results, hence it won't recongnize a digit written in some strange way not included in the training sample !!!**).\n",
    "\n",
    "Let's try to implement an ANN that is capable of recognizing handwritten digits.\n",
    "To start we need to install three new modules, the easiest way of doing that is to run Anaconda on you computers (repl.it is too slow in this case):\n",
    "\n",
    "* open an anaconda-shell and type the following:\n",
    "```pip install keras, mnist, tensorflow```\n",
    "\n",
    "```pip``` is a very useful tool that allows to install new modules to your python libraries.\n",
    "Alternatively using the Anaconda GUI you should be able to install the packages using the *Environment* tab.\n",
    "\n",
    "Our program will be based on a Convolutional Neural Network (CNN, will see later other two types of NN) which is designed for image/pattern recognition. It works essentially by applying on top of an image a series of filters (matrices) that works as edge detectors and with them it classifies the images according to their features.\n",
    "\n",
    "![](edges.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# contains our dataset for training\n",
    "import mnist \n",
    "# keras gives us all the tools to work with NN\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load the training\n",
    "train_images = mnist.train_images() # the actual images\n",
    "train_labels = mnist.train_labels() # the truth (it is a 0, 1, 2...)\n",
    "\n",
    "# transform data for convenience\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "# for technical reasons you need to expand axis\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "\n",
    "# definition of the actual network\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# the input size reflects the size of the image with\n",
    "# the numbers 28x28 pixels\n",
    "model = Sequential([\n",
    "    Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)), \n",
    "    MaxPooling2D(pool_size=pool_size), # reduce the size of the representation\n",
    "                                       # to reduce the size of the parameters\n",
    "    Flatten(), # this step of flattening is necessary to transform a \n",
    "               # 2D matrix into a vector to connect to a classifier\n",
    "    Dense(10, activation=\"softmax\") # softmax is just another type of\n",
    "                                    # activation functions like sigmoid or step func.\n",
    "])\n",
    "# the output is given by 10 neurons returning the \n",
    "# probability that image is in each class.\n",
    "\n",
    "# adam is an algorithm to adjust the weights every cycle\n",
    "# loss function compute the error between the prediction and the truth \n",
    "# metrics which error to use \n",
    "model.compile('adam', loss=\"categorical_crossentropy\",\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "model.fit(train_images,\n",
    "          to_categorical(train_labels),\n",
    "          epochs=3)#,\n",
    "          #validation_data=(test_images, to_categorical(test_labels)))\n",
    "    \n",
    "model.save('digit_training.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see how well our NN predicts MNIST testing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('digit_training.h5')\n",
    "\n",
    "# testing with mnist test sample\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "predictions = model.predict(test_images[:5])\n",
    "print (\"Tesing on MNIST digits...\")\n",
    "print(\"Predicted: \", np.argmax(predictions, axis=1)) \n",
    "print(\"Truth:\", test_labels[:5])\n",
    "print(\"%:\", [\"{:.3f}\".format(p[np.argmax(p)]) for p in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our NN behaves with different kind of digits we will try to check how it works with your own calligraphy.\n",
    "\n",
    "* Open `paint` and create a 280x280 white square\n",
    "* Change brush type and set the maximum size\n",
    "* With the mouse draw a digit\n",
    "* Finally save the file (e.g. five.png)\n",
    "\n",
    "Before passing the image to the NN it has to be resized and this is done with an ad hoc function (`transform_image`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from digit_converter import transform_image\n",
    "\n",
    "model = load_model('digit_training.h5')\n",
    "\n",
    "test_images = np.array(transform_image(\"five.png\"))\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "predict = model.predict(test_images)\n",
    "print (\"\\n\")\n",
    "print (\"Tesing on custom digits...\")\n",
    "print (\"Predicted: \", np.argmax(predict, axis=1))\n",
    "print(\"%:\", [\"{:.3f}\".format(p[np.argmax(p)]) for p in predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those the images I have checked:\n",
    "\n",
    "<img src=\"four.png\" width=80>\n",
    "<img src=\"five.png\" width=80>\n",
    "\n",
    "## Black-Scholes call options\n",
    "\n",
    "The first financial application of a NN concerns the pricing of european call options. \n",
    "In this case I have generated myself a large number of call options with a strike (100) and a maturity (1 year), simulated the underlying development and finally trained the NN using as inputs: volatility, strike, maturity and the underlying. The truth is the price of the call computed using the Black-Scholes formula.\n",
    "\n",
    "![](underlyings.png)\n",
    "\n",
    "The code used for the simulation is in $\\href{https://repl.it/@MatteoSani/exercises8}{\\textrm{bs_simulation.py}}$. I have also simulated two testing samples, one with the parameters included in the training events, and one with parameters outside.\n",
    "The training and testing samples have been stored in a *csv* (comma-separated values) file,\n",
    "which has a particular format very easy to read.\n",
    "\n",
    "In the training I have used a *traditional* NN with an input layer with 4 neurons (the number of inputs), an hidden layer with 10 neurons and an output layer with 1 single neuron (since I need just a number, the price of the call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regression Example \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"bs_training.csv\")\n",
    "X_train = dataset.iloc[:, :4].values\n",
    "Y_train = dataset.iloc[:, 4].values\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=1000, verbose=1)#, batch_size=10)\n",
    "evaluator = model.evaluate(X_train, Y_train)\n",
    "print('Test: {}'.format(evaluator))\n",
    "\n",
    "model.save('bs_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how the NN behaves with the two testing samples. First the one generated with parameters in the training phase space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model('bs_model.h5')\n",
    "\n",
    "dataset = pd.read_csv(\"bs_testing.csv\")\n",
    "X_test = dataset.iloc[:, :4].values\n",
    "Y_test = dataset.iloc[:, 4].values\n",
    "\n",
    "plt.plot(model.predict(X_test), color=\"red\", label=\"NN price\")\n",
    "plt.plot(Y_test, label=\"BS price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"comparison_fair.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement is pretty good. To illustrate how a neural network is not able to extrapolate results if the prediction is tried with inputs outside the phase-space of the training (i.e. testing sample different from the one used in the training) I have tried to predict the price of a call with different maturity (strike and vol are in the range of the training instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model('bs_model.h5')\n",
    "\n",
    "dataset = pd.read_csv(\"bs_testing_off.csv\")\n",
    "X_test = dataset.iloc[:, :4].values\n",
    "Y_test = dataset.iloc[:, 4].values\n",
    "\n",
    "plt.plot(model.predict(X_test), color=\"red\", label=\"NN price\")\n",
    "plt.plot(Y_test, label=\"BS price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"comparison_off.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Analysis\n",
    "In finance, technical analysis is a security analysis discipline for forecasting the direction of prices through the study of past market data, primarily price and volume.\n",
    "Essentially the analyst looks for particular patterns in the price time series that are *known* to develop in predictable ways to take profit of it.\n",
    "\n",
    "<img src=\"H_and_s_top_new.jpg\" width=400>\n",
    "<img src=\"Triangle-ascending.jpg\" width=400>\n",
    "\n",
    "As you may imagine we will try to develop a CNN (like in the handwriting case) capable of classifying features in time series to be used in a technical analysis (this is much faster than having somebody looking at thousands of time series by eye...).\n",
    "\n",
    "As in the previous application I have generated myself the training set simulating 21600 time series (1/3 with head and shoulder patter, 1/3 with triangle pattern and 1/3 with no pattern). *To make the training easier the features have been exagerated.*\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_1.png\" width=300>\n",
    "<figcaption>No pattern</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_2.png\" width=300>\n",
    "<figcaption>Head and shoulder pattern</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_0.png\" width=300>\n",
    "<figcaption>Tringle pattern</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv1D, Dropout, MaxPooling1D, Flatten, GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load the training set\n",
    "with open(\"training_techana_labels.json\", \"r\") as f:\n",
    "    train_labels = json.load(f)\n",
    "train_labels = train_labels[:3000]\n",
    "train_images = []\n",
    "\n",
    "with open(\"training_techana_images.json\", \"r\") as f:\n",
    "    train_images = json.load(f)\n",
    "train_images = train_images[:3000]\n",
    "train_images = np.array(train_images)\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "\n",
    "# define the CNN \n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=80, kernel_size=20, \n",
    "                 activation='relu', input_shape=(101, 1)))\n",
    "model.add(Conv1D(filters=80, kernel_size=15, \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(filters=100, kernel_size=10, \n",
    "                 activation='relu'))\n",
    "model.add(Conv1D(filters=100, kernel_size=5, \n",
    "                 activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# make the training\n",
    "model.fit(train_images, to_categorical(train_labels), \n",
    "          epochs=80, batch_size=35, verbose=2)\n",
    "\n",
    "model.save('techana.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the perfomance I have created a longer time series and passed as input to the CNN a sliding time window to simulate the evolution of the price and a feature that is coming.\n",
    "The goal is to check when the neural net is capable of predicting the incoming pattern.\n",
    "\n",
    "<img src=\"closing_price.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "test_images = []\n",
    "\n",
    "with open(\"testing_techana_frames.json\", \"r\") as f:\n",
    "    test_images = json.load(f)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "#for i in range(test_images.shape[0]):\n",
    "#    plt.plot(test_images[i, :])\n",
    "#    plt.show()\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "model = load_model('techana.h5')\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "for i in range(len(predictions)):\n",
    "    print (np.argmax(predictions[i]), [\"{:.2f}\".format(p) for p in predictions[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at the 6th sample the CNN start recognizing the *head and shoulder* pattern in the price evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 8.1\n",
    "Try to repeat all the example shown during the lesson.\n",
    "\n",
    "### Exercise 8.2\n",
    "Using the same code illustrated above, test the ANN to recognize digits with your own handwriting (e.g. try to exagerate some feature to fool the NN, or even pass it letters instead of digits and interpret the results).\n",
    "\n",
    "### Exercise 8.3\n",
    "Taking as example the pricing NN trained on call, try to price put options.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
