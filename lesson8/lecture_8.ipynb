{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Overview\n",
    "In this lesson we will see how machine learning techniques can be successfully applied to solve financial problems. We will first do a quick tour on the theory behind neural networks and then we will see an example and two practical applications regarding regression and classification issues. \n",
    "\n",
    "**Disclaimer**: this lecture just scratches the surface of the machine learning topic which has seen a huge development in the latest years leading to thousands of applications in many different fields.\n",
    "\n",
    "## Neural network Definition\n",
    "Artificial Neural Networks (ANN or simply NN) are information processing models that are developed by inspiring from the working principles of human brain. Their most essential property is the ability of learning from sample sets. \n",
    "\n",
    "The basic unit of ANN architecture are neurons which are internally in connection with other neurons. \n",
    "\n",
    "![Model of an artificial neuron.](neuron.jpeg)\n",
    "\n",
    "A neuron consists of weights ($w_i$) and real numbers ($x_i$). All inputs injected into a neuron are individually weighted, added together and passed into the activation function which produce the neuron output. There are many different types of activation function but one of the simplest is the *step function* which returns just 0 or 1 according to the input value (another is the *sigmoid* which can be thought of as the continuous version of the step function). \n",
    "\n",
    "![Step function.](step_function.png)\n",
    "![Sigmoid function.](sigmoid.png)\n",
    "\n",
    "\n",
    "## Training of a neuron\n",
    "\n",
    "When teaching children how to recognize a bus, we just tell them, showing an example: “This is a bus. That is not a bus.” until they learn the concept of what a bus is. \n",
    "Furthermore, if the child sees new objects that she hasn’t seen before, we could expect her to recognize correctly whether the new object is a bus or not.\n",
    "\n",
    "This is exactly the idea behind neurons.\n",
    "Similarly, inputs from a *training* set are presented to the neuron one after the other together with the correct output and the neuron weights are modified accordingly.\n",
    "\n",
    "When an entire pass through all of the input training vectors is completed the neuron has learnt ! \n",
    "\n",
    "At this time, if an input vector $\\vec{P}$ (already in the training set) is given to the neuron, it will output the correct value. If $\\vec{P}$ is not in the training set, the network will respond with an output similar to other training vectors close to $\\vec{P}$.\n",
    "\n",
    "Unfortunately using just a neuron is not too useful since it is not possible to solve\n",
    "the interesting problems we would like to face with just that simple architecture. The next step is then to put together more neurons in *layers*.\n",
    "\n",
    "### Multi-layered neural networks\n",
    "\n",
    "![A multi-layered neural network.](multilayer.jpeg)\n",
    "\n",
    "Each input from the *input layer* is fed up to each node in the next hidden layer, and from there to each node on the output layer. We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "But to train this network we need a learning algorithm which should be able to tune not only the weights between the output layer and the hidden layer but also the weights between the hidden layer and the input layer. \n",
    "\n",
    "### Back propagation\n",
    "\n",
    "In order to tune the weights at each layer at every iteration we should know what the output would be at each node. But this is not possbile since the only value we know is the correct output at the last node (the final output of the NN which can be compared to our truth of the training output).\n",
    "The method that was suggested to overcome the issue was to take the errors at the output layer (last node) and proportionally propagate it backwards to all the hidden layer.\n",
    "\n",
    "So, what it's going to happen is:\n",
    "\n",
    "* present a training sample to the neural network (initialized with random weights);\n",
    "* compute the output received by calculating activations of each layer and thus calculate the error as the difference between the NN output and the training sample expected result;\n",
    "* having calculated the error, readjust the weights such that the error (difference) decreases;\n",
    "* continue the process for all training samples several times until the weights are not changing too much (a.k.a. the process converged).\n",
    "\n",
    "The NN error is computed by the *loss function* (usually it is either the mean squared error or the mean absolute error) and an *optimization function* is then \n",
    "used to choose the appropriate weight values in order to reduce the loss function value (we will use *Adam* as optimizator in the following but there are more).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Neural Network Design\n",
    "\n",
    "There is no rule to guide developer into the design of a neural network in terms of number of layers and neuron per layer. The most common strategy is a trail and error one where you finally pick up the solution giving the best accuracy. In general a larger number of nodes is better to catch highly structured data with a lot of feature although it may require larger training sample to work correctly.\n",
    "\n",
    "A common mistake to avoid is to *overtrain* a NN. Overtraining is what happens when the NN learns too well the training sample but its performance degrade substantially in an independent testing sample. \n",
    "\n",
    "So usually it is required to split the available sample in two parts training and testing (e.g. 80% and 20%) and to use the former to perform the training and the latter to cross-check the performance. **Usually performance are *measured* with the mean square error computed between the truth of the sample and the NN predictions.**\n",
    "\n",
    "Anyway as a rule of thumb a NN with just one hidden layer with a number of neurons averaging the inputs and outputs is sufficient in most cases. In the following we will use more complex networks just for illustration, no strong attempt in optimizing the layout has been done though.\n",
    "\n",
    "## Practical Examples\n",
    "\n",
    "Below it will be illustrated few practical applications of neural network trainings in python. \n",
    "Various modules are available to develop neural network in ```python```, we will\n",
    "use ```Keras``` a relatively high level library which in turn use ```TensorFlow``` a very famous machine learning tool developed by Google.\n",
    "\n",
    "In the attempt of keeping things as simple as possible I have added another layer above ```Keras```, ```FinNN``` so that you can try \n",
    "to design some NN without caring too much about the many details and parameters that are involved in the process.\n",
    "\n",
    "### Function approximation \n",
    "\n",
    "As a first practical example let's try to design an ANN which is capable of learning the functional form underlying a set of data.\n",
    "\n",
    "Let's generate a sample with $x$ (input), $f(x)$ (truth) pairs where $f(x) = x^3 +2$ and let's start to code the NN structure. \n",
    "\n",
    "We start by importing the necessary modules.\n",
    "Then we generate the training sample (i.e. the $x$, $f(x)$ pairs) and apply a simple transformation on the sample in order to have all the inputs and outputs in the $[0, 1]$ range. This is usually done to provide the NN with *normalized* data, infact the NN can be fooled by large or very small numbers giving unstable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of original data  -2.0 1.9949999999999148 -6.0 9.940149874998983\n",
      "The same data after the normalization  0.0 1.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "from finnn import FinNN\n",
    "from numpy import arange, array\n",
    "\n",
    "# define the dataset\n",
    "x = array([i for i in arange(-2, 2, 0.005)])\n",
    "\n",
    "y = array([i**3+2 for i in x])\n",
    "print(\"Distribution of original data \", x.min(), x.max(), y.min(), y.max())\n",
    "\n",
    "trainer = FinNN()\n",
    "trainer.setData(x, y)\n",
    "\n",
    "#trainer.normalize()\n",
    "\n",
    "# here you should see that x and y are between 0 and 1\n",
    "print(\"The same data after the normalization \", trainer.x.min(), \n",
    "      trainer.x.max(), trainer.y.min(), trainer.y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can define the structure of the neural network. There is no predefined rule to decide the number of layers and nodes you need to go by trial and error. Here the problem is quite simple so there is no need to use a complecated NN. \n",
    "\n",
    "In the end I have decided to use two layers with 10 nodes each and a *sigmoidal* activation function. The input_dim parameter has to be set to 1 since we have just one single input, the $x$ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "799/799 [==============================] - 0s 127us/step - loss: 0.0919\n",
      "Epoch 2/800\n",
      "799/799 [==============================] - 0s 20us/step - loss: 0.0366\n",
      "799/799 [==============================] - 0s 12us/step - loss: 5.3795e-05\n",
      "Epoch 799/800\n",
      "799/799 [==============================] - 0s 15us/step - loss: 5.0436e-05\n",
      "Epoch 800/800\n",
      "799/799 [==============================] - 0s 20us/step - loss: 5.3768e-05\n"
     ]
    }
   ],
   "source": [
    "# design the neural network model\n",
    "trainer.addInputLayer(1, 15, 'tanh')\n",
    "trainer.addHiddenLayer(5, 'tanh')\n",
    "trainer.addOutputLayer(1)\n",
    "\n",
    "# define the loss function (mean squared error) and optimization algorithm (Adam)\n",
    "trainer.compileModel('mse', 'adam')\n",
    "\n",
    "# fit the model on the training dataset\n",
    "# using 500 epochs, a batch_size of 10\n",
    "trainer.fit(800, 100, verbose=1)\n",
    "\n",
    "# make predictions for the input data\n",
    "trainer.fullPrediction()\n",
    "\n",
    "# invert the previous transformation to get back the real data and not the normalized one\n",
    "trainer.reverseNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is completed we can evaluate how good it is. To do this we can compute the residuals or the square root of the sum of the squared difference between the true value and the one predicted by the NN. We will also plot the true function and the predicted one in order to have a graphical representation of the goodness of our training.\n",
    "To have a numerical estimate of the agreement it has been computed also the *mean squared error* defined as:\n",
    "\n",
    "$MSE = \\frac{\\sum_{i=1}^n{\\big(\\frac{x_{i}^{pred} - x_i^{truth}}{x_i^{truth}}\\big)^2}}{n}$\n",
    "\n",
    "A *perfect* prediction would lead to $MSE=0$ so the lower this number the better the agreement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# report model error computing the mean squared error\n",
    "print('MSE: %.3f' % mean_squared_error(trainer.y, trainer.predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what it is going on in the picture below are shown the actual function we want to approximate and different predictions of our NN obtained with four epoch numbers (5, 100, 800, 5000).\n",
    "\n",
    "<img src=\"training_vs_epoch.png\">\n",
    "\n",
    "It is clear how the agreement improves with higher number of epochs which means that the NN has more opportunities to adapt the weights and reduce the loss (or error or distance) to the target values. Even in the case of 5000 epochs zooming in you could see discrepancies not visible at the scale of the plot. Clearly increasing further the number of epochs may lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1\n",
    "\n",
    "In order to see how different parameter choices affect the training (both looking at a plot like the one before and the the $MSE$) try to:\n",
    "\n",
    "* reduce the number of points used in the training (change the step from 0.1 to 1 or to 0.01 in $\\tt{x = arange(-50, 51, 0.1))}$, expect worse results with less points;\n",
    "* change the number of nodes per layer;\n",
    "* change the activation function from 'sigmoid' to 'relu';\n",
    "* change the number of epochs, this is the number of times the neural network will process the sample data to improve the training; setting verbose to 1 will show the progress with an estimate of the goodness of the training after each epoch; expect worse training with less epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Black-Scholes call options\n",
    "\n",
    "The first financial application of a NN concerns the pricing of european call options: essentially we will create a neural network capable of approximate the famous Black-Scholes pricing formula\n",
    "\n",
    "$$ P_\\textrm{call} = F_\\textrm{BS}(K, r, \\sigma, ttm)$$\n",
    "\n",
    "Like before we are going to generate the training sample this time made of a grid of volatility-rate pairs $(\\sigma, r)$ (for simplicity we are going to set moneyness and time to maturity to 1). The truth values are the price of a call computed using the pricing function in the $\\tt{finmarkets.py}$ library with the corresponding inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finmarkets import call\n",
    "\n",
    "data = []\n",
    "rates = np.arange(0.01, 0.11, 0.001)\n",
    "sigmas = np.arange(0.1, 0.6, 0.005)\n",
    "\n",
    "for r in rates:\n",
    "    for sigma in sigmas:\n",
    "        call_price = call(1, r, sigma, 1)\n",
    "        data.append([r, sigma, call_price])\n",
    "        \n",
    "# we transform the list to a numpy array just because \n",
    "# an array it is more convenient to use later\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it takes some time to generate data samples, it is always advisable to save them in a file since we may need to load it many times during the NN development.\n",
    "This can be done with $\\tt{pandas}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['vol'] = data[:, 1]\n",
    "df['rate'] = data[:, 0]\n",
    "df['price'] = data[:, 2]\n",
    "\n",
    "df.to_csv(\"bs_training_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previous example we will use the $\\tt{FinNN}$ utility class to develop the NN and also we will *normalize* data to get better results.\n",
    "**Beware that this time we have TWO input parameters (rate and volatilty)** and not just one.\n",
    "\n",
    "Furthermore we will also split the generated sample into a training and a testing part so that we could later check for overfitting by comparing the perfomance in the two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first load back data\n",
    "import pandas as pd\n",
    "from finnn import FinNN\n",
    "\n",
    "data =  pd.read_csv(\"bs_training_sample.csv\")\n",
    "\n",
    "x = data.iloc[:, 1:3].values\n",
    "y = data.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.0101\n",
      "Epoch 2/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0039\n",
      "Epoch 1998/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.3043e-07\n",
      "Epoch 1999/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.5889e-07\n",
      "Epoch 2000/2000\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.1917e-06\n",
      "8000/8000 [==============================] - 0s 16us/step\n",
      "Training: 9.342097100670799e-07\n",
      "2000/2000 [==============================] - 0s 14us/step\n",
      "Test: 9.361866041217582e-07\n"
     ]
    }
   ],
   "source": [
    "trainer = FinNN()\n",
    "\n",
    "trainer.setData(x, y, 0.20)\n",
    "# the last parameter tells the class to split the original\n",
    "# sample in training (80%) and testing parts (20%)\n",
    "trainer.normalize()\n",
    "\n",
    "# define the NN architecture\n",
    "trainer.addInputLayer(2, 20, 'tanh')\n",
    "trainer.addHiddenLayer(8, 'tanh')\n",
    "trainer.addOutputLayer(1)\n",
    "        \n",
    "# define loss and optimizer algorithms\n",
    "trainer.compileModel('mse', 'adam')\n",
    "    \n",
    "# run the training\n",
    "# this time we are using many more epochs \n",
    "# and a larger batch_size\n",
    "trainer.fit(2000, 500, 1)\n",
    "\n",
    "# here we compare the performance \n",
    "# on the training and test sample\n",
    "trainer.evaluate()\n",
    "\n",
    "# when the training takes some time it is useful\n",
    "# to save the model weights in a file to use it later on\n",
    "trainer.saveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training and test samples give roughly the same $MSE$ value so we are reasonably sure that there hasn't been *overfitting*.\n",
    "After the training is completed again we can evaluate graphically how good it is.\n",
    "<img src=\"vol_rate.png\">\n",
    "We can also compare the prediction in a practical case; let's say we want to know the price of a call (with moneyness 1 and time to maturity 1 year) when the interest rate is 0.015 and the volatility 0.234:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.234, 0.015]] => 0.0999 (expected 0.1001)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from finmarkets import call\n",
    "\n",
    "# here we load the trained model\n",
    "trainer.loadModel('test')\n",
    "\n",
    "# this is our input vector\n",
    "rv = np.array([[0.234, 0.015]])\n",
    "# here we compare the predection with the BS call price                 \n",
    "print ('{} => {:.4f} (expected {:.4f})'.format(rv.tolist(), \n",
    "                                        trainer.predict(rv)[0][0], \n",
    "                                        call(1, rv[0][1], rv[0][0], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very import to remeber that a **NN cannot estrapolate**. Indeed if you try to predict the price of a call from rate and volatility outside the training *phase space* (with values that aren't in the intervals used in the training), say $r = 0.22$ and $\\sigma = 0.01$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 0.22]] => 0.1709 (expected 0.1975)\n"
     ]
    }
   ],
   "source": [
    "# this is our input vector\n",
    "rv = np.array([[0.01, 0.22]])\n",
    "                 \n",
    "# here we compare the predection with the BS call price                 \n",
    "print ('{} => {:.4f} (expected {:.4f})'.format(rv.tolist(), \n",
    "                                        trainer.predict(rv)[0][0], \n",
    "                                        call(1, rv[0][1], rv[0][0], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "The function approximation of a neural network can serve other scopes rather than predicting prices. An very useful application is indeed *model calibration* which consists of deriving parameters of a model directly from market values. This is especially convenient to estimate parameters (e.g. volatility) which are otherwise complicated to compute.\n",
    "\n",
    "Assume we need to estimate the implied volatilty of a stock price in real time. If in the market are available call options with out stock as underlying we can exploit again the Black and Scholes formula. The idea is in fact to train a NN where the input is a list of price, moneyness, rate and time to maturity $(P_\\textrm{call}, K, r, ttm)$ and the target output is the volatility derived from the inversion of the call option pricing formula\n",
    "\n",
    "$$ \\sigma = F^{-1}_\\textrm{BS}(P_\\textrm{call}, K, r, ttm)$$\n",
    "\n",
    "We can than calibrate our model by predicting the stock volatility with the trained NN using as input the market price of the option and its characteristics.\n",
    "\n",
    "### Historical vs. Implied Volatility\n",
    "\n",
    "Historical volatility is the realized volatility of the underlying asset over a previous time period. It is determined by measuring the standard deviation of the underlying asset from the mean during that time period.\n",
    "\n",
    "Standard deviation is a statistical measure of the variability of price changes from the mean price change. This estimate differs from the Black-Scholes method's implied volatility, as it is based on the actual volatility of the underlying asset. However, using historical volatility also has some drawbacks. Volatility shifts as markets go through different regimes. Thus, historical volatility may not be an accurate measure of future volatility. Implied volatility takes into account all of the information used by market participants to determine prices in the options market, instead of just past prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we can reuse the training sample created before (again we are going to set $T=1$ and $K=1$). Clearly now $\\tt{x}$ will be pairs of rate and price and $\\tt{y}$ the volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01       0.04485236]\n",
      " [0.01       0.04682511]\n",
      " [0.01       0.04879844]\n",
      " ...\n",
      " [0.109      0.27351542]\n",
      " [0.109      0.27529369]\n",
      " [0.109      0.27707117]]\n",
      "[0.1   0.105 0.11  ... 0.585 0.59  0.595]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from finnn import FinNN\n",
    "\n",
    "data =  pd.read_csv(\"bs_training_sample.csv\")\n",
    "\n",
    "x = data.iloc[:, 2:4].values\n",
    "y = data.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 1.7378\n",
      "Epoch 2/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.7167\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.0864e-06\n",
      "Epoch 1997/2000\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 1.4929e-06\n",
      "Epoch 1998/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 1.4073e-06\n",
      "Epoch 1999/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.7558e-06\n",
      "Epoch 2000/2000\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 1.5793e-06\n",
      "8000/8000 [==============================] - 0s 15us/step\n",
      "Training: 1.9411823022892348e-06\n",
      "2000/2000 [==============================] - 0s 14us/step\n",
      "Test: 1.997178464080207e-06\n"
     ]
    }
   ],
   "source": [
    "trainer = FinNN()\n",
    "\n",
    "trainer.setData(x, y, 0.20)\n",
    "trainer.normalize()\n",
    "\n",
    "trainer.addInputLayer(2, 20, 'tanh')\n",
    "trainer.addHiddenLayer(8, 'tanh')\n",
    "trainer.addOutputLayer(1)\n",
    "\n",
    "trainer.compileModel('mse', 'adam')\n",
    "    \n",
    "trainer.fit(2000, 500, 1)\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.saveModel(\"calibration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided our training includes the correct range of market prices of our call we can quickly and easily estimate the volatility, for example if the risk-free rate is 2% and the current price is 0.15 (remember that we are using the BS formula in terms of moneyness, strike divided by underlying price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02, 0.15]] => 0.3554 (expected call price 0.1498)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from finmarkets import call\n",
    "\n",
    "trainer.loadModel('calibration')\n",
    "\n",
    "rv = np.array([[0.02, 0.15]])\n",
    "\n",
    "print ('{} => {:.4f} (expected call price {:.4f})'.format(rv.tolist(), \n",
    "                                        trainer.predict(rv)[0][0], \n",
    "                                        call(1, 0.02, trainer.predict(rv)[0][0], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neural net to recognize handwritten digits\n",
    "\n",
    "We don't usually appreciate how tough a problem our visual system solve, maybe it is enough to consider that it involves 5 visual cortices containing 140 million neurons each. \n",
    "However the difficulties of visual pattern recognition become apparent if you attempt to write a computer program to recognize digits like those below\n",
    "\n",
    "<img src=\"mnist_100_digits.png\">\n",
    "\n",
    "Simple intuition about how we recognize shapes (e.g. a 9 has a loop at the top, and a vertical stroke in the bottom right) turns out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions and caveats and special cases so that it seems hopeless.\n",
    "\n",
    "Neural networks approach the problem in a different way. The idea is to take a large number of handwritten digits and then develop a system which can learn from those. \n",
    "\n",
    "By increasing the number of training examples, the network can learn more and more about handwriting, and so improve its accuracy. So while it has been shown just 100 training digits above, we could certainly build a better handwriting recognizer by using thousands or even millions or billions of training examples (**as we have seen above neural nets are not capable of extrapolating results, hence it won't recongnize a digit written in some strange way not included in the training sample !!!**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement a NN that is capable of recognizing handwritten digits.\n",
    "To start we need to install another module, $\\tt{mnist}$ which containes various predefined training samples.\n",
    "\n",
    "Our program will be based on a slightly different kind of neural network, one type specifically designed for image/pattern recognition, the Convolutional Neural Network (CNN). We won't go in the details of its implementation since it is outside the scope of these lectures but it works essentially by applying on top of an image a series of filters (*convolutional layers*) that works as edge detectors. With them it classifies the images according to their relevant features.\n",
    "\n",
    "Convolutional layers prove very effective, and stacking them allows to learn low-level features (e.g. lines) and high-order or more abstract features, like shapes or specific objects.\n",
    "\n",
    "<img src=\"edges.jpg\">\n",
    "\n",
    "Another important difference with respect to the previous examples is that in this case we are going to solve a classification problem (contrary to before when we were trying to regress a sample or in other word to approximate a function). Indeed our NN output won't be a single number but rather a list containing the probabilties that an image belong to class on the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# contains our dataset for training\n",
    "import mnist \n",
    "\n",
    "from finnn import FinNN\n",
    "\n",
    "# load the training\n",
    "train_images = mnist.train_images() # the actual images\n",
    "train_labels = mnist.train_labels() # the truth (it is a 0, 1, 2...)\n",
    "\n",
    "# 0 means do not split the sample in training and testing sets\n",
    "# (MNIST has already dont it for us)\n",
    "# the last parameter tells FinNN class that we are going to develop a CNN\n",
    "trainer = FinNN(\"CNN2D\")\n",
    "trainer.setData(train_images, train_labels)\n",
    "trainer.normalize()\n",
    "\n",
    "# for technical reasons you need to expand axis\n",
    "trainer.x = np.expand_dims(trainer.x, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "59999/59999 [==============================] - 12s 199us/step - loss: 2.3481\n",
      "Epoch 2/5\n",
      "59999/59999 [==============================] - 12s 198us/step - loss: 0.3692\n",
      "Epoch 3/5\n",
      "59999/59999 [==============================] - 12s 197us/step - loss: 0.2321\n",
      "Epoch 4/5\n",
      "59999/59999 [==============================] - 12s 201us/step - loss: 0.1950\n",
      "Epoch 5/5\n",
      "59999/59999 [==============================] - 12s 200us/step - loss: 0.1856\n"
     ]
    }
   ],
   "source": [
    "# define our convolutional NN\n",
    "# we decide to apply 8 filters to the images \n",
    "# each with 3x3 pixels size\n",
    "# the input images have 28x28 pixels size instead\n",
    "trainer.addConv2DLayer(8, 3, (28, 28, 1))\n",
    "\n",
    "# the output is given by 10 neurons returning the \n",
    "# probability that image is in each class.\n",
    "trainer.addCNNOutputLayer(10)\n",
    "        \n",
    "# adam is an algorithm to adjust the weights every cycle\n",
    "# loss function compute the error between the prediction and the truth \n",
    "# metrics which error to use \n",
    "trainer.compileModel('categorical_crossentropy', 'adam')\n",
    "\n",
    "trainer.fit(5)\n",
    "#validation_data=(test_images, to_categorical(test_labels)))\n",
    "    \n",
    "trainer.saveModel('digit_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the most curious students\n",
    "\n",
    "If you look closely to the `finnn.py` module you will notice that I have cheated when\n",
    "describing the CNN architecture. In particular I have not mentioned the `MaxPooling2D`\n",
    "layer, so let's clarify its feature.\n",
    "\n",
    "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input.\n",
    "\n",
    "A limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n",
    "\n",
    "Imagine a program that look for car plates in pictures taken by a speed radar, cars won't\n",
    "be in the same position in the frame so there may be differences in the classification \n",
    "of similar (but not equal) pictures.\n",
    "\n",
    "A common approach to address this problem from signal processing is called *down sampling*. This is where a lower resolution version of an input signal (e.g. the picture) is created that still contains the large or important structural elements, without the fine detail that may not be as useful to the task.\n",
    "\n",
    "Down sampling can be achieved using a pooling layer.\n",
    "\n",
    "Pooling involves selecting a pooling operation, much like a filter to be applied to feature maps. The size of the pooling operation or filter is smaller than the size of the feature map; specifically, it is almost always 2×2 pixels.\n",
    "This means that the pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved. For example, a pooling layer applied to a feature map of 6×6 (36 pixels) will result in an output pooled feature map of 3×3 (9 pixels).\n",
    "\n",
    "The most common pooling operation are:\n",
    "* Average Pooling: calculate the average value for each patch on the feature map;\n",
    "* Maximum Pooling (or Max Pooling): calculate the maximum value for each patch of the feature map.\n",
    "\n",
    "\n",
    "Now let's try to see how well our NN predicts MNIST testing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesing on MNIST digits...\n",
      "Predicted:  [7 2 1 0 4 1 4 9 5 9]\n",
      "Truth: [7 2 1 0 4 1 4 9 5 9]\n",
      "highest prob.: ['0.999999', '0.999928', '0.999889', '1.000000', '1.000000', '0.999999', '0.999977', '0.999983', '0.931519', '0.999535']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "trainer.loadModel('digit_training')\n",
    "\n",
    "# testing with mnist test sample\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "predictions = trainer.predict(test_images[:10])\n",
    "print (\"Tesing on MNIST digits...\")\n",
    "print(\"Predicted: \", np.argmax(predictions, axis=1)) \n",
    "print(\"Truth:\", test_labels[:10])\n",
    "\n",
    "# this line returns the highest probability of the vector\n",
    "print(\"highest prob.:\", [\"{:.6f}\".format(p[np.argmax(p)]) for p in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the last but one digit has lower probability let's check the returned list to see which other number have non-zero probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9th digit: ['dig 0: 0.000023', 'dig 1: 0.000000', 'dig 2: 0.000019', 'dig 3: 0.000000', 'dig 4: 0.000001', 'dig 5: 0.931519', 'dig 6: 0.047921', 'dig 7: 0.000000', 'dig 8: 0.020517', 'dig 9: 0.000000']\n"
     ]
    }
   ],
   "source": [
    "print(\"9th digit:\", [\"dig {}: {:.6f}\".format(i, p) for i, p in enumerate(predictions[8])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the second ranked digit is a 6 (which can be confused with a five if the lower loop is almost closed). I am not sure how a 5 could be confused with a 8 though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our NN behaves with different kind of digits we will try to check how it works with my calligraphy (as homework try to repeat the exercise using your own digit following the instructions given below).\n",
    "\n",
    "* Open $\\tt{paint}$ and create a 280x280 white square\n",
    "* Change brush type and set the maximum size\n",
    "* With the mouse draw a digit\n",
    "* Finally save the file (e.g. five.png)\n",
    "\n",
    "Before passing the image to the NN it has to be resized and this is done with an ad-hoc function ($\\tt{transform\\_image}$) which is in the $\\tt{digit\\_converter.py}$ module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tesing on custom digits...\n",
      "Predicted:  [4]\n",
      "%: ['1.000']\n",
      "\n",
      "\n",
      "Tesing on custom digits...\n",
      "Predicted:  [5]\n",
      "%: ['0.991']\n"
     ]
    }
   ],
   "source": [
    "from digit_converter import transform_image\n",
    "\n",
    "filenames = ['four.png', 'five.png']\n",
    "\n",
    "for f in filenames:\n",
    "    test_images = np.array(transform_image(f))\n",
    "    test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "    predict = trainer.predict(test_images)\n",
    "    print (\"\\n\")\n",
    "    print (\"Tesing on custom digits...\")\n",
    "    print (\"Predicted: \", np.argmax(predict, axis=1))\n",
    "    print(\"%:\", [\"{:.3f}\".format(p[np.argmax(p)]) for p in predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those the images I have checked:\n",
    "\n",
    "<img src=\"four.png\" width=80>\n",
    "<img src=\"five.png\" width=80>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration cont.\n",
    "\n",
    "When the parameter(s) of our model we need to calibrate can be expressed as a function of three variables the CNN can be used. \n",
    "\n",
    "Consider again the Black and Scholes formula for the call options. Assume you need to calibrate the rate $r$ and the volatity $\\sigma$. A convolutiona neural network can be trained fed with special images which represents $ttm, K$ and $P_\\textrm{call}$.\n",
    "\n",
    "A black-white image indeed can be interpreted as a map where each pixel is a pair ($ttm, K)$ and the pixel color, an integer between 0 (black) and 255 (white), represents $P_\\textrm{call}$. As in the previous examples the neural network was classifing the pictures into digits, now it will assign them to classes identified by $r, \\sigma$ pairs.\n",
    "\n",
    "The creation of the training sample is a little more complicated now. For convenience we will use also a new format to save data image, $\\tt{json}$. This will be done through the corresponding module simply using the functions $\\tt{dump}$ and $\\tt{load}$ to store and retrieve data.\n",
    "The module $\\tt{PIL}$ (pillow) is instead used to visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, json\n",
    "import pandas as pd\n",
    "\n",
    "# read rates and vols\n",
    "data =  pd.read_csv(\"bs_training_sample.csv\")\n",
    "\n",
    "rates = data.iloc[:, 2].values\n",
    "vols = data.iloc[:, 1].values\n",
    "\n",
    "#k = np.arange(0.8, 1.2, (1.2-0.8)/20)\n",
    "#ttm = np.arange(1, 5, 4/20)\n",
    "\n",
    "## for each r, sigma pair\n",
    "## generate a matrix of prices\n",
    "#maximum = 0\n",
    "#minimum = np.inf\n",
    "#prices = []\n",
    "#for v in vols:\n",
    "#    for r in rates:\n",
    "#        price =  np.zeros(shape=(20, 20))\n",
    "#        for ik, kv in enumerate(k):\n",
    "#            for it, t in enumerate(ttm):\n",
    "#                price[ik, it] = call(kv, r, v, t)\n",
    "#        prices.append(price)\n",
    "#        # max and min are saved to \n",
    "#        # normalize our matrices\n",
    "#        new_max = np.max(price)\n",
    "#        new_min = np.min(price)\n",
    "#        if new_max > maximum:\n",
    "#            maximum = new_max\n",
    "#        if new_min < minimum:\n",
    "#            minimum = new_min\n",
    "#            \n",
    "# normalize each matrix in the range 0, 255\n",
    "for ip, p in enumerate(prices):\n",
    "    prices[ip] = np.interp(p, (minimum, maximum), (0, 255)) \n",
    "\n",
    "json.dump(prices, open(\"2d_label.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below an example of the 20x20 images that have been created.\n",
    "\n",
    "<img src=\"2d_training_images.png\" width=200>\n",
    "\n",
    "Then the training is similar to what has been done for the handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-9dfdeae8d274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vols' is not defined"
     ]
    }
   ],
   "source": [
    "from finnn import FinNN\n",
    "labels = []\n",
    "for i in range(len(vols)):\n",
    "    for j in range(len(rates)):\n",
    "        labels.append((vols[i], rates[j]))            \n",
    "\n",
    "images = np.array(json.load(open(\"2d.json\")))\n",
    "\n",
    "trainer = FinNN(\"CNN2D\")\n",
    "trainer.setData(images, labels, 0.2)\n",
    "trainer.normalize()\n",
    "\n",
    "trainer.addConv2DLayer(8, 3, (20, 20, 1), activatio='relu')\n",
    "trainer.addFlatten()\n",
    "trainer.addHiddenLayer(10, activation='relu')\n",
    "trainer.addOutputLayer(2, activation='relu', bias_initializer='random_uniform')\n",
    "trainer.compile('mse', 'adam')\n",
    "\n",
    "trainer.fit(300, 32, verbose=1)\n",
    "#model.save(\"2d.b5\")\n",
    "#eval1 = model.evaluate(x_train, z_train)\n",
    "#print('Training: {}'.format(eval1))\n",
    "\n",
    "#eval2 = model.evaluate(x_test, z_test)\n",
    "#print('Test: {}'.format(eval2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Analysis\n",
    "\n",
    "In finance, *technical analysis* is a security analysis discipline for forecasting the direction of prices through the study of past market data, primarily price and volume.\n",
    "Essentially the analyst looks for particular patterns in the price time series that are *known* to develop in predictable ways to take profit of it.\n",
    "\n",
    "<img src=\"H_and_s_top_new.jpg\" width=400>\n",
    "<img src=\"Triangle-ascending.jpg\" width=400>\n",
    "\n",
    "As you may imagine we will try to develop a CNN (like in the handwriting case) capable of classifying features in time series to be used in a technical analysis (this is much faster than having somebody looking at thousands of time series by eye...).\n",
    "\n",
    "I have generated myself the training set simulating 21600 time series (1/3 with head and shoulder patter, 1/3 with triangle pattern and 1/3 with no pattern). *To make the training easier the features have been exagerated.*\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_1.png\" width=300>\n",
    "<figcaption>No pattern</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_2.png\" width=300>\n",
    "<figcaption>Head and shoulder pattern</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"image_0.png\" width=300>\n",
    "<figcaption>Tringle pattern</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "21598/21598 [==============================] - 17s 774us/step - loss: 0.8423\n",
      "Epoch 2/80\n",
      "21598/21598 [==============================] - 16s 738us/step - loss: 0.6163\n",
      "Epoch 3/80\n",
      "21598/21598 [==============================] - 16s 748us/step - loss: 0.5836\n",
      "Epoch 78/80\n",
      "21598/21598 [==============================] - 17s 775us/step - loss: 0.1894\n",
      "Epoch 79/80\n",
      "21598/21598 [==============================] - 17s 789us/step - loss: 0.2116\n",
      "Epoch 80/80\n",
      "21598/21598 [==============================] - 17s 804us/step - loss: 0.1881\n"
     ]
    }
   ],
   "source": [
    "from finnn import FinNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_labels = pd.read_csv(\"training_techana_labels.csv\")\n",
    "train_images = pd.read_csv(\"training_techana_images.csv\")\n",
    "#train_images = train_images[:3000]\n",
    "#train_images = np.array(train_images)\n",
    "train_images = np.expand_dims(train_images, axis=2)\n",
    "\n",
    "trainer = FinNN(\"CNN1D\")\n",
    "trainer.setData(train_images, train_labels)\n",
    "\n",
    "# define the CNN \n",
    "trainer.addConv1DInputLayer(80, 20, (101, 1))\n",
    "trainer.addConv1DLayer(80, 15)\n",
    "trainer.addMaxPooling1D(3)\n",
    "trainer.addConv1DLayer(100, 10)\n",
    "trainer.addConv1DLayer(100, 5)\n",
    "trainer.addGlobalAveragePooling1D()\n",
    "trainer.addDropout(0.5)\n",
    "trainer.addCNNOutputLayer(3)\n",
    "\n",
    "trainer.compileModel('categorical_crossentropy', 'adam')\n",
    "\n",
    "# make the training\n",
    "trainer.fit(80, 35)\n",
    "\n",
    "trainer.saveModel('techana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again for the most curious students\n",
    "\n",
    "Large neural nets trained on relatively small datasets can overfit the training data.\n",
    "\n",
    "This has the effect of the model learning the statistical noise in the training data, which results in poor performance when the model is evaluated on new data, e.g. a test dataset. \n",
    "\n",
    "One approach to reduce overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model. This is not feasible in practice, and can be approximated using a small collection of different models, called an ensemble.\n",
    "A problem even with the ensemble approximation is that it requires multiple models to be fit and stored, which can be a challenge if the models are large, requiring days or weeks to train and tune.\n",
    "\n",
    "*Dropout* is a regularization method that approximates training a large number of neural networks with different architectures in parallel.\n",
    "\n",
    "During training, some number of layer outputs are randomly ignored or *dropped out*. This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.\n",
    "\n",
    "Even if the it may seems counterintuitive (better training when switching off nodes) indeed dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the perfomance I wanted to simulate a real case scenario where the time series are analyzed in real-time in order to predict as soon as possible a particular pattern and take advantage of the prediction.\n",
    "\n",
    "TO do so I have created a longer time series and passed as input to the CNN sliding time windows to simulate the evolution of the time series. The goal was to check when the neural network was capable of predicting the incoming pattern.\n",
    "\n",
    "<img src=\"closing_price.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['0.71', '0.00', '0.29']\n",
      "0 ['0.66', '0.00', '0.34']\n",
      "0 ['0.82', '0.00', '0.18']\n",
      "0 ['0.91', '0.00', '0.09']\n",
      "2 ['0.40', '0.06', '0.54']\n",
      "1 ['0.00', '1.00', '0.00']\n",
      "1 ['0.00', '1.00', '0.00']\n",
      "1 ['0.00', '1.00', '0.00']\n",
      "1 ['0.00', '1.00', '0.00']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "test_images = pd.read_csv(\"testing_techana_frames.csv\")\n",
    "test_images = np.expand_dims(test_images, axis=2)\n",
    "\n",
    "trainer.loadModel('techana')\n",
    "\n",
    "predictions = trainer.predict(test_images)\n",
    "for i in range(len(predictions)):\n",
    "    print (np.argmax(predictions[i]), [\"{:.2f}\".format(p) for p in predictions[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at the 6th sample the CNN start recognizing the *head and shoulder* pattern in the price evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 8.3\n",
    "Taking as example the pricing NN trained on call, try to price put options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
