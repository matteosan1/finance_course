\documentclass[12pt,a4paper]{exam}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{enumitem}

\geometry{a4paper, margin=2cm}

\usepackage{cprotect}

\usepackage{xcolor}
\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython-frame}{RGB}{207, 207, 207}
\definecolor{ipython-bg}{RGB}{247, 247, 247}
\definecolor{ipython-red}{RGB}{186, 33, 33}
\definecolor{ipython-green}{RGB}{0, 128, 0}
\definecolor{ipython-cyan}{RGB}{64, 128, 128}
\definecolor{ipython-purple}{RGB}{170, 34, 255}

\usepackage{listings}
\lstdefinelanguage{iPython}{
	morekeywords={access,and,del,except,exec,in,is,lambda,not,or,raise},
	morekeywords=[2]{for,print,abs,all,any,basestring,bin,bool,bytearray,callable,chr,classmethod,cmp,compile,complex,delattr,dict,dir,divmod,enumerate,eval,execfile,file,filter,float,format,frozenset,getattr,globals,hasattr,hash,help,hex,id,input,int,isinstance,issubclass,iter,len,list,locals,long,map,max,memoryview,min,next,object,oct,open,ord,pow,property,range,reduce,reload,repr,reversed,round,set,setattr,slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,vars,xrange,zip,apply,buffer,coerce,intern,elif,else,if,continue,break,while,class,def,return,try,except,import,finally,try,except,from,global,pass, True, False},
	sensitive=true,
	morecomment=[l]\#,%
	morestring=[b]',%
	morestring=[b]",%
	moredelim=**[is][\color{black}]{@@}{@@},
	identifierstyle=\color{black}\footnotesize\ttfamily,
	commentstyle=\color{ipython-cyan}\footnotesize\itshape\ttfamily,
	stringstyle=\color{ipython-red}\footnotesize\ttfamily,
	keepspaces=true,
	showspaces=false,
	showstringspaces=false,
	rulecolor=\color{ipython-frame},
	frame=single,
	frameround={t}{t}{t}{t},
	backgroundcolor=\color{ipython-bg},
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=[2]\color{ipython-green}\bfseries\footnotesize\ttfamily, 
	keywordstyle=\color{ipython-purple}\bfseries\footnotesize\ttfamily
}

\lstdefinelanguage{iOutput} {
	sensitive=true,
	identifierstyle=\color{black}\small\ttfamily,
	stringstyle=\color{ipython-red}\small\ttfamily,
	keepspaces=true,
	showspaces=false,
	showstringspaces=false,
	rulecolor=\color{ipython-frame},
	basicstyle=\small\ttfamily,
}

\lstnewenvironment{ipython}[1][]{\lstset{language=iPython,mathescape=true,escapeinside={*@}{@*}}%
}{%
}

\lstnewenvironment{ioutput}[1][]{\lstset{language=iOutput,mathescape=true,escapeinside={*@}{@*}}%
}{%
}


\title{Financial Market Course 23/24\\ Exam}
\author{Prof. Simone Freschi, Prof. Matteo Sani}
\date{$21^{\mathrm{st}}$ January 2023}

\printanswers
%\noprintanswers
\begin{document}
\maketitle

\begin{center}
\fbox{\fbox{\parbox{5.5in}{\centering
Answer the questions in the spaces provided. If you run out of room for an answer, continue on the page back.}}}
\end{center}

\begin{center}
\vspace{5mm}
\makebox[0.75\textwidth]{Student's name:\enspace\hrulefill}
\end{center}

\section*{Questions}
\vspace{.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question Consider that the spot yield on government bonds are 4\%, 5\% and 5\% for 1-year, 2-year and 3-year maturity respectively. For corporate BBB bonds, the spot yields are 5\%, 7\% and 8\% respectively. If the recovery rate is 0\%, what is the implied cumulative probability that companies issuing BBB bonds will default after 3 years? What if the recovery rate is 10\%?
\begin{solution}
The forward rates are respectively:
F(0;1,2) = 0.06
F(0;2,3) = 0.05

F_c(0;1,2) = 0.09
F_c(0;2,3) = 0.1
 
Call $p_{01}$ the probability that the corporate bond will not default between year 0 and year 1.
\begin{equation}
p_{01} = \cfrac{1.04}{1.05} = 0.9905 \Rightarrow 1-p_{01} = 0.95\%
\end{equation}
For further periods:
\begin{equation}
p_{12} = \cfrac{1.06}{1.09} = 0.9724 \Rightarrow 1-p_{12} = 2.76\%
p_{23} = \cfrac{1.05}{1.1} = 0.9545 \Rightarrow 1-p_{23} = 4.55\%
\end{equation}
 
The cumulative probability of default is then around 8.26\%.
If the recovery rate is 10\%, we get:
\begin{equation}
p_{01}(1+r_{c1}) + 0.1(1-p_{01})(1+r_{c1} = 1+r_1 \Rightarrow (1+r_{c1})(0.9p_{01}+0.1) = 1+r_1 
\Rightarrow 0.9p_{01} = \cfrac{1+r_1}{1+r_{c1}}-0.1 = 1.04/1.05 - 0.1 \Rightarrow 1-p_{01} = 1.06\%
\end{equation}
The same way we find:
 \begin{equation}
0.9p_{12}= 1.06/1.09 - 0.1 \Rightarrow 1-p_{12} = 3.05\%
0.9p_{23}= 1.05/1.1 - 0.1 \Rightarrow 1-p_{23} = 5.05\%
\end{equation}
The  cumulative probability of default is then around 9.16\%. Intuitively, the more the bond pays in case of default, the lower are the required yields. Hence, if yields are high despite the recovery rate of 10\%, it must be that the default probability is higher. 

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question
Let us consider at date $t$, a bond with $FV = \$ 100$, an annual coupon rate of 7\% and time-to-maturity 3 years (T - t = 3 years).
Let also imagine that at date $t$ the market price of the bond at \$ 112.
Suppose that, at date $t$, ZCBs with face value equal to \$ 1, are traded with residual maturity of 1, 2 and 3 years.
Finally assume that the market prices are $B_{1Y} = 0.98$, $B_{2Y} = 0.94$ and $B_{3Y} = 0.90$.
Is there an arbitrage opportunity? Why?
\fillwithlines{3cm}
\begin{solution}
The no-arbitrage price of the bond is
\begin{equation*}
  P = (0.07\cdot100)\cdot 0.98 + (0.07\cdot100)\cdot 0.94 + (100 + 0.07\cdot100)\cdot 0.90 = 109.74
\end{equation*}

This price is smaller than the quoted price hence buying an appropriate number of $B_{1Y}$, $B_{2Y}$ and $B_{3Y}$ and selling the coupon bearing bond it is possible to make a positive net profit.

In particular buying 7 $B_{1Y}$, 7 $B_{2Y}$ and 107 $B_{3Y}$ the profit is \$ 2.26.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{checkboxes}
\choice myvar = myvar + 6.99
\choice myvar + 6.99 = myvar
\choice myvar += 6.99
\choice None of the above
\end{checkboxes}
\begin{solution}
The second answer is the correct one.
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question
What is the output of the following code ?

\begin{ipython}
d = {"apple":10, "pear":4, "orange":3}

print (d["peach"])
\end{ipython}
\makeemptybox{3cm}
\begin{solution}
\texttt{KeyError: 'peach'}
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\question
List down the names of some popular Activation Functions used in Neural Networks.
\fillwithlines{3cm}
\begin{solution}
Some of the popular activation functions that are used while building the deep learning models are as follows:
Sigmoid function, Hyperbolic tangent function, Rectified linear unit (RELU) function, Leaky RELU function, Maxout function, Exponential Linear unit (ELU) function.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What do you mean by Backpropagation ?
\fillwithlines{3cm}
\begin{solution}
The backpropagation algorithm is used to train multilayer perceptrons. It propagates the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient or derivatives.
Backpropagation can be divided into the following steps:
\begin{itemize}
\item it can forward the propagation of training data through the network to generate output;
\item it uses target value and output value to compute error derivatives by concerning the output activations;
\item it can backpropagate to calculate the derivatives of the error concerning output activations in the previous layer and continue for all the hidden layers;
\item it uses the previously computed derivatives for output and all hidden layers to calculate the error derivative concerning weights;
\item it updates the weights and repeats until the cost function is minimized.
\end{itemize}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
How to initialize Weights and Biases in Neural Networks ?
\fillwithlines{3cm}
\begin{solution}
Neural network initialization means initialized the values of the parameters i.e, weights and biases. Biases can be initialized to zero but we can’t initialize weights with zero.
Weight initialization is one of the crucial factors in neural networks since bad weight initialization can prevent a neural network from learning the patterns.

On the contrary, a good weight initialization helps in giving a quicker convergence to the global minimum. As a rule of thumb, the rule for initializing the weights is to be close to zero without being too small.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Why is zero initialization of weight in a Neural Network, not a good initialization technique ?
\fillwithlines{3cm}
\begin{solution}
If we initialize the set of weights in the neural network as zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.
As a result, the neural network cannot learn anything at all because there is no source of asymmetry between different neurons. Therefore, we add randomness while initializing the weight in neural networks.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Explain the different steps used in Gradient Descent Algorithm.
\fillwithlines{3cm}
\begin{solution}
The five main steps that are used to initialize and use the gradient descent algorithm are as follows:
\begin{enumerate}
\item initialize biases and weights for the neural network;
\item pass the input data through the network i.e, the input layer;
\item compute the difference or the error between the expected and the predicted values;
\item adjust the values i.e, weight updation in neurons to minimize the loss function;
\item we repeat the same steps i.e, multiple iterations to determine the best weights for efficient working.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Explain the term “Data Normalization”.
\fillwithlines{3cm}
\begin{solution}
Data normalization is an essential preprocessing step, which is used to rescale the initial values to a specific range. It ensures better convergence during backpropagation.
In general, data normalization boils down each of the data points to subtracting the mean and dividing by its standard deviation. This technique improves the performance and stability of neural networks since we normalized the inputs in every layer.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What is the difference between Forward propagation and Backward Propagation in Neural Networks ?
\fillwithlines{3cm}
\begin{solution}
\emph{Forward propagation}: The input is fed into the network. In each layer, there is a specific activation function and between layers, there are weights that represent the connection strength of the neurons. The input runs through the individual layers of the network, which ultimately generates an output.

\emph{Backward propagation}: an error function measures how accurate the output of the network is. To improve the output, the weights have to be optimized. The backpropagation algorithm is used to determine how the individual weights have to be adjusted. The weights are adjusted during the gradient descent method.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What do you mean by Hyperparameters ?
\fillwithlines{3cm}
\begin{solution}
Once the data is formatted correctly, we are usually working with hyperparameters in neural networks. A hyperparameter is a kind of parameter whose values are fixed before the learning process begins.
It decides how a neural network is trained and also the structure of the network which includes: the number of hidden units, the learning rate, the number of epochs, etc\ldots
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Why is ReLU the most commonly used Activation Function ?
\fillwithlines{3cm}
\begin{solution}
ReLU (Rectified Linear Unit) is the most commonly used activation function in neural networks due to the following reasons:
\begin{enumerate}
\item No vanishing gradient: The derivative of the RELU activation function is either 0 or 1, so it could be not in the range of [0,1]. As a result, the product of several derivatives would also be either 0 or 1, because of this property, the vanishing gradient problem doesn’t occur during backpropagation.
\item Faster training: Networks with RELU tend to show better convergence performance. Therefore, we have a much lower run time.
\item Sparsity: For all negative inputs, a RELU generates an output of 0. This means that fewer neurons of the network are firing. So we have sparse and efficient activations in the neural network.
\end{enumerate}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Explain the vanishing and exploding gradient problems.
\fillwithlines{3cm}
\begin{solution}
These are the major problems in training deep neural networks.
While Backpropagation, in a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large e.g, If use ReLU like activation function then the value of the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of Exploding gradient.

On the contrary, if the derivatives are small e.g, If use a Sigmoid activation function then the gradient will decrease exponentially as we propagate through the model until it eventually vanishes, and this is the Vanishing gradient problem.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What do you mean by Optimizers ?
\fillwithlines{3cm}
\begin{solution}
Optimizers are algorithms or methods that are used to adjust the parameters of the neural network such as weights, biases, and learning rate, etc to minimize the loss function. These are used to solve the optimization problems by minimizing the function.
The most common used optimizers in deep learning are as follows:

Gradient Descent
Stochastic Gradient Descent (SGD)
Mini Batch Stochastic Gradient Descent (MB-SGD)
SGD with momentum
Nesterov Accelerated Gradient (NAG)
Adaptive Gradient (AdaGrad)
AdaDelta
RMSprop
Adam
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Why are Deep Neural Networks preferred over Shallow Neural Networks ?
\fillwithlines{3cm}
\begin{solution}
Neural networks contain hidden layers apart from input and output layers. There is only a single hidden layer between the input and output layers for shallow neural networks whereas, for Deep neural networks, there are multiple layers used.
To approximate any function, both shallow and deep networks are good enough and capable but when a shallow neural network fits into any function, it requires a lot of parameters to learn. On the contrary, deep networks can fit functions even better with a limited number of parameters since they contain several hidden layers.

So, for the same level of accuracy, deeper networks can be much more powerful and efficient in terms of both computation and the number of parameters to learn.

One other important thing about deeper networks is that they can create deep representations and at every layer, the network learns a new, more abstract representation of the input.

Therefore, in modern days deep neural networks have become preferable owing to their ability to work on any kind of data modeling.
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Overfitting is one of the most common problems every Machine Learning practitioner faces. Explain some methods to avoid overfitting in Neural Networks.
\fillwithlines{3cm}
\begin{solution}
\emph{Dropout}: It is a regularization technique that prevents the neural network from overfitting. It randomly drops neurons from the neural network during training which is equivalent to training different neural networks. The different networks will overfit differently, so the net effect of the dropout regularization technique will be to reduce overfitting so that our model will be good for predictive analysis.

\emph{Early stopping}: This regularization technique updates the model to make it better fit the training data with each iteration. After a certain number of iterations, new iterations improve the model. After that point, however, the model begins to overfit the training data. Early stopping refers to stopping the training process before that point.
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What is the difference between Epoch, Batch, and Iteration in Neural Networks ?
\fillwithlines{3cm}
\begin{solution}
Epoch, iteration, and batch are different types that are used for processing the datasets and algorithms for gradient descent. All these three methods, i.e., epoch, iteration, and batch size are basically ways of working on the gradient descent depending on the size of the data set.
\emph{Epoch}: it represents one iteration over the entire training dataset (everything put into the training model).
\emph{Batch}: this refers to when we are not able to pass the entire dataset into the neural network at once due to the problem of high computations, so we divide the dataset into several batches.
\emph{Iteration}: let’s have 10,000 images as our training dataset and we choose a batch size of 200. then an epoch should run (10000/200) iterations i.e, 50 iterations.
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Suppose we have a neuron having weights corresponding to the three inputs have the following values: $w_1 = 2$; $w_2 = -4$; and $w_3 = 1$
and the activation of the unit is given by the step-function: 
\begin{equation*}
\phi(v) = \begin{cases}
1, & \text{if $v\leq0$}\\
0, & \text{otherwise}
\end{cases}
\end{equation*}
Calculate the output value y of the given perceptron for each of the following input patterns:
\begin{center}
\begin{tabular}{ c c c c c }
Pattern	&  $P_1$ & $P_2$ & $P_3$ & $P_4$ \\
$x_1$ & 1 & 0 & 1 & 1 \\
$x_2$ & 0 & 1 & 0 & 1 \\
$x_3$ & 0 & 1 & 1 & 1
\end{tabular}
\end{center}
\fillwithlines{3cm}
\begin{solution}
To calculate the output value y for each of the given patterns we have to follow below two steps:
\begin{enumerate}[label=\alph*]
\item Calculate the weighted sum: $v = \sum_i(w_i\cdot x_i)= w_1 \cdot x_1 + w_2\cdot x_2 + w3\cdot x_3$;
\item Apply the activation function to $v$.
\end{enumerate}
The calculations for each input pattern are:
\begin{itemize}
\item $P_1$: $v = 2\cdot 1-4\cdot 0+1\cdot 0=2, (2>0),  y=\phi(2)=1$;
\item $P_2$: $v = 2\cdot 0-4\cdot 1+1\cdot 1=-3, (-3<0),  y=\phi(-3)=0$;
\item $P_3$: $v = 2\cdot 1-4\cdot 0+1\cdot 1=3, (3>0),  y=\phi(3)=1$;
\item $P_4$: $v = 2\cdot 1-4\cdot 1+1\cdot 1=-1, (-1<0),  y=\phi(-1)=0$.
\end{itemize}
\end{solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
Consider a feed-forward Neural Network having 2 inputs ($x_1$ and $x_2$) with fully connected layers and with 2 hidden layers:
\begin{itemize}
\item hidden layer-1: nodes labeled as 3 and 4;
\item hidden layer-2: nodes labeled as 5 and 6.
\end{itemize}

A weight on the connection between nodes $i$ and $j$ is represented by $w_{ij}$, such as $w_{24}$ is the weight on the connection between nodes 2 and 4. The following lists contain all the weights values used in the given network:
$w_{13}=-2$, $w_{35}=1$, $w_{23} = 3$, $w_{45} = -1$, $w_{14} = 4$, $w_{36} = -1$, $w_{24}=-1$, $w_{46}=1$.

Each of the nodes 3, 4, 5, and 6 use the following activation function:

\begin{equation*}
\phi(v) = \begin{cases}
1, & \text{if $v\leq0$}\\
0, & \text{otherwise}
\end{cases}
\end{equation*}

where $v$ denotes the weighted sum of a node. Each of the input nodes (1 and 2) can only receive binary values (either 0 or 1). Calculate the output of the network ($y_5$ and $y_6$) for the input pattern given by $x_1 = x_2 = 0$.
\fillwithlines{3cm}
\begin{solution}
To find the output of the network it is necessary to calculate weighted sums of hidden nodes 3 and 4:
\begin{equation*}
v_3 =w_{13}\cdot x_1 +w_{23}\cdot x_2 , v_4 =w_{14}\cdot x_1 +w_{24}\cdot x_2
\end{equation*}

Then find the outputs from hidden nodes using activation function $\phi$:
\begin{equation*}
y_3 =\phi(v_3), y_4 =\phi(v_4)
\end{equation*}

Use the outputs of the hidden nodes $y_3$ and $y_4$ as the input values to the output layer (nodes 5 and 6), and find weighted sums of output nodes 5 and 6:
\begin{equation*}
v_5 =w_{35}\cdot y_3 +w_{45}\cdot y_4 , v_6 =w_{36}\cdot y_3 +w_{46}\cdot y_4
\end{equation*}

Finally, find the outputs from nodes 5 and 6 (also using $\phi$):
\begin{equation*}
y_5 =\phi(v_5), y_6 =\phi(v_6)
\end{equation*}

The output pattern will be ($y_5$, $y_6$).

Perform this calculation for the given input – Input pattern (0, 0)
\begin{align*}
%\begin{gathered}
v_3 & =-2\cdot 0+3\cdot 0=0 &\rightarrow &y_3 =\phi(0)=1\\
v_4 &=4\cdot 0-1\cdot 0=0 &\rightarrow &y_4 =\phi(0)=1\\
v_5 &=1\cdot 1-1\cdot 1=0 &\rightarrow &y_5 =\phi(0)=1\\
v_6 &=-1\cdot 1+1\cdot 1=0 &\rightarrow &y_6 =\phi(0)=1\\
%\end{gathered}
\end{align*}

Therefore, the output of the network for a given input pattern is (1, 1).
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question
What value is printed by the following code ?
\begin{ipython}
import math

def f(x):
  y = math.exp(-math.sin(2*math.pi*x))

print (f(0.1234))
\end{ipython}

\begin{checkboxes}
\choice None
\choice 0
\choice 1
\choice False
\end{checkboxes}
\begin{solution}
\texttt{None}
\end{solution}


\end{questions}
\end{document}
